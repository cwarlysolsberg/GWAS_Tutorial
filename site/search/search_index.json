{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Overview \u00b6 This tutorial provides a step-by-step guide to performing basic Genome Wide Association Study (GWAS) analyses and accompanies our GWAS Guide paper . The aim of this tutorial is to provide a simple introduction of GWAS analyses while equipping existing users with a better understanding of the processes and implementation of mainstay GWAS tools. The tutorial is separated into five main section. Quality Control of GWAS Data Population Stratification Association Analyses Visualizing of Association results Deep dive into underpinnings of associations Polygenic Risk Score Warning Data used in this tutorial are simulated and intended for demonstration purposes only. The results from this tutorial will not reflect the true performance of different software. Notes We assume you have basic knownledges on how to use the terminal, plink and R . If you are unfamiliar with any of those, you can refer to the following online resources: Software link terminal (OS X / Linux) 1 , 2 terminal (Windows) 1 , 2 plink v1.90 , v1.75 R 1 Note This tutorial is written for Linux and OS X operating systems. Windows users will need to change some commands accordingly. Requirements \u00b6 To follow the tutorial, you will need the following programs installed: R ( version 3.2.3+ ) PLINK 1.9 Citation \u00b6 If you find this tutorial helpful for a publication, then please consider citing: Citation https://medium.com/tenxor/how-to-generate-a-sequence-diagram-within-markdown-using-js-sequence-diagram-and-mkdocs-91dd4fe0b8fb when we want to make flowchart \u00b6","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#overview","text":"This tutorial provides a step-by-step guide to performing basic Genome Wide Association Study (GWAS) analyses and accompanies our GWAS Guide paper . The aim of this tutorial is to provide a simple introduction of GWAS analyses while equipping existing users with a better understanding of the processes and implementation of mainstay GWAS tools. The tutorial is separated into five main section. Quality Control of GWAS Data Population Stratification Association Analyses Visualizing of Association results Deep dive into underpinnings of associations Polygenic Risk Score Warning Data used in this tutorial are simulated and intended for demonstration purposes only. The results from this tutorial will not reflect the true performance of different software. Notes We assume you have basic knownledges on how to use the terminal, plink and R . If you are unfamiliar with any of those, you can refer to the following online resources: Software link terminal (OS X / Linux) 1 , 2 terminal (Windows) 1 , 2 plink v1.90 , v1.75 R 1 Note This tutorial is written for Linux and OS X operating systems. Windows users will need to change some commands accordingly.","title":"Overview"},{"location":"#requirements","text":"To follow the tutorial, you will need the following programs installed: R ( version 3.2.3+ ) PLINK 1.9","title":"Requirements"},{"location":"#citation","text":"If you find this tutorial helpful for a publication, then please consider citing: Citation","title":"Citation"},{"location":"#httpsmediumcomtenxorhow-to-generate-a-sequence-diagram-within-markdown-using-js-sequence-diagram-and-mkdocs-91dd4fe0b8fb-when-we-want-to-make-flowchart","text":"","title":"https://medium.com/tenxor/how-to-generate-a-sequence-diagram-within-markdown-using-js-sequence-diagram-and-mkdocs-91dd4fe0b8fb when we want to make flowchart"},{"location":"Additional_considerations/","text":"Introduction \u00b6 While this tutorial is intended to be as robust as possible, there are some additional processes and considerations that may be needed for individual datasets. This section serves to provide examples of when and how to incorportant various tools and commands that were not necessary for our example HapMap dataset, but are likely to come up in other analyses. Liftover \u00b6 Note This section is neccessary when the build of your own dataset is different from the reference set used for annotation or population stratification anchoring. While the update-map tool can be used (as demonstrated in the Population Stratification section), Liftover is also useful when performing a meta-analysis to ensure all individual datasets are in the same build prior to combining summary statistics. The commands outlined here will provide an example for downloading Liftover and neccessary associated files to convert a data set from build hg38 to hg19. You can install Liftover and its dependencies with the following commands: wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/liftOver ##replace the following chain file to match your desired build conversion wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/liftOver/hg38ToHg19.over.chain.gz wget https://raw.githubusercontent.com/Shicheng-Guo/GscPythonUtility/master/liftOverPlink.py chmod +x liftOverPlink.py chmod +x liftOver Once you have downloaded the necessary filesets and changed the permissions to facilitate running the python code you can declare your starting fileset: fileset=startingfileset Next you need to change from space delimiter to tab delimiter with the '--recode tab' handle. plink --bfile $fileset --recode tab --out $fileset.tab Finally, you can run liftover to change your fileset build and then convert back to plink format. ./liftOverPlink.py -m $fileset.tab.map -p $fileset.tab.ped -o $fileset.hg19 -c hg38ToHg19.over.chain.gz -e ./liftOver plink --ped $fileset.hg19.ped --map $fileset.hg19.map --make-bed --out $fileset.final.hg19 To confirm your file was appropriately lifted over, it is good practice to choose a few snps from your bim file and confirm on dbgap whether your snpid appropriately matches with the corresponding chr:bp positions presented. Here, we see we appropriately lifted-over to build hg19 (GRCh37) Imputation \u00b6 Meta-Analysis using METAL software \u00b6 Separated by Chromosome \u00b6 Here, we demonstrate how to perform population stratification using the much larger updated 1000 Genomes data set that is separated by chromosome. The steps here are not only useful for the purpose of population stratification but also to teach users how to handle any data set separated by chromosome (common for large datasets) First, we can download the data set using a wget command: gr=_GRCh38 for chr in {1..22}; do \\ wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/GRCh38_positions/ALL.chr$chr$gr.genotypes.20170504.vcf.gz wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/GRCh38_positions/ALL.chr$chr$gr.genotypes.20170504.vcf.gz.tbi done This loop takes a long time to run. You may want to let it sit over night if you have the option to. for chr in {1..11}; do \\ plink --vcf ALL.chr \\(chr\\) gr.genotypes.20170504.vcf.gz --make-bed --vcf-half-call m --out chr_$chr.GRCh38 done for chr in {12..22}; do \\ plink --bfile chr_ \\(chr.GRCh38 --exclude 1000genomes_hg38-merge.missnp --allow-extra-chr --make-bed --out chr\\) chr.exclude \u00b6 plink --bfile chr_ \\(chr.GRCh38 --allow-extra-chr --make-bed --out chr\\) chr done ls .exclude.bed >allfiles.txt sed 's/.. \\(//' < allfiles.txt > test sed 's/..\\) //' < test > allfiles.txt plink --bfile chr_10 --merge-list allfiles.txt --allow-extra-chr --make-bed -out 1000genomes_hg38 Cluster computing \u00b6 Admixture \u00b6","title":"6. Additional Considerations"},{"location":"Additional_considerations/#introduction","text":"While this tutorial is intended to be as robust as possible, there are some additional processes and considerations that may be needed for individual datasets. This section serves to provide examples of when and how to incorportant various tools and commands that were not necessary for our example HapMap dataset, but are likely to come up in other analyses.","title":"Introduction"},{"location":"Additional_considerations/#liftover","text":"Note This section is neccessary when the build of your own dataset is different from the reference set used for annotation or population stratification anchoring. While the update-map tool can be used (as demonstrated in the Population Stratification section), Liftover is also useful when performing a meta-analysis to ensure all individual datasets are in the same build prior to combining summary statistics. The commands outlined here will provide an example for downloading Liftover and neccessary associated files to convert a data set from build hg38 to hg19. You can install Liftover and its dependencies with the following commands: wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/liftOver ##replace the following chain file to match your desired build conversion wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/liftOver/hg38ToHg19.over.chain.gz wget https://raw.githubusercontent.com/Shicheng-Guo/GscPythonUtility/master/liftOverPlink.py chmod +x liftOverPlink.py chmod +x liftOver Once you have downloaded the necessary filesets and changed the permissions to facilitate running the python code you can declare your starting fileset: fileset=startingfileset Next you need to change from space delimiter to tab delimiter with the '--recode tab' handle. plink --bfile $fileset --recode tab --out $fileset.tab Finally, you can run liftover to change your fileset build and then convert back to plink format. ./liftOverPlink.py -m $fileset.tab.map -p $fileset.tab.ped -o $fileset.hg19 -c hg38ToHg19.over.chain.gz -e ./liftOver plink --ped $fileset.hg19.ped --map $fileset.hg19.map --make-bed --out $fileset.final.hg19 To confirm your file was appropriately lifted over, it is good practice to choose a few snps from your bim file and confirm on dbgap whether your snpid appropriately matches with the corresponding chr:bp positions presented. Here, we see we appropriately lifted-over to build hg19 (GRCh37)","title":"Liftover"},{"location":"Additional_considerations/#imputation","text":"","title":"Imputation"},{"location":"Additional_considerations/#meta-analysis-using-metal-software","text":"","title":"Meta-Analysis using METAL software"},{"location":"Additional_considerations/#separated-by-chromosome","text":"Here, we demonstrate how to perform population stratification using the much larger updated 1000 Genomes data set that is separated by chromosome. The steps here are not only useful for the purpose of population stratification but also to teach users how to handle any data set separated by chromosome (common for large datasets) First, we can download the data set using a wget command: gr=_GRCh38 for chr in {1..22}; do \\ wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/GRCh38_positions/ALL.chr$chr$gr.genotypes.20170504.vcf.gz wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/GRCh38_positions/ALL.chr$chr$gr.genotypes.20170504.vcf.gz.tbi done This loop takes a long time to run. You may want to let it sit over night if you have the option to. for chr in {1..11}; do \\ plink --vcf ALL.chr \\(chr\\) gr.genotypes.20170504.vcf.gz --make-bed --vcf-half-call m --out chr_$chr.GRCh38 done for chr in {12..22}; do \\","title":"Separated by Chromosome"},{"location":"Additional_considerations/#plink-bfile-chr_chrgrch38-exclude-1000genomes_hg38-mergemissnp-allow-extra-chr-make-bed-out-chrchrexclude","text":"plink --bfile chr_ \\(chr.GRCh38 --allow-extra-chr --make-bed --out chr\\) chr done ls .exclude.bed >allfiles.txt sed 's/.. \\(//' < allfiles.txt > test sed 's/..\\) //' < test > allfiles.txt plink --bfile chr_10 --merge-list allfiles.txt --allow-extra-chr --make-bed -out 1000genomes_hg38","title":"plink --bfile chr_\\(chr.GRCh38 --exclude 1000genomes_hg38-merge.missnp --allow-extra-chr --make-bed --out chr\\)chr.exclude"},{"location":"Additional_considerations/#cluster-computing","text":"","title":"Cluster computing"},{"location":"Additional_considerations/#admixture","text":"","title":"Admixture"},{"location":"QC/","text":"QC of HapMap3 data \u00b6 In this tutorial, you will learn how to perform all quality control steps that one typically takes before analyzing their genetic data further. The tutorial is based on insert download link for code . This code-file forms a pipeline that performs all quality control steps taken here in tandem. The code can easily be adjusted such that it works with any genetic dataset, and is a good starting point to perform your own analyses. Note You will need plink in this section, which can be download from here . Install the program plink and include its location in your PATH directory, which allows us to use plink instead of ./plink in the commands below. If PLINK is not in your PATH directory and is instead in your working directory, replace all instances of plink in the tutorial with ./plink . Obtaining HapMap data \u00b6 The first step in GWAS analyses is to generate or obtain the genotype or sequencing data. In this tutorial, we will store this data in binary PLINK format. Binary PLINK datasets consist of three corresponding files: *.bim, *.bed, and *.fam. A *.fam file contains information on the individuals in the dataset, whereas the *.bim file contains information on the genetic markers that are included. The *.bed file is typically the largest and contains the genotypes. As it is written in binary, it is not in readable format for humans. You can download the HapMap example data here Reading the genotype data file: \u00b6 We will set a FILESET variable to refer to our raw genotype datafile throughout this tutorial. This makes it easier to adjust the code used here for analyses on your own data, as you only need to change the FILESET variable towards the name of your own datafile, provided that it is stored in the .bim/.bed/.fam format. Next, we will set our working directory towards the directory in which we have stored the HapMap data. Furthermore, ensure that the PLINK application is in your working directory. You can use the bash command ls to check whether your working directory holds all the necessary files. FILESET=HapMap_3_r3_1 cd HOME/{user}/{path/folder containing your files} ls Check whether all files are stored in your working directory: $ ls HapMap_3_r3_1.bed HapMap_3_r3_1.bim HapMap_3_r3_1.fam plink Before we start performing QC on the HapMap data, ensure that you familiarize yourself with the format in which the datafiles are stored. Use the head command to see the beginning of a file. head HapMap_3_r3_1.fam head HapMap_3_r3_1.bim This results in the following output: $ head HapMap_3_r3_1.fam 1328 NA06989 0 0 2 2 1377 NA11891 0 0 1 2 1349 NA11843 0 0 1 1 1330 NA12341 0 0 2 2 1444 NA12739 NA12748 NA12749 1 -9 1344 NA10850 0 NA12058 2 -9 1328 NA06984 0 0 1 2 1463 NA12877 NA12889 NA12890 1 -9 1418 NA12275 0 0 2 1 13291 NA06986 0 0 1 1 $ head HapMap_3_r3_1.bim 1 rs2185539 0 556738 T C 1 rs11510103 0 557616 G A 1 rs11240767 0 718814 T C 1 rs3131972 0 742584 A G 1 rs3131969 0 744045 A G 1 rs1048488 0 750775 C T 1 rs12562034 0 758311 A G 1 rs12124819 0 766409 G A 1 rs4040617 0 769185 G A 1 rs2905036 0 782343 C T These files have no headers but are always structured the same. The structure of a .fam file is as follows: Column 1 contains the family id Column 2 contains the Within-family id Column 3 contains the Within-family id of the father ('0' if the father is not included in the data) Column 4 contains the Within-family id of the mother ('0' if the mother is not included in the data) Column 5 contains the individual's sex ('1' for male, '2' for female '0' if unknown) Column 6 contains the value of the phenotype ('1' for cases, '2' for controls. Missing data points are often stored as '-9', '0', or a non-numeric character) The structure of a .bim file is as follows: Column 1 contains the chromosome code (integer, 'X'/'Y'/'XY'/'MT', or '0' if unknown), or the chromosome name Column 2 contains the identifier of the genetic variant Column 3 contains the position in morgans or centimorgans, but this value is usually set to '0'. Column 4 contains the base-pair coordinate. Column 5 contains the reference allele (Allele 1), which is usually the minor allele. Column 6 contains the alternative allele (Allele 2), which is usually the major allele. One could run the command head HapMap_3_r3_1.bed as well, but this would result in gibberish: the .bed file is not legible to humans. Visualizing and correcting for missingness \u00b6 As a first step in our QC, we evoke PLINK to produce SNP and individual missingness data. The plink command in bash will start the PLINK program. The option -bfile is used to read the data (in binary PLINK format), the option --missing creates a plink.imiss and plink.lmiss output file with the individual and SNP missingness data, respectively. plink -bfile $FILESET --missing PLINK gives us the following output: PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to plink.log. Options in effect: --bfile HapMap_3_r3_1 --missing 12574 MB RAM detected; reserving 6287 MB for main workspace. 1457897 variants loaded from .bim file. 165 people (80 males, 85 females) loaded from .fam. 112 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 53 nonfounders present. Calculating allele frequencies... done. Warning: 225 het. haploid genotypes present (see plink.hh ); many commands treat these as missing. Total genotyping rate is 0.997378. --missing: Sample missing data report written to plink.imiss, and variant-based missing data report written to plink.lmiss. Inspect plink.imiss and plink.lmiss head plink.imiss FID IID MISS_PHENO N_MISS N_GENO F_MISS 1328 NA06989 N 4203 1457897 0.002883 1377 NA11891 N 20787 1457897 0.01426 1349 NA11843 N 1564 1457897 0.001073 1330 NA12341 N 6218 1457897 0.004265 1444 NA12739 Y 29584 1457897 0.02029 1344 NA10850 Y 2631 1457897 0.001805 1328 NA06984 N 9638 1457897 0.006611 1463 NA12877 Y 3788 1457897 0.002598 1418 NA12275 N 5349 1457897 0.003669 The column headers correspond to the following: FID : Family ID IID : Individual ID MISS_PHENO : Missing phenotype? (Y/N) N_MISS : Number of missing SNPs N_GENO : Number of non-obligatory missing genotypes F_MISS : Proportion of missing SNPs head plink.lmiss CHR SNP N_MISS N_GENO F_MISS 1 rs2185539 0 165 0 1 rs11510103 4 165 0.02424 1 rs11240767 0 165 0 1 rs3131972 0 165 0 1 rs3131969 0 165 0 1 rs1048488 1 165 0.006061 1 rs12562034 0 165 0 1 rs12124819 1 165 0.006061 1 rs4040617 0 165 0 The column headers correspond to the following: CHR : Chromosome SNP : SNP rsID N_MISS : Number of individuals for which this SNP is missing N_GENO : Number of non-obligatory missing genotypes F_MISS : Proportion of sample missing for this SNP Generate Histogram to view missingness. \u00b6 To summarize the extent of missing data, we load both files into R, and generate histograms to visualize the proportion of missingness in the data, per individual and per SNP, respectively. Performed in R Rsc indmiss<-read.table(file=\"plink.imiss\", header=TRUE) snpmiss<-read.table(file=\"plink.lmiss\", header=TRUE) # read data into R png(\"histimiss.png\") #indicates png format and gives title to file hist(indmiss[,6],main=\"Histogram individual missingness\") #selects column 6, names header of file dev.off() png(\"histlmiss.png\") hist(snpmiss[,5],main=\"Histogram SNP missingness\") dev.off() # shuts down the current device Most individuals have next to no missing SNP data, although some individuals have missing data at 2% or even more of all SNP loci. Similarily, most SNPs are available in virtually all individuals in our dataset, although some SNPs are missing with considerable frequency, i.e. in 2% or even up to 5% of all individuals. Filter on missingness \u00b6 As a first step of the quality control process, we will drop SNPs that are missing with considerable frequency from our data. Furthermore, we will drop individuals who frequently have missing genotype calls. We begin with defining our starting filter values. We recommend a missing genotype threshold of 2% (MIND) for each individual, as well as a missingness threshold of 2% (GENO) for each SNP. Again, defining these values as a seperate variable makes them easier to change when using this code for different analyses. GENO=0.02 MIND=0.02 In the following plink commands, the --make-bed flag instructs plink to convert the data back into the PLINK binary format after the filtering has been performed. The --out flag is used to name the resulting output file. It is important to filter on SNP missingness ( --geno ) prior to filtering on individual missingness ( --mind ). Furtermore, we recommend to filter for missingness in two steps: once with a very lenient missingness threshold (i.e. 20%), and next with the missingness threshold you set (i.e. 2%). This two-step approach shields you from bad scenarios where, for example, many SNPs would fall above the missingness threshold if a subset of individuals have extremely high rates of missingness. Note, however, that most high quality genetic datasets should not have any SNPs or individuals with missingness >20%, and if this is the case, it should give reason to worry about whether the data was pre-processed correctly. plink --bfile $FILESET --geno 0.2 --make-bed --out $FILESET.geno.temp plink --bfile $FILESET.geno.temp --mind 0.2 --make-bed --out $FILESET.mind.temp plink --bfile $FILESET.mind.temp --geno $GENO --make-bed --out $FILESET.geno plink --bfile $FILESET.geno --mind $MIND --make-bed --out $FILESET.mind This results in the following output: $ plink --bfile $FILESET --geno 0.2 --make-bed --out $FILESET.geno.temp PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.geno.temp.log. Options in effect: --bfile HapMap_3_r3_1 --geno 0.2 --make-bed --out HapMap_3_r3_1.geno.temp 12574 MB RAM detected; reserving 6287 MB for main workspace. 1457897 variants loaded from .bim file. 165 people (80 males, 85 females) loaded from .fam. 112 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 53 nonfounders present. Calculating allele frequencies... done. Warning: 225 het. haploid genotypes present (see HapMap_3_r3_1.geno.temp.hh ); many commands treat these as missing. Total genotyping rate is 0.997378. 0 variants removed due to missing genotype data (--geno). 1457897 variants and 165 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (53 phenotypes are missing.) --make-bed to HapMap_3_r3_1.geno.temp.bed + HapMap_3_r3_1.geno.temp.bim + HapMap_3_r3_1.geno.temp.fam ... done. $plink --bfile $FILESET.geno.temp --mind 0.2 --make-bed --out $FILESET.mind.temp PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.mind.temp.log. Options in effect: --bfile HapMap_3_r3_1.geno.temp --make-bed --mind 0.2 --out HapMap_3_r3_1.mind.temp 12574 MB RAM detected; reserving 6287 MB for main workspace. 1457897 variants loaded from .bim file. 165 people (80 males, 85 females) loaded from .fam. 112 phenotype values loaded from .fam. 0 people removed due to missing genotype data (--mind). Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 53 nonfounders present. Calculating allele frequencies... done. Warning: 225 het. haploid genotypes present (see HapMap_3_r3_1.mind.temp.hh ); many commands treat these as missing. Total genotyping rate is 0.997378. 1457897 variants and 165 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (53 phenotypes are missing.) --make-bed to HapMap_3_r3_1.mind.temp.bed + HapMap_3_r3_1.mind.temp.bim + HapMap_3_r3_1.mind.temp.fam ... done. $ plink --bfile $FILESET.mind.temp --geno $GENO --make-bed --out $FILESET.geno PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.geno.log. Options in effect: --bfile HapMap_3_r3_1.mind.temp --geno 0.02 --make-bed --out HapMap_3_r3_1.geno 12574 MB RAM detected; reserving 6287 MB for main workspace. 1457897 variants loaded from .bim file. 165 people (80 males, 85 females) loaded from .fam. 112 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 53 nonfounders present. Calculating allele frequencies... done. Warning: 225 het. haploid genotypes present (see HapMap_3_r3_1.geno.hh ); many commands treat these as missing. Total genotyping rate is 0.997378. 27454 variants removed due to missing genotype data (--geno). 1430443 variants and 165 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (53 phenotypes are missing.) --make-bed to HapMap_3_r3_1.geno.bed + HapMap_3_r3_1.geno.bim + HapMap_3_r3_1.geno.fam ... done. $ plink --bfile $FILESET.geno --mind $MIND --make-bed --out $FILESET.mind PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.mind.log. Options in effect: --bfile HapMap_3_r3_1.geno --make-bed --mind 0.02 --out HapMap_3_r3_1.mind 12574 MB RAM detected; reserving 6287 MB for main workspace. 1430443 variants loaded from .bim file. 165 people (80 males, 85 females) loaded from .fam. 112 phenotype values loaded from .fam. 0 people removed due to missing genotype data (--mind). Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 53 nonfounders present. Calculating allele frequencies... done. Warning: 179 het. haploid genotypes present (see HapMap_3_r3_1.mind.hh ); many commands treat these as missing. Total genotyping rate is 0.997899. 1430443 variants and 165 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (53 phenotypes are missing.) --make-bed to HapMap_3_r3_1.mind.bed + HapMap_3_r3_1.mind.bim + HapMap_3_r3_1.mind.fam ... done. As expected, In the first step, 0 variants and 0 individuals were removed from the data. When subsequently filtering on the stricter threshold of 0.02, 27454 SNPs were removed from the data, leaving 1430443 SNPs. Finally, 0 people were removed due to missing genotype data, leaving 165 individuals. Check for Sex Discrepancies \u00b6 We check whether each individual's assigned sex corresponds with their genotype. The --check-sex flag in PLINK tests this by calculating X chromosome homozygosity. Males should have an X chromosome homozygosity estimate >0.8. Females should have a value < 0.2. Subjects who do not fulfill these requirements are flagged as a \"PROBLEM\" by PLINK. Note The --check-sex flag supposes that the x chromosome has been split. This means that in the .bim file you should see SNPs assigned to a 25th chromosome. This chromosome represents the SNPs lying on the pseudoautosomal region of the X chromosome. If you have not already done so, YOU MUST use the --split-x flag to seperate the pseudoautosomal region in this fashion. First, we want to visualize the F-values and check for discrepencies. plink --bfile $FILESET.mind --check-sex This results in the following output: PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to plink.log. Options in effect: --bfile HapMap_3_r3_1.mind --check-sex 12574 MB RAM detected; reserving 6287 MB for main workspace. 1430443 variants loaded from .bim file. 165 people (80 males, 85 females) loaded from .fam. 112 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 53 nonfounders present. Calculating allele frequencies... done. Warning: 179 het. haploid genotypes present (see plink.hh ); many commands treat these as missing. Total genotyping rate is 0.997899. 1430443 variants and 165 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (53 phenotypes are missing.) --check-sex: 23424 Xchr and 0 Ychr variant(s) scanned, 1 problem detected. Report written to plink.sexcheck . Plink reports that one individual has a different sex as determined by the X chromosome, compared to sex assigned in the pedigree file. Inspect plink.sexcheck: head plink.sexcheck FID IID PEDSEX SNPSEX STATUS F 1328 NA06989 2 2 OK -0.01184 1377 NA11891 1 1 OK 1 1349 NA11843 1 1 OK 1 1330 NA12341 2 2 OK -0.01252 1444 NA12739 1 1 OK 1 1344 NA10850 2 2 OK 0.01496 1328 NA06984 1 1 OK 1 1463 NA12877 1 1 OK 1 1418 NA12275 2 2 OK -0.1028 The column headers correspond to the following FID : Family ID IID : Individual ID PEDSEX : Sex as determined in pedigree file (1=male, 2=female) SNPSEX : Sex as determined by X chromosome STATUS : Displays \"PROBLEM\" or \"OK\" for each individual F : The actual X chromosome inbreeding (homozygosity) estimate We now visualize the estimated X chromosome homozygosity for assigned males and females in R: Performed in R gender <- read.table(\"plink.sexcheck\", header=T,as.is=T) png(\"Gender_check.png\") hist(gender[,6],main=\"Gender\", xlab=\"F\") dev.off() png(\"Men_check.png\") male=subset(gender, gender$PEDSEX==1) hist(male[,6],main=\"Men\",xlab=\"F\") dev.off() png(\"Women_check.png\") female=subset(gender, gender$PEDSEX==2) hist(female[,6],main=\"Women\",xlab=\"F\") dev.off() The third histogram shows where the problem lies: one individual is marked as female in the pedigree file, but is male according to the X chromosome. PLINK has marked this case as ``PROBLEM'' in column 5 of plink.sexcheck. We will use grep and awk to print the FID and IID of this person, and remove this person from our data using the --remove flag in PLINK. grep \"PROBLEM\" plink.sexcheck| awk '{print$1,$2}'> sex_discrepancy.txt plink --bfile $FILESET.mind --remove sex_discrepancy.txt --make-bed --out $FILESET.rem Limit data to autosomal SNPs only \u00b6 Most genetic analysis focuses solely on autosomal SNPs, ignoring the sex chromosome or mitochondrial DNA. We will use the --extract flag in PLINK to extract only such SNPs. Before using this flag, we will generate a list of RSids that lie on the autosome (chromosome 1-22), using the awk command. awk '{ if ($1 >= 1 && $1 <= 22) print $2 }' $FILESET.rem.bim > snp_1_22.txt plink --bfile $FILESET.rem --extract snp_1_22.txt --make-bed --out $FILESET.autosome Perform MAF check and filter by MAF Threshold \u00b6 Next, we will check the minor allele frequency (MAF) of each SNP. SNPs with a low MAF are rare. As a result, power to detect their associations with a given phenotype is low. Furthermore, such SNPs are more prone to genotyping errors. The --freq flag in PLINK is used to calculate MAFs. plink --bfile $FILESET.autosome --freq --out MAF_check PLINK reports the following output: $ plink --bfile $FILESET.rem --extract snp_1_22.txt --make-bed --out $FILESET.autosome PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.autosome.log. Options in effect: --bfile HapMap_3_r3_1.rem --extract snp_1_22.txt --make-bed --out HapMap_3_r3_1.autosome 12574 MB RAM detected; reserving 6287 MB for main workspace. 1430443 variants loaded from .bim file. 164 people (80 males, 84 females) loaded from .fam. 112 phenotype values loaded from .fam. --extract: 1398544 variants remaining. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 52 nonfounders present. Calculating allele frequencies... done. Total genotyping rate is 0.998052. 1398544 variants and 164 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (52 phenotypes are missing.) --make-bed to HapMap_3_r3_1.autosome.bed + HapMap_3_r3_1.autosome.bim + HapMap_3_r3_1.autosome.fam ... done. This creates the file MAF_check.frq. Inspect MAF_check.frq: head MAF_check.frq CHR SNP A1 A2 MAF NCHROBS 1 rs2185539 T C 0 224 1 rs11240767 T C 0 224 1 rs3131972 A G 0.1652 224 1 rs3131969 A G 0.1339 224 1 rs1048488 C T 0.1667 222 1 rs12562034 A G 0.1027 224 1 rs12124819 G A 0.2902 224 1 rs4040617 G A 0.1295 224 1 rs2905036 C T 0 224 Again, use R to visualize the MAF distribution. Performed in R maf_freq <- read.table(\"MAF_check.frq\", header =TRUE, as.is=T) png(\"MAF_distribution.png\") hist(maf_freq[,5],main = \"MAF distribution\", xlab = \"MAF\") dev.off() Now, we want to filter any SNPs with a low MAF. The larger your dataset, the higher your power, such that you can set a less conservative MAF threshold. We recommend to throw out all SNPs with MAF < 0.05 for a small data set such as the current one. For a larger dataset What is larger? , this threshold can be relaxed to 0.01. The --maf flag in PLINK filters out SNPs that fall below the given threshold. MAF=0.05 plink --bfile $FILESET.autosome --maf $MAF --make-bed --out $FILESET.maf PLINK reports the following output: ```bash $ plink --bfile $FILESET.autosome --maf $MAF --make-bed --out $FILESET.maf PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.maf.log. Options in effect: --bfile HapMap_3_r3_1.autosome --maf 0.05 --make-bed --out HapMap_3_r3_1.maf 12574 MB RAM detected; reserving 6287 MB for main workspace. 1398544 variants loaded from .bim file. 164 people (80 males, 84 females) loaded from .fam. 112 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 52 nonfounders present. Calculating allele frequencies... done. Total genotyping rate is 0.998052. 325318 variants removed due to minor allele threshold(s) (--maf/--max-maf/--mac/--max-mac). 1073226 variants and 164 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (52 phenotypes are missing.) --make-bed to HapMap_3_r3_1.maf.bed + HapMap_3_r3_1.maf.bim + HapMap_3_r3_1.maf.fam ... done ``` PLINK removed 325,318 SNPs, leaving 1,073,226 variants. View heterozygosity distribution and remove outliers \u00b6 We next check heterozygosity rates for all individuals within the sample. Excessively high or low heterozygosity rates could indicate sample contamination or inbreeding. Generate a pruned subset of SNPs that are in approximate linkage equilibrium. \u00b6 Heterozygosity must be computed in a set of independent SNPs. SNPs that are in linkage disequilibrium violate the independence assumption. Hence, we perform a procedure called ``variant pruning'' to reduce our dataset towards a set of SNPs that are in approximate linkage equilibrium with one another. The --indep-pairwise flag in PLINK performs the pruning procedure. Its syntax is --indep-pairwise ['kb'] . Within each window, PLINK computes the pairwise r^2 between each SNP, removes one of the SNPs (the one with the lowest MAF) if the pairwise r^2 is larger than the r^2 threshold, and repeats until no SNPs within the window have an r^2 above the given threshold. The algorithm will then shift its window, as given by the step size argument, and repeat. We recommend to set a window of 50kb, a step size of 5, and an r^2 threshold of 0.5. plink --bfile $FILESET.maf --indep-pairwise 50 5 0.5 PLINK reports the following output: ```bash plink --bfile $FILESET.maf --indep-pairwise 50 5 0.5 PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to plink.log. Options in effect: --bfile HapMap_3_r3_1.maf --indep-pairwise 50 5 0.5 12574 MB RAM detected; reserving 6287 MB for main workspace. 1073226 variants loaded from .bim file. 164 people (80 males, 84 females) loaded from .fam. 112 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 52 nonfounders present. Calculating allele frequencies... done. Total genotyping rate is 0.998039. 1073226 variants and 164 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (52 phenotypes are missing.) Pruned 67124 variants from chromosome 1, leaving 20596. Pruned 69461 variants from chromosome 2, leaving 20337. Pruned 57330 variants from chromosome 3, leaving 17532. Pruned 50485 variants from chromosome 4, leaving 15644. Pruned 52786 variants from chromosome 5, leaving 16231. Pruned 55187 variants from chromosome 6, leaving 16566. Pruned 44767 variants from chromosome 7, leaving 14337. Pruned 45018 variants from chromosome 8, leaving 13625. Pruned 37678 variants from chromosome 9, leaving 12654. Pruned 43374 variants from chromosome 10, leaving 13851. Pruned 42449 variants from chromosome 11, leaving 12835. Pruned 39534 variants from chromosome 12, leaving 13031. Pruned 30539 variants from chromosome 13, leaving 9951. Pruned 26275 variants from chromosome 14, leaving 8869. Pruned 23752 variants from chromosome 15, leaving 8256. Pruned 24630 variants from chromosome 16, leaving 9189. Pruned 20966 variants from chromosome 17, leaving 8152. Pruned 23347 variants from chromosome 18, leaving 8308. Pruned 13565 variants from chromosome 19, leaving 6097. Pruned 20631 variants from chromosome 20, leaving 7317. Pruned 11312 variants from chromosome 21, leaving 4170. Pruned 10938 variants from chromosome 22, leaving 4530. Pruning complete. 811148 of 1073226 variants removed. Marker lists written to plink.prune.in and plink.prune.out ``` The list of SNP rsIDs kept by the pruning procedure is given by plink.prune.in. The SNPs that are removed (811,148 of 1,073,226 variants), as they were in linkage disequilibrium with one or more of the SNPs in plink.prune.in, are listed in plink.prune.out Inspect plink.prune.in: head plink.prune.in rs1048488 rs12562034 rs12124819 rs4970383 rs1806509 rs28576697 rs13303106 rs13303010 rs2341354 rs35940137 Compute method-of-moments F coefficient estimates The --het flag in PLINK computes observed and expected autosomal homozygous genotype counts for each sample, and reports method-of-moments F coefficient estimates (i.e. (observed hom. count - expected count) / (total observations - expected count)) in the output file. We compute these statistics on the pruned SNPs only. plink --bfile $FILESET.maf --extract plink.prune.in --het --out R_check Show PLINK output: ```bash $ plink --bfile $FILESET .maf --extract plink.prune.in --het --out R_check PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to R_check.log. Options in effect: --bfile HapMap_3_r3_1.maf --extract plink.prune.in --het --out R_check 12574 MB RAM detected; reserving 6287 MB for main workspace. 1073226 variants loaded from .bim file. 164 people (80 males, 84 females) loaded from .fam. 112 phenotype values loaded from .fam. --extract: 262078 variants remaining. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 52 nonfounders present. Calculating allele frequencies... done. Total genotyping rate is 0.997962. 262078 variants and 164 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (52 phenotypes are missing.) --het: 262078 variants scanned, report written to R_check.het . ``` Inspect R_check.het bash head R_check.het FID IID O(HOM) E(HOM) N(NM) F 1328 NA06989 172125 1.727e+05 261511 -0.006857 1377 NA11891 170744 1.711e+05 259125 -0.004518 1349 NA11843 172346 1.73e+05 261907 -0.007276 1330 NA12341 170768 1.725e+05 261235 -0.01998 1444 NA12739 170563 1.704e+05 257944 0.00224 1344 NA10850 173379 1.729e+05 261734 0.005574 1328 NA06984 171767 1.722e+05 260670 -0.004437 1463 NA12877 172550 1.728e+05 261603 -0.002632 1418 NA12275 171259 1.726e+05 261348 -0.0153 The column headers correspond to the following FID : Family ID IID : Within-family ID O(HOM) : Observed number of homozygotes E(HOM) : Expected number of homozygotes N(NM) : Number of non-missing autosomal genotypes F : Method-of-moments F coefficient estimate Let's visualize the heterozygosity distribution in a histogram: Performed in R het <- read.table(\"R_check.het\", head=TRUE) png(\"heterozygosity.png\") het$HET_RATE = (het$\"N.NM.\" - het$\"O.HOM.\")/het$\"N.NM.\" hist(het$HET_RATE, xlab=\"Heterozygosity Rate\", ylab=\"Frequency\", main= \"Heterozygosity Rate\") dev.off() Next, let's create a list of individuals who deviate more than 3 standard deviations from the mean heterozygosity rate. Performed in R het <- read.table(\"R_check.het\", head=TRUE) het$HET_RATE = (het$\"N.NM.\" - het$\"O.HOM.\")/het$\"N.NM.\" het_fail = subset(het, (het$HET_RATE < mean(het$HET_RATE)-3*sd(het$HET_RATE)) | (het$HET_RATE > mean(het$HET_RATE)+3*sd(het$HET_RATE))); het_fail$HET_DST = (het_fail$HET_RATE-mean(het$HET_RATE))/sd(het$HET_RATE); write.table(het_fail, \"fail-het-qc.txt\", row.names=FALSE) Inspect fail-het-qc.txt : bash cat fail-het-qc.txt \"FID\" \"IID\" \"O.HOM.\" \"E.HOM.\" \"N.NM.\" \"F\" \"HET_RATE\" \"HET_DST\" 1330 \"NA12342\" 174790 172100 260519 0.0308 0.329070048633689 -4.69408664581716 1459 \"NA12874\" 175863 173000 261854 0.03269 0.328392921246191 -4.93949187693739 1340 \"NA06994\" 168357 171300 259327 -0.03301 0.350792628611753 3.17863399503596 Three individuals have an excessively low heterozygosity rate. We will remove these heterozygous outliers from the genetic data using PLINK. First, we save the first two columns of fail-het-qc.txt in a seperate file (sed 's/\"// g' is used to remove the quotations marks, such that the file, which was created in R in R becomes compatible with PLINK). Next, we use --remove in PLINK to remove the outliers. sed 's/\"// g' fail-het-qc.txt | awk '{print$1, $2}' > het_fail_ind.txt remove individuals that are heterozygous outliers plink --bfile $FILESET.maf --remove het_fail_ind.txt --make-bed --out $FILESET.het_fix The resulting output: ```bash plink --bfile $FILESET.maf --remove het_fail_ind.txt --make-bed --out $FILESET.het_fix PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.het_fix.log. Options in effect: --bfile HapMap_3_r3_1.maf --make-bed --out HapMap_3_r3_1.het_fix --remove het_fail_ind.txt 12574 MB RAM detected; reserving 6287 MB for main workspace. 1073226 variants loaded from .bim file. 164 people (80 males, 84 females) loaded from .fam. 112 phenotype values loaded from .fam. --remove: 161 people remaining. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 109 founders and 52 nonfounders present. Calculating allele frequencies... done. Total genotyping rate in remaining samples is 0.998098. 1073226 variants and 161 people pass filters and QC. Among remaining phenotypes, 55 are cases and 54 are controls. (52 phenotypes are missing.) --make-bed to HapMap_3_r3_1.het_fix.bed + HapMap_3_r3_1.het_fix.bim + HapMap_3_r3_1.het_fix.fam ... done. ``` Transfer files to use for population stratification \u00b6 Congratulations! You have finished the first tutorial on quality control of genetic data. We will use the cleaned data in the next tutorial to diagnose and correct for population stratification. To set up the data file you created for the next tutorial, create a new directory, and move the final .bim/.bed/.fam files towards. We will also Also make sure PLINK is copied towards this new directory. rename files and copy to population stratification folder mkdir ../2_Population_stratification cp $FILESET.het_fix.bed ../2_Population_stratification/$FILESET.qcout.bed cp $FILESET.het_fix.bim ../2_Population_stratification/$FILESET.qcout.bim cp $FILESET.het_fix.fam ../2_Population_stratification/$FILESET.qcout.fam cp plink.prune.in ../2_Population_stratification cp plink ../2_Population_stratification change directory into population stratification folder cd ../2_Population_stratification Further tips and further reading \u00b6","title":"1. Quality Control of GWAS Data"},{"location":"QC/#qc-of-hapmap3-data","text":"In this tutorial, you will learn how to perform all quality control steps that one typically takes before analyzing their genetic data further. The tutorial is based on insert download link for code . This code-file forms a pipeline that performs all quality control steps taken here in tandem. The code can easily be adjusted such that it works with any genetic dataset, and is a good starting point to perform your own analyses. Note You will need plink in this section, which can be download from here . Install the program plink and include its location in your PATH directory, which allows us to use plink instead of ./plink in the commands below. If PLINK is not in your PATH directory and is instead in your working directory, replace all instances of plink in the tutorial with ./plink .","title":"QC of HapMap3 data"},{"location":"QC/#obtaining-hapmap-data","text":"The first step in GWAS analyses is to generate or obtain the genotype or sequencing data. In this tutorial, we will store this data in binary PLINK format. Binary PLINK datasets consist of three corresponding files: *.bim, *.bed, and *.fam. A *.fam file contains information on the individuals in the dataset, whereas the *.bim file contains information on the genetic markers that are included. The *.bed file is typically the largest and contains the genotypes. As it is written in binary, it is not in readable format for humans. You can download the HapMap example data here","title":"Obtaining HapMap data"},{"location":"QC/#reading-the-genotype-data-file","text":"We will set a FILESET variable to refer to our raw genotype datafile throughout this tutorial. This makes it easier to adjust the code used here for analyses on your own data, as you only need to change the FILESET variable towards the name of your own datafile, provided that it is stored in the .bim/.bed/.fam format. Next, we will set our working directory towards the directory in which we have stored the HapMap data. Furthermore, ensure that the PLINK application is in your working directory. You can use the bash command ls to check whether your working directory holds all the necessary files. FILESET=HapMap_3_r3_1 cd HOME/{user}/{path/folder containing your files} ls Check whether all files are stored in your working directory: $ ls HapMap_3_r3_1.bed HapMap_3_r3_1.bim HapMap_3_r3_1.fam plink Before we start performing QC on the HapMap data, ensure that you familiarize yourself with the format in which the datafiles are stored. Use the head command to see the beginning of a file. head HapMap_3_r3_1.fam head HapMap_3_r3_1.bim This results in the following output: $ head HapMap_3_r3_1.fam 1328 NA06989 0 0 2 2 1377 NA11891 0 0 1 2 1349 NA11843 0 0 1 1 1330 NA12341 0 0 2 2 1444 NA12739 NA12748 NA12749 1 -9 1344 NA10850 0 NA12058 2 -9 1328 NA06984 0 0 1 2 1463 NA12877 NA12889 NA12890 1 -9 1418 NA12275 0 0 2 1 13291 NA06986 0 0 1 1 $ head HapMap_3_r3_1.bim 1 rs2185539 0 556738 T C 1 rs11510103 0 557616 G A 1 rs11240767 0 718814 T C 1 rs3131972 0 742584 A G 1 rs3131969 0 744045 A G 1 rs1048488 0 750775 C T 1 rs12562034 0 758311 A G 1 rs12124819 0 766409 G A 1 rs4040617 0 769185 G A 1 rs2905036 0 782343 C T These files have no headers but are always structured the same. The structure of a .fam file is as follows: Column 1 contains the family id Column 2 contains the Within-family id Column 3 contains the Within-family id of the father ('0' if the father is not included in the data) Column 4 contains the Within-family id of the mother ('0' if the mother is not included in the data) Column 5 contains the individual's sex ('1' for male, '2' for female '0' if unknown) Column 6 contains the value of the phenotype ('1' for cases, '2' for controls. Missing data points are often stored as '-9', '0', or a non-numeric character) The structure of a .bim file is as follows: Column 1 contains the chromosome code (integer, 'X'/'Y'/'XY'/'MT', or '0' if unknown), or the chromosome name Column 2 contains the identifier of the genetic variant Column 3 contains the position in morgans or centimorgans, but this value is usually set to '0'. Column 4 contains the base-pair coordinate. Column 5 contains the reference allele (Allele 1), which is usually the minor allele. Column 6 contains the alternative allele (Allele 2), which is usually the major allele. One could run the command head HapMap_3_r3_1.bed as well, but this would result in gibberish: the .bed file is not legible to humans.","title":"Reading the genotype data file:"},{"location":"QC/#visualizing-and-correcting-for-missingness","text":"As a first step in our QC, we evoke PLINK to produce SNP and individual missingness data. The plink command in bash will start the PLINK program. The option -bfile is used to read the data (in binary PLINK format), the option --missing creates a plink.imiss and plink.lmiss output file with the individual and SNP missingness data, respectively. plink -bfile $FILESET --missing PLINK gives us the following output: PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to plink.log. Options in effect: --bfile HapMap_3_r3_1 --missing 12574 MB RAM detected; reserving 6287 MB for main workspace. 1457897 variants loaded from .bim file. 165 people (80 males, 85 females) loaded from .fam. 112 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 53 nonfounders present. Calculating allele frequencies... done. Warning: 225 het. haploid genotypes present (see plink.hh ); many commands treat these as missing. Total genotyping rate is 0.997378. --missing: Sample missing data report written to plink.imiss, and variant-based missing data report written to plink.lmiss. Inspect plink.imiss and plink.lmiss head plink.imiss FID IID MISS_PHENO N_MISS N_GENO F_MISS 1328 NA06989 N 4203 1457897 0.002883 1377 NA11891 N 20787 1457897 0.01426 1349 NA11843 N 1564 1457897 0.001073 1330 NA12341 N 6218 1457897 0.004265 1444 NA12739 Y 29584 1457897 0.02029 1344 NA10850 Y 2631 1457897 0.001805 1328 NA06984 N 9638 1457897 0.006611 1463 NA12877 Y 3788 1457897 0.002598 1418 NA12275 N 5349 1457897 0.003669 The column headers correspond to the following: FID : Family ID IID : Individual ID MISS_PHENO : Missing phenotype? (Y/N) N_MISS : Number of missing SNPs N_GENO : Number of non-obligatory missing genotypes F_MISS : Proportion of missing SNPs head plink.lmiss CHR SNP N_MISS N_GENO F_MISS 1 rs2185539 0 165 0 1 rs11510103 4 165 0.02424 1 rs11240767 0 165 0 1 rs3131972 0 165 0 1 rs3131969 0 165 0 1 rs1048488 1 165 0.006061 1 rs12562034 0 165 0 1 rs12124819 1 165 0.006061 1 rs4040617 0 165 0 The column headers correspond to the following: CHR : Chromosome SNP : SNP rsID N_MISS : Number of individuals for which this SNP is missing N_GENO : Number of non-obligatory missing genotypes F_MISS : Proportion of sample missing for this SNP","title":"Visualizing and correcting for missingness"},{"location":"QC/#generate-histogram-to-view-missingness","text":"To summarize the extent of missing data, we load both files into R, and generate histograms to visualize the proportion of missingness in the data, per individual and per SNP, respectively. Performed in R Rsc indmiss<-read.table(file=\"plink.imiss\", header=TRUE) snpmiss<-read.table(file=\"plink.lmiss\", header=TRUE) # read data into R png(\"histimiss.png\") #indicates png format and gives title to file hist(indmiss[,6],main=\"Histogram individual missingness\") #selects column 6, names header of file dev.off() png(\"histlmiss.png\") hist(snpmiss[,5],main=\"Histogram SNP missingness\") dev.off() # shuts down the current device Most individuals have next to no missing SNP data, although some individuals have missing data at 2% or even more of all SNP loci. Similarily, most SNPs are available in virtually all individuals in our dataset, although some SNPs are missing with considerable frequency, i.e. in 2% or even up to 5% of all individuals.","title":"Generate Histogram to view missingness."},{"location":"QC/#filter-on-missingness","text":"As a first step of the quality control process, we will drop SNPs that are missing with considerable frequency from our data. Furthermore, we will drop individuals who frequently have missing genotype calls. We begin with defining our starting filter values. We recommend a missing genotype threshold of 2% (MIND) for each individual, as well as a missingness threshold of 2% (GENO) for each SNP. Again, defining these values as a seperate variable makes them easier to change when using this code for different analyses. GENO=0.02 MIND=0.02 In the following plink commands, the --make-bed flag instructs plink to convert the data back into the PLINK binary format after the filtering has been performed. The --out flag is used to name the resulting output file. It is important to filter on SNP missingness ( --geno ) prior to filtering on individual missingness ( --mind ). Furtermore, we recommend to filter for missingness in two steps: once with a very lenient missingness threshold (i.e. 20%), and next with the missingness threshold you set (i.e. 2%). This two-step approach shields you from bad scenarios where, for example, many SNPs would fall above the missingness threshold if a subset of individuals have extremely high rates of missingness. Note, however, that most high quality genetic datasets should not have any SNPs or individuals with missingness >20%, and if this is the case, it should give reason to worry about whether the data was pre-processed correctly. plink --bfile $FILESET --geno 0.2 --make-bed --out $FILESET.geno.temp plink --bfile $FILESET.geno.temp --mind 0.2 --make-bed --out $FILESET.mind.temp plink --bfile $FILESET.mind.temp --geno $GENO --make-bed --out $FILESET.geno plink --bfile $FILESET.geno --mind $MIND --make-bed --out $FILESET.mind This results in the following output: $ plink --bfile $FILESET --geno 0.2 --make-bed --out $FILESET.geno.temp PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.geno.temp.log. Options in effect: --bfile HapMap_3_r3_1 --geno 0.2 --make-bed --out HapMap_3_r3_1.geno.temp 12574 MB RAM detected; reserving 6287 MB for main workspace. 1457897 variants loaded from .bim file. 165 people (80 males, 85 females) loaded from .fam. 112 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 53 nonfounders present. Calculating allele frequencies... done. Warning: 225 het. haploid genotypes present (see HapMap_3_r3_1.geno.temp.hh ); many commands treat these as missing. Total genotyping rate is 0.997378. 0 variants removed due to missing genotype data (--geno). 1457897 variants and 165 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (53 phenotypes are missing.) --make-bed to HapMap_3_r3_1.geno.temp.bed + HapMap_3_r3_1.geno.temp.bim + HapMap_3_r3_1.geno.temp.fam ... done. $plink --bfile $FILESET.geno.temp --mind 0.2 --make-bed --out $FILESET.mind.temp PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.mind.temp.log. Options in effect: --bfile HapMap_3_r3_1.geno.temp --make-bed --mind 0.2 --out HapMap_3_r3_1.mind.temp 12574 MB RAM detected; reserving 6287 MB for main workspace. 1457897 variants loaded from .bim file. 165 people (80 males, 85 females) loaded from .fam. 112 phenotype values loaded from .fam. 0 people removed due to missing genotype data (--mind). Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 53 nonfounders present. Calculating allele frequencies... done. Warning: 225 het. haploid genotypes present (see HapMap_3_r3_1.mind.temp.hh ); many commands treat these as missing. Total genotyping rate is 0.997378. 1457897 variants and 165 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (53 phenotypes are missing.) --make-bed to HapMap_3_r3_1.mind.temp.bed + HapMap_3_r3_1.mind.temp.bim + HapMap_3_r3_1.mind.temp.fam ... done. $ plink --bfile $FILESET.mind.temp --geno $GENO --make-bed --out $FILESET.geno PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.geno.log. Options in effect: --bfile HapMap_3_r3_1.mind.temp --geno 0.02 --make-bed --out HapMap_3_r3_1.geno 12574 MB RAM detected; reserving 6287 MB for main workspace. 1457897 variants loaded from .bim file. 165 people (80 males, 85 females) loaded from .fam. 112 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 53 nonfounders present. Calculating allele frequencies... done. Warning: 225 het. haploid genotypes present (see HapMap_3_r3_1.geno.hh ); many commands treat these as missing. Total genotyping rate is 0.997378. 27454 variants removed due to missing genotype data (--geno). 1430443 variants and 165 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (53 phenotypes are missing.) --make-bed to HapMap_3_r3_1.geno.bed + HapMap_3_r3_1.geno.bim + HapMap_3_r3_1.geno.fam ... done. $ plink --bfile $FILESET.geno --mind $MIND --make-bed --out $FILESET.mind PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.mind.log. Options in effect: --bfile HapMap_3_r3_1.geno --make-bed --mind 0.02 --out HapMap_3_r3_1.mind 12574 MB RAM detected; reserving 6287 MB for main workspace. 1430443 variants loaded from .bim file. 165 people (80 males, 85 females) loaded from .fam. 112 phenotype values loaded from .fam. 0 people removed due to missing genotype data (--mind). Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 53 nonfounders present. Calculating allele frequencies... done. Warning: 179 het. haploid genotypes present (see HapMap_3_r3_1.mind.hh ); many commands treat these as missing. Total genotyping rate is 0.997899. 1430443 variants and 165 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (53 phenotypes are missing.) --make-bed to HapMap_3_r3_1.mind.bed + HapMap_3_r3_1.mind.bim + HapMap_3_r3_1.mind.fam ... done. As expected, In the first step, 0 variants and 0 individuals were removed from the data. When subsequently filtering on the stricter threshold of 0.02, 27454 SNPs were removed from the data, leaving 1430443 SNPs. Finally, 0 people were removed due to missing genotype data, leaving 165 individuals.","title":"Filter on missingness"},{"location":"QC/#check-for-sex-discrepancies","text":"We check whether each individual's assigned sex corresponds with their genotype. The --check-sex flag in PLINK tests this by calculating X chromosome homozygosity. Males should have an X chromosome homozygosity estimate >0.8. Females should have a value < 0.2. Subjects who do not fulfill these requirements are flagged as a \"PROBLEM\" by PLINK. Note The --check-sex flag supposes that the x chromosome has been split. This means that in the .bim file you should see SNPs assigned to a 25th chromosome. This chromosome represents the SNPs lying on the pseudoautosomal region of the X chromosome. If you have not already done so, YOU MUST use the --split-x flag to seperate the pseudoautosomal region in this fashion. First, we want to visualize the F-values and check for discrepencies. plink --bfile $FILESET.mind --check-sex This results in the following output: PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to plink.log. Options in effect: --bfile HapMap_3_r3_1.mind --check-sex 12574 MB RAM detected; reserving 6287 MB for main workspace. 1430443 variants loaded from .bim file. 165 people (80 males, 85 females) loaded from .fam. 112 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 53 nonfounders present. Calculating allele frequencies... done. Warning: 179 het. haploid genotypes present (see plink.hh ); many commands treat these as missing. Total genotyping rate is 0.997899. 1430443 variants and 165 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (53 phenotypes are missing.) --check-sex: 23424 Xchr and 0 Ychr variant(s) scanned, 1 problem detected. Report written to plink.sexcheck . Plink reports that one individual has a different sex as determined by the X chromosome, compared to sex assigned in the pedigree file. Inspect plink.sexcheck: head plink.sexcheck FID IID PEDSEX SNPSEX STATUS F 1328 NA06989 2 2 OK -0.01184 1377 NA11891 1 1 OK 1 1349 NA11843 1 1 OK 1 1330 NA12341 2 2 OK -0.01252 1444 NA12739 1 1 OK 1 1344 NA10850 2 2 OK 0.01496 1328 NA06984 1 1 OK 1 1463 NA12877 1 1 OK 1 1418 NA12275 2 2 OK -0.1028 The column headers correspond to the following FID : Family ID IID : Individual ID PEDSEX : Sex as determined in pedigree file (1=male, 2=female) SNPSEX : Sex as determined by X chromosome STATUS : Displays \"PROBLEM\" or \"OK\" for each individual F : The actual X chromosome inbreeding (homozygosity) estimate We now visualize the estimated X chromosome homozygosity for assigned males and females in R: Performed in R gender <- read.table(\"plink.sexcheck\", header=T,as.is=T) png(\"Gender_check.png\") hist(gender[,6],main=\"Gender\", xlab=\"F\") dev.off() png(\"Men_check.png\") male=subset(gender, gender$PEDSEX==1) hist(male[,6],main=\"Men\",xlab=\"F\") dev.off() png(\"Women_check.png\") female=subset(gender, gender$PEDSEX==2) hist(female[,6],main=\"Women\",xlab=\"F\") dev.off() The third histogram shows where the problem lies: one individual is marked as female in the pedigree file, but is male according to the X chromosome. PLINK has marked this case as ``PROBLEM'' in column 5 of plink.sexcheck. We will use grep and awk to print the FID and IID of this person, and remove this person from our data using the --remove flag in PLINK. grep \"PROBLEM\" plink.sexcheck| awk '{print$1,$2}'> sex_discrepancy.txt plink --bfile $FILESET.mind --remove sex_discrepancy.txt --make-bed --out $FILESET.rem","title":"Check for Sex Discrepancies"},{"location":"QC/#limit-data-to-autosomal-snps-only","text":"Most genetic analysis focuses solely on autosomal SNPs, ignoring the sex chromosome or mitochondrial DNA. We will use the --extract flag in PLINK to extract only such SNPs. Before using this flag, we will generate a list of RSids that lie on the autosome (chromosome 1-22), using the awk command. awk '{ if ($1 >= 1 && $1 <= 22) print $2 }' $FILESET.rem.bim > snp_1_22.txt plink --bfile $FILESET.rem --extract snp_1_22.txt --make-bed --out $FILESET.autosome","title":"Limit data to autosomal SNPs only"},{"location":"QC/#perform-maf-check-and-filter-by-maf-threshold","text":"Next, we will check the minor allele frequency (MAF) of each SNP. SNPs with a low MAF are rare. As a result, power to detect their associations with a given phenotype is low. Furthermore, such SNPs are more prone to genotyping errors. The --freq flag in PLINK is used to calculate MAFs. plink --bfile $FILESET.autosome --freq --out MAF_check PLINK reports the following output: $ plink --bfile $FILESET.rem --extract snp_1_22.txt --make-bed --out $FILESET.autosome PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.autosome.log. Options in effect: --bfile HapMap_3_r3_1.rem --extract snp_1_22.txt --make-bed --out HapMap_3_r3_1.autosome 12574 MB RAM detected; reserving 6287 MB for main workspace. 1430443 variants loaded from .bim file. 164 people (80 males, 84 females) loaded from .fam. 112 phenotype values loaded from .fam. --extract: 1398544 variants remaining. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 52 nonfounders present. Calculating allele frequencies... done. Total genotyping rate is 0.998052. 1398544 variants and 164 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (52 phenotypes are missing.) --make-bed to HapMap_3_r3_1.autosome.bed + HapMap_3_r3_1.autosome.bim + HapMap_3_r3_1.autosome.fam ... done. This creates the file MAF_check.frq. Inspect MAF_check.frq: head MAF_check.frq CHR SNP A1 A2 MAF NCHROBS 1 rs2185539 T C 0 224 1 rs11240767 T C 0 224 1 rs3131972 A G 0.1652 224 1 rs3131969 A G 0.1339 224 1 rs1048488 C T 0.1667 222 1 rs12562034 A G 0.1027 224 1 rs12124819 G A 0.2902 224 1 rs4040617 G A 0.1295 224 1 rs2905036 C T 0 224 Again, use R to visualize the MAF distribution. Performed in R maf_freq <- read.table(\"MAF_check.frq\", header =TRUE, as.is=T) png(\"MAF_distribution.png\") hist(maf_freq[,5],main = \"MAF distribution\", xlab = \"MAF\") dev.off() Now, we want to filter any SNPs with a low MAF. The larger your dataset, the higher your power, such that you can set a less conservative MAF threshold. We recommend to throw out all SNPs with MAF < 0.05 for a small data set such as the current one. For a larger dataset What is larger? , this threshold can be relaxed to 0.01. The --maf flag in PLINK filters out SNPs that fall below the given threshold. MAF=0.05 plink --bfile $FILESET.autosome --maf $MAF --make-bed --out $FILESET.maf PLINK reports the following output: ```bash $ plink --bfile $FILESET.autosome --maf $MAF --make-bed --out $FILESET.maf PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.maf.log. Options in effect: --bfile HapMap_3_r3_1.autosome --maf 0.05 --make-bed --out HapMap_3_r3_1.maf 12574 MB RAM detected; reserving 6287 MB for main workspace. 1398544 variants loaded from .bim file. 164 people (80 males, 84 females) loaded from .fam. 112 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 52 nonfounders present. Calculating allele frequencies... done. Total genotyping rate is 0.998052. 325318 variants removed due to minor allele threshold(s) (--maf/--max-maf/--mac/--max-mac). 1073226 variants and 164 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (52 phenotypes are missing.) --make-bed to HapMap_3_r3_1.maf.bed + HapMap_3_r3_1.maf.bim + HapMap_3_r3_1.maf.fam ... done ``` PLINK removed 325,318 SNPs, leaving 1,073,226 variants.","title":"Perform MAF check and filter by MAF Threshold"},{"location":"QC/#view-heterozygosity-distribution-and-remove-outliers","text":"We next check heterozygosity rates for all individuals within the sample. Excessively high or low heterozygosity rates could indicate sample contamination or inbreeding.","title":"View heterozygosity distribution and remove outliers"},{"location":"QC/#generate-a-pruned-subset-of-snps-that-are-in-approximate-linkage-equilibrium","text":"Heterozygosity must be computed in a set of independent SNPs. SNPs that are in linkage disequilibrium violate the independence assumption. Hence, we perform a procedure called ``variant pruning'' to reduce our dataset towards a set of SNPs that are in approximate linkage equilibrium with one another. The --indep-pairwise flag in PLINK performs the pruning procedure. Its syntax is --indep-pairwise ['kb'] . Within each window, PLINK computes the pairwise r^2 between each SNP, removes one of the SNPs (the one with the lowest MAF) if the pairwise r^2 is larger than the r^2 threshold, and repeats until no SNPs within the window have an r^2 above the given threshold. The algorithm will then shift its window, as given by the step size argument, and repeat. We recommend to set a window of 50kb, a step size of 5, and an r^2 threshold of 0.5. plink --bfile $FILESET.maf --indep-pairwise 50 5 0.5 PLINK reports the following output: ```bash plink --bfile $FILESET.maf --indep-pairwise 50 5 0.5 PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to plink.log. Options in effect: --bfile HapMap_3_r3_1.maf --indep-pairwise 50 5 0.5 12574 MB RAM detected; reserving 6287 MB for main workspace. 1073226 variants loaded from .bim file. 164 people (80 males, 84 females) loaded from .fam. 112 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 52 nonfounders present. Calculating allele frequencies... done. Total genotyping rate is 0.998039. 1073226 variants and 164 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (52 phenotypes are missing.) Pruned 67124 variants from chromosome 1, leaving 20596. Pruned 69461 variants from chromosome 2, leaving 20337. Pruned 57330 variants from chromosome 3, leaving 17532. Pruned 50485 variants from chromosome 4, leaving 15644. Pruned 52786 variants from chromosome 5, leaving 16231. Pruned 55187 variants from chromosome 6, leaving 16566. Pruned 44767 variants from chromosome 7, leaving 14337. Pruned 45018 variants from chromosome 8, leaving 13625. Pruned 37678 variants from chromosome 9, leaving 12654. Pruned 43374 variants from chromosome 10, leaving 13851. Pruned 42449 variants from chromosome 11, leaving 12835. Pruned 39534 variants from chromosome 12, leaving 13031. Pruned 30539 variants from chromosome 13, leaving 9951. Pruned 26275 variants from chromosome 14, leaving 8869. Pruned 23752 variants from chromosome 15, leaving 8256. Pruned 24630 variants from chromosome 16, leaving 9189. Pruned 20966 variants from chromosome 17, leaving 8152. Pruned 23347 variants from chromosome 18, leaving 8308. Pruned 13565 variants from chromosome 19, leaving 6097. Pruned 20631 variants from chromosome 20, leaving 7317. Pruned 11312 variants from chromosome 21, leaving 4170. Pruned 10938 variants from chromosome 22, leaving 4530. Pruning complete. 811148 of 1073226 variants removed. Marker lists written to plink.prune.in and plink.prune.out ``` The list of SNP rsIDs kept by the pruning procedure is given by plink.prune.in. The SNPs that are removed (811,148 of 1,073,226 variants), as they were in linkage disequilibrium with one or more of the SNPs in plink.prune.in, are listed in plink.prune.out Inspect plink.prune.in: head plink.prune.in rs1048488 rs12562034 rs12124819 rs4970383 rs1806509 rs28576697 rs13303106 rs13303010 rs2341354 rs35940137 Compute method-of-moments F coefficient estimates The --het flag in PLINK computes observed and expected autosomal homozygous genotype counts for each sample, and reports method-of-moments F coefficient estimates (i.e. (observed hom. count - expected count) / (total observations - expected count)) in the output file. We compute these statistics on the pruned SNPs only. plink --bfile $FILESET.maf --extract plink.prune.in --het --out R_check Show PLINK output: ```bash $ plink --bfile $FILESET .maf --extract plink.prune.in --het --out R_check PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to R_check.log. Options in effect: --bfile HapMap_3_r3_1.maf --extract plink.prune.in --het --out R_check 12574 MB RAM detected; reserving 6287 MB for main workspace. 1073226 variants loaded from .bim file. 164 people (80 males, 84 females) loaded from .fam. 112 phenotype values loaded from .fam. --extract: 262078 variants remaining. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 112 founders and 52 nonfounders present. Calculating allele frequencies... done. Total genotyping rate is 0.997962. 262078 variants and 164 people pass filters and QC. Among remaining phenotypes, 56 are cases and 56 are controls. (52 phenotypes are missing.) --het: 262078 variants scanned, report written to R_check.het . ``` Inspect R_check.het bash head R_check.het FID IID O(HOM) E(HOM) N(NM) F 1328 NA06989 172125 1.727e+05 261511 -0.006857 1377 NA11891 170744 1.711e+05 259125 -0.004518 1349 NA11843 172346 1.73e+05 261907 -0.007276 1330 NA12341 170768 1.725e+05 261235 -0.01998 1444 NA12739 170563 1.704e+05 257944 0.00224 1344 NA10850 173379 1.729e+05 261734 0.005574 1328 NA06984 171767 1.722e+05 260670 -0.004437 1463 NA12877 172550 1.728e+05 261603 -0.002632 1418 NA12275 171259 1.726e+05 261348 -0.0153 The column headers correspond to the following FID : Family ID IID : Within-family ID O(HOM) : Observed number of homozygotes E(HOM) : Expected number of homozygotes N(NM) : Number of non-missing autosomal genotypes F : Method-of-moments F coefficient estimate Let's visualize the heterozygosity distribution in a histogram: Performed in R het <- read.table(\"R_check.het\", head=TRUE) png(\"heterozygosity.png\") het$HET_RATE = (het$\"N.NM.\" - het$\"O.HOM.\")/het$\"N.NM.\" hist(het$HET_RATE, xlab=\"Heterozygosity Rate\", ylab=\"Frequency\", main= \"Heterozygosity Rate\") dev.off() Next, let's create a list of individuals who deviate more than 3 standard deviations from the mean heterozygosity rate. Performed in R het <- read.table(\"R_check.het\", head=TRUE) het$HET_RATE = (het$\"N.NM.\" - het$\"O.HOM.\")/het$\"N.NM.\" het_fail = subset(het, (het$HET_RATE < mean(het$HET_RATE)-3*sd(het$HET_RATE)) | (het$HET_RATE > mean(het$HET_RATE)+3*sd(het$HET_RATE))); het_fail$HET_DST = (het_fail$HET_RATE-mean(het$HET_RATE))/sd(het$HET_RATE); write.table(het_fail, \"fail-het-qc.txt\", row.names=FALSE) Inspect fail-het-qc.txt : bash cat fail-het-qc.txt \"FID\" \"IID\" \"O.HOM.\" \"E.HOM.\" \"N.NM.\" \"F\" \"HET_RATE\" \"HET_DST\" 1330 \"NA12342\" 174790 172100 260519 0.0308 0.329070048633689 -4.69408664581716 1459 \"NA12874\" 175863 173000 261854 0.03269 0.328392921246191 -4.93949187693739 1340 \"NA06994\" 168357 171300 259327 -0.03301 0.350792628611753 3.17863399503596 Three individuals have an excessively low heterozygosity rate. We will remove these heterozygous outliers from the genetic data using PLINK. First, we save the first two columns of fail-het-qc.txt in a seperate file (sed 's/\"// g' is used to remove the quotations marks, such that the file, which was created in R in R becomes compatible with PLINK). Next, we use --remove in PLINK to remove the outliers. sed 's/\"// g' fail-het-qc.txt | awk '{print$1, $2}' > het_fail_ind.txt remove individuals that are heterozygous outliers plink --bfile $FILESET.maf --remove het_fail_ind.txt --make-bed --out $FILESET.het_fix The resulting output: ```bash plink --bfile $FILESET.maf --remove het_fail_ind.txt --make-bed --out $FILESET.het_fix PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to HapMap_3_r3_1.het_fix.log. Options in effect: --bfile HapMap_3_r3_1.maf --make-bed --out HapMap_3_r3_1.het_fix --remove het_fail_ind.txt 12574 MB RAM detected; reserving 6287 MB for main workspace. 1073226 variants loaded from .bim file. 164 people (80 males, 84 females) loaded from .fam. 112 phenotype values loaded from .fam. --remove: 161 people remaining. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 109 founders and 52 nonfounders present. Calculating allele frequencies... done. Total genotyping rate in remaining samples is 0.998098. 1073226 variants and 161 people pass filters and QC. Among remaining phenotypes, 55 are cases and 54 are controls. (52 phenotypes are missing.) --make-bed to HapMap_3_r3_1.het_fix.bed + HapMap_3_r3_1.het_fix.bim + HapMap_3_r3_1.het_fix.fam ... done. ```","title":"Generate a pruned subset of SNPs that are in approximate linkage equilibrium."},{"location":"QC/#transfer-files-to-use-for-population-stratification","text":"Congratulations! You have finished the first tutorial on quality control of genetic data. We will use the cleaned data in the next tutorial to diagnose and correct for population stratification. To set up the data file you created for the next tutorial, create a new directory, and move the final .bim/.bed/.fam files towards. We will also Also make sure PLINK is copied towards this new directory. rename files and copy to population stratification folder mkdir ../2_Population_stratification cp $FILESET.het_fix.bed ../2_Population_stratification/$FILESET.qcout.bed cp $FILESET.het_fix.bim ../2_Population_stratification/$FILESET.qcout.bim cp $FILESET.het_fix.fam ../2_Population_stratification/$FILESET.qcout.fam cp plink.prune.in ../2_Population_stratification cp plink ../2_Population_stratification change directory into population stratification folder cd ../2_Population_stratification","title":"Transfer files to use for population stratification"},{"location":"QC/#further-tips-and-further-reading","text":"","title":"Further tips and further reading"},{"location":"assoc/","text":"Performing Association Analyses \u00b6 In this tutorial, you will learn how to use the HapMap files generated in the previous tutorial named: popstratout (with .bed, .bim, and .fam. extensions) and covar_pca.txt to perform association analysis on a simulated phenotype. We only need to define a few variables for this section of the analysis \u00b6 Before we start, we define two variable. FILE_GWAS refers to the dataset to perform the association analysis on and FILE_COV to the dataset with the relevant covariates (Principal components). FILE_GWAS=popstratout FILE_COV=covar_PCs.txt Perform Association Analyses: \u00b6 We will use Plink to perform association analyses. These types of analyses run M individual regressions, with M the number of SNPs in the .bim file. For case/control data, the simples form of association analysis is performed using the --assoc flag. However, running multivariable regression analyses, one for each SNP, is an attractive way of estimating SNPs associations, correcting for a multitude of control variables. The --linear flag is used for linear regression (apropriate for quantitative traits and possibly binary traits), the --logistic flag for logistic regression (appropriate for binary traits). Below, we show how to perform each of these three types of association analyses on a binary, simulated phenotype (stored in the 6th column of the .fam file). All these analyses are performed M times, for each SNP. As a result, conventional p-values need to be adjusted for multiple hypothesis testing. The --adjust flag performs this correction, and will add different types of multiple hypothesis corrected p-values to the output. --assoc Note the --assoc option does not allow to correct covariates such as principal components (PC's) MDS components, or other variables such as sex and age. We therefore do not recommend to use this type of anlysis, as results will be vulnerable to confounding by population stratification. plink --bfile $FILE_GWAS --assoc --adjust --out assoc_results logistic The --logistic method performs logistic analysis and allows you to include covariates in your association analysis. We include sex as a covariate (which is recommended for many phenotypes), adding sex to the --logistic flag as shown below. Sex will be inferred from the .fam file. We will be using 10 principal components as covariates in this logistic analysis. We use the PCA components calculated from the previous tutorial: covar_PCA.txt. In that tutorial, we showed in the scree plot that the first two PCs should in principal be sufficient to control for population stratification. Nonetheless, we add the first 10 as this is considered the minimum in the literature and controlling for too many PCs is unlikely to have a large effect on the overall results (Price et al., 2006). We use the option hide-covar to only show the additive results of the SNPs in the output file. ```bash plink --bfile $FILE_GWAS --covar $FILE_COV --logistic 'hide-covar' sex --adjust --out logistic_results ``` **Remove NA values, those might give problems generating plots in later steps.** ```bash awk '!/'NA'/' logistic_results.assoc.logistic > logistic_results.assoc_2.logistic ``` --linear In case of a quantitative outcome measure the option --logistic should be replaced by --linear as to perform linear regression analysis. The use of the --assoc option is also possible for quantitative outcome measures (as metioned previously, this option does not allow the use of covariates). Account for Multiple testing \u00b6 There are various way to deal with multiple testing outside of the conventional genome-wide significance threshold of 5.0E-8, below we present a couple: adjust \u00b6 plink --bfile $FILE_GWAS -assoc --adjust --out adjusted_assoc_results grep -oP '(?<==).*' adjusted_assoc_results.log >test.txt sed 's/\\.$//' test.txt >lambda.txt The output file gives a Bonferroni corrected p-value, along with FDR and others. Warning This is a computational intensive step. Further pros and cons of this method, which can be used for association and dealing with multiple testing, are described in our article corresponding to this tutorial (https://www.ncbi.nlm.nih.gov/pubmed/29484742).To reduce computational time, we only perform this test on a subset of the SNPs from chromosome 22. The EMP2 column provides the values for multiple testing corrected p-value. # Perform 1000000 permutations. plink --bfile $FILE_GWAS --assoc --mperm 1000000 --out 1M_perm_result # Order your data, from lowest to highest p-value. sort -gk 4 1M_perm_result.assoc.mperm > sorted_subset.txt # Check ordered permutation results head sorted_subset.txt Generating Manhattan plot in R list.of.packages <- c(\"karyoploteR\") new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])] if(length(new.packages)) install.packages(new.packages) library(\"karyoploteR\") results_log <- read.table(\"logistic_results.assoc_2.logistic\", head=TRUE) results_as <- read.table(\"assoc_results.assoc\", head=TRUE) results_log=data.frame(results_log) results_as=data.frame(results_as) results_log$CHR <- sub(\"^\", \"chr\", results_log$CHR ) ## addchr to column results_as$CHR <- sub(\"^\", \"chr\",results_as$CHR ) results_log=makeGRangesFromDataFrame(results_log,keep.extra.columns=TRUE,ignore.strand=TRUE,seqinfo=NULL,seqnames.field=c(\"seqnames\", \"seqname\",\"chromosome\", \"chrom\",\"chr\", \"chromosome_name\",\"seqid\"),start.field=\"BP\",end.field=c(\"BP\", \"stop\"),strand.field=\"strand\",starts.in.df.are.0based=FALSE) results_as=makeGRangesFromDataFrame(results_as,keep.extra.columns=TRUE,ignore.strand=TRUE,seqinfo=NULL,seqnames.field=c(\"seqnames\", \"seqname\",\"chromosome\", \"chrom\",\"chr\", \"chromosome_name\",\"seqid\"),start.field=\"BP\",end.field=c(\"BP\", \"stop\"),strand.field=\"strand\",starts.in.df.are.0based=FALSE) jpeg(\"manhattan.jpeg\",width=800) kp <- plotKaryotype(plot.type=4, chromosomes=c(\"chr1\", \"chr2\",\"chr3\" ,\"chr4\",\"chr5\",\"chr6\",\"chr7\",\"chr8\",\"chr9\",\"chr10\",\"chr11\",\"chr12\",\"chr13\",\"chr14\",\"chr15\",\"chr16\",\"chr17\",\"chr18\",\"chr19\",\"chr20\",\"chr21\",\"chr22\"),labels.plotter = NULL) kpAddChromosomeNames(kp,srt=45) kpAddLabels(kp, labels = \"results_as\", srt=90, pos=3, r0=autotrack(1,2)) kp <- kpPlotManhattan(kp, data=results_as,points.col = \"2blues\", r0=autotrack(1,2)) kpAddLabels(kp, labels = \"results_log\", srt=90, pos=3, r0=autotrack(2,2)) kp <- kpPlotManhattan(kp, data=results_log,points.col = \"2blues\", r0=autotrack(2,2)) dev.off() Generating QQ plot in R list.of.packages <- c(\"ggplot2\") new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])] if(length(new.packages)) install.packages(new.packages) library(ggplot2) inflation=read.table(\"lambda.txt\") inflation=inflation$V1 rs_log <- read.table(\"logistic_results.assoc_2.logistic\", head=TRUE) rs_as <- read.table(\"assoc_results.assoc\", head=TRUE) ###function for plotting with 95% confidence interval gg_qqplot <- function(ps, ci = 0.95) { n <- length(ps) df <- data.frame( observed = -log10(sort(ps)), expected = -log10(ppoints(n)), clower = -log10(qbeta(p = (1 - ci) / 2, shape1 = 1:n, shape2 = n:1)), cupper = -log10(qbeta(p = (1 + ci) / 2, shape1 = 1:n, shape2 = n:1)) ) log10Pe <- expression(paste(\"Expected -log\"[10], plain(P))) log10Po <- expression(paste(\"Observed -log\"[10], plain(P))) ggplot(df) + geom_ribbon( mapping = aes(x = expected, ymin = clower, ymax = cupper), alpha = 0.1 ) + geom_point(aes(expected, observed), shape = 1, size = 3) + geom_abline(intercept = 0, slope = 1, alpha = 0.5) + geom_line(aes(expected, cupper), linetype = 2, size = 0.5) + geom_line(aes(expected, clower), linetype = 2, size = 0.5) + xlab(log10Pe) + ylab(log10Po) } ##Run--> jpeg(\"QQ-Plot_logistic_COVAR.jpeg\") gg_qqplot(rs_log$P) + theme_bw(base_size = 24) + annotate( geom = \"text\", x = -Inf, y = Inf, hjust = -0.15, vjust = 1 + 0.15 * 3, label = sprintf(\"\u03bb = %.2f\", inflation), size = 8 ) + theme( axis.ticks = element_line(size = 0.5), panel.grid = element_blank() # panel.grid = element_line(size = 0.5, color = \"grey80\") ) dev.off() jpeg(\"QQ-Plot_assoc.jpeg\") gg_qqplot(rs_as$P) + theme_bw(base_size = 24)+ theme(axis.ticks = element_line(size = 0.5), panel.grid = element_blank()) dev.off()","title":"3. Association Analyses"},{"location":"assoc/#performing-association-analyses","text":"In this tutorial, you will learn how to use the HapMap files generated in the previous tutorial named: popstratout (with .bed, .bim, and .fam. extensions) and covar_pca.txt to perform association analysis on a simulated phenotype.","title":"Performing Association Analyses"},{"location":"assoc/#we-only-need-to-define-a-few-variables-for-this-section-of-the-analysis","text":"Before we start, we define two variable. FILE_GWAS refers to the dataset to perform the association analysis on and FILE_COV to the dataset with the relevant covariates (Principal components). FILE_GWAS=popstratout FILE_COV=covar_PCs.txt","title":"We only need to define a few variables for this section of the analysis"},{"location":"assoc/#perform-association-analyses","text":"We will use Plink to perform association analyses. These types of analyses run M individual regressions, with M the number of SNPs in the .bim file. For case/control data, the simples form of association analysis is performed using the --assoc flag. However, running multivariable regression analyses, one for each SNP, is an attractive way of estimating SNPs associations, correcting for a multitude of control variables. The --linear flag is used for linear regression (apropriate for quantitative traits and possibly binary traits), the --logistic flag for logistic regression (appropriate for binary traits). Below, we show how to perform each of these three types of association analyses on a binary, simulated phenotype (stored in the 6th column of the .fam file). All these analyses are performed M times, for each SNP. As a result, conventional p-values need to be adjusted for multiple hypothesis testing. The --adjust flag performs this correction, and will add different types of multiple hypothesis corrected p-values to the output. --assoc Note the --assoc option does not allow to correct covariates such as principal components (PC's) MDS components, or other variables such as sex and age. We therefore do not recommend to use this type of anlysis, as results will be vulnerable to confounding by population stratification. plink --bfile $FILE_GWAS --assoc --adjust --out assoc_results logistic The --logistic method performs logistic analysis and allows you to include covariates in your association analysis. We include sex as a covariate (which is recommended for many phenotypes), adding sex to the --logistic flag as shown below. Sex will be inferred from the .fam file. We will be using 10 principal components as covariates in this logistic analysis. We use the PCA components calculated from the previous tutorial: covar_PCA.txt. In that tutorial, we showed in the scree plot that the first two PCs should in principal be sufficient to control for population stratification. Nonetheless, we add the first 10 as this is considered the minimum in the literature and controlling for too many PCs is unlikely to have a large effect on the overall results (Price et al., 2006). We use the option hide-covar to only show the additive results of the SNPs in the output file. ```bash plink --bfile $FILE_GWAS --covar $FILE_COV --logistic 'hide-covar' sex --adjust --out logistic_results ``` **Remove NA values, those might give problems generating plots in later steps.** ```bash awk '!/'NA'/' logistic_results.assoc.logistic > logistic_results.assoc_2.logistic ``` --linear In case of a quantitative outcome measure the option --logistic should be replaced by --linear as to perform linear regression analysis. The use of the --assoc option is also possible for quantitative outcome measures (as metioned previously, this option does not allow the use of covariates).","title":"Perform Association Analyses:"},{"location":"assoc/#account-for-multiple-testing","text":"There are various way to deal with multiple testing outside of the conventional genome-wide significance threshold of 5.0E-8, below we present a couple:","title":"Account for Multiple testing"},{"location":"assoc/#adjust","text":"plink --bfile $FILE_GWAS -assoc --adjust --out adjusted_assoc_results grep -oP '(?<==).*' adjusted_assoc_results.log >test.txt sed 's/\\.$//' test.txt >lambda.txt The output file gives a Bonferroni corrected p-value, along with FDR and others. Warning This is a computational intensive step. Further pros and cons of this method, which can be used for association and dealing with multiple testing, are described in our article corresponding to this tutorial (https://www.ncbi.nlm.nih.gov/pubmed/29484742).To reduce computational time, we only perform this test on a subset of the SNPs from chromosome 22. The EMP2 column provides the values for multiple testing corrected p-value. # Perform 1000000 permutations. plink --bfile $FILE_GWAS --assoc --mperm 1000000 --out 1M_perm_result # Order your data, from lowest to highest p-value. sort -gk 4 1M_perm_result.assoc.mperm > sorted_subset.txt # Check ordered permutation results head sorted_subset.txt Generating Manhattan plot in R list.of.packages <- c(\"karyoploteR\") new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])] if(length(new.packages)) install.packages(new.packages) library(\"karyoploteR\") results_log <- read.table(\"logistic_results.assoc_2.logistic\", head=TRUE) results_as <- read.table(\"assoc_results.assoc\", head=TRUE) results_log=data.frame(results_log) results_as=data.frame(results_as) results_log$CHR <- sub(\"^\", \"chr\", results_log$CHR ) ## addchr to column results_as$CHR <- sub(\"^\", \"chr\",results_as$CHR ) results_log=makeGRangesFromDataFrame(results_log,keep.extra.columns=TRUE,ignore.strand=TRUE,seqinfo=NULL,seqnames.field=c(\"seqnames\", \"seqname\",\"chromosome\", \"chrom\",\"chr\", \"chromosome_name\",\"seqid\"),start.field=\"BP\",end.field=c(\"BP\", \"stop\"),strand.field=\"strand\",starts.in.df.are.0based=FALSE) results_as=makeGRangesFromDataFrame(results_as,keep.extra.columns=TRUE,ignore.strand=TRUE,seqinfo=NULL,seqnames.field=c(\"seqnames\", \"seqname\",\"chromosome\", \"chrom\",\"chr\", \"chromosome_name\",\"seqid\"),start.field=\"BP\",end.field=c(\"BP\", \"stop\"),strand.field=\"strand\",starts.in.df.are.0based=FALSE) jpeg(\"manhattan.jpeg\",width=800) kp <- plotKaryotype(plot.type=4, chromosomes=c(\"chr1\", \"chr2\",\"chr3\" ,\"chr4\",\"chr5\",\"chr6\",\"chr7\",\"chr8\",\"chr9\",\"chr10\",\"chr11\",\"chr12\",\"chr13\",\"chr14\",\"chr15\",\"chr16\",\"chr17\",\"chr18\",\"chr19\",\"chr20\",\"chr21\",\"chr22\"),labels.plotter = NULL) kpAddChromosomeNames(kp,srt=45) kpAddLabels(kp, labels = \"results_as\", srt=90, pos=3, r0=autotrack(1,2)) kp <- kpPlotManhattan(kp, data=results_as,points.col = \"2blues\", r0=autotrack(1,2)) kpAddLabels(kp, labels = \"results_log\", srt=90, pos=3, r0=autotrack(2,2)) kp <- kpPlotManhattan(kp, data=results_log,points.col = \"2blues\", r0=autotrack(2,2)) dev.off() Generating QQ plot in R list.of.packages <- c(\"ggplot2\") new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])] if(length(new.packages)) install.packages(new.packages) library(ggplot2) inflation=read.table(\"lambda.txt\") inflation=inflation$V1 rs_log <- read.table(\"logistic_results.assoc_2.logistic\", head=TRUE) rs_as <- read.table(\"assoc_results.assoc\", head=TRUE) ###function for plotting with 95% confidence interval gg_qqplot <- function(ps, ci = 0.95) { n <- length(ps) df <- data.frame( observed = -log10(sort(ps)), expected = -log10(ppoints(n)), clower = -log10(qbeta(p = (1 - ci) / 2, shape1 = 1:n, shape2 = n:1)), cupper = -log10(qbeta(p = (1 + ci) / 2, shape1 = 1:n, shape2 = n:1)) ) log10Pe <- expression(paste(\"Expected -log\"[10], plain(P))) log10Po <- expression(paste(\"Observed -log\"[10], plain(P))) ggplot(df) + geom_ribbon( mapping = aes(x = expected, ymin = clower, ymax = cupper), alpha = 0.1 ) + geom_point(aes(expected, observed), shape = 1, size = 3) + geom_abline(intercept = 0, slope = 1, alpha = 0.5) + geom_line(aes(expected, cupper), linetype = 2, size = 0.5) + geom_line(aes(expected, clower), linetype = 2, size = 0.5) + xlab(log10Pe) + ylab(log10Po) } ##Run--> jpeg(\"QQ-Plot_logistic_COVAR.jpeg\") gg_qqplot(rs_log$P) + theme_bw(base_size = 24) + annotate( geom = \"text\", x = -Inf, y = Inf, hjust = -0.15, vjust = 1 + 0.15 * 3, label = sprintf(\"\u03bb = %.2f\", inflation), size = 8 ) + theme( axis.ticks = element_line(size = 0.5), panel.grid = element_blank() # panel.grid = element_line(size = 0.5, color = \"grey80\") ) dev.off() jpeg(\"QQ-Plot_assoc.jpeg\") gg_qqplot(rs_as$P) + theme_bw(base_size = 24)+ theme(axis.ticks = element_line(size = 0.5), panel.grid = element_blank()) dev.off()","title":"adjust"},{"location":"cal_prs/","text":"Calculating and Analysing PRS \u00b6 Background \u00b6 In this section of the tutorial you will use four different software programs to compute PRS from the base and target data that you QC'ed in the previous two sections. The programs are PLINK PRSice-2 LDPred-2 lassosum","title":"Calculating and Analysing PRS"},{"location":"cal_prs/#calculating-and-analysing-prs","text":"","title":"Calculating and Analysing PRS"},{"location":"cal_prs/#background","text":"In this section of the tutorial you will use four different software programs to compute PRS from the base and target data that you QC'ed in the previous two sections. The programs are PLINK PRSice-2 LDPred-2 lassosum","title":"Background"},{"location":"liftover/","text":"Introduction \u00b6 While this tutorial is intended to be as robust as possible, there are some additional processes and considerations that may be needed for individual datasets. This section serves to provide examples of when and how to incorportant various tools and commands that were not necessary for our example HapMap dataset, but are likely to come up in other analyses. Installing LDpred-2 \u00b6 Note The script used here is based on LDpred 2 implemented under bigsnpr version 1.4.7 Note For more details, please refer to LDpred 2's homepage You can install LDpred and its dependencies in R with the following command: install.packages(\"remotes\") library(remotes) remotes::install_github(\"https://github.com/privefl/bigsnpr.git\") Note For mac users, you might need to follow the guide here to be able to install LDpred2 Required Data \u00b6 We assume that you have the following files (or you can download it from here ): File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Warning While we do provide a rough guide on how to perform LDpred on bed files separated into individual chromosomes, this script is untested and extra caution is required 0. Prepare workspace \u00b6 On some server, you might need to first use the following code in order to run LDpred with multi-thread prepare workspace and load bigsnpr library(bigsnpr) options(bigstatsr.check.parallel.blas = FALSE) options(default.nproc.blas = NULL) 1. Read in the phenotype and covariate files \u00b6 read in phenotype and covariates library(data.table) library(magrittr) phenotype <- fread(\"EUR.height\") covariate <- fread(\"EUR.cov\") pcs <- fread(\"EUR.eigenvec\") # rename columns colnames(pcs) <- c(\"FID\",\"IID\", paste0(\"PC\",1:6)) # generate required table pheno <- merge(phenotype, covariate) %>% merge(., pcs) 2. Obtain HapMap3 SNPs \u00b6 LDpred2 authors recommend restricting the analysis to only the HapMap3 SNPs load HapMap3 SNPs info <- readRDS(url(\"https://github.com/privefl/bigsnpr/raw/master/data-raw/hm3_variants.rds\")) 3. Load and transform the summary statistic file \u00b6 Load summary statistic file # Read in the summary statistic file sumstats <- bigreadr::fread2(\"Height.QC.gz\") # LDpred 2 require the header to follow the exact naming names(sumstats) <- c(\"chr\", \"pos\", \"rsid\", \"a1\", \"a0\", \"n_eff\", \"beta_se\", \"p\", \"OR\", \"INFO\", \"MAF\") # Transform the OR into log(OR) sumstats$beta <- log(sumstats$OR) # Filter out hapmap SNPs sumstats <- sumstats[sumstats$rsid%in% info$rsid,] Warning Here, we know the exact ordering of the summary statistics file. However, in many cases, the ordering of the summary statistics differ, thus one must rename the columns according to their actual ordering 3. Calculate the LD matrix \u00b6 Genome Wide bed file # Get maximum amount of cores NCORES <- nb_cores() # Open a temporary file tmp <- tempfile(tmpdir = \"tmp-data\") on.exit(file.remove(paste0(tmp, \".sbk\")), add = TRUE) # Initialize variables for storing the LD score and LD matrix corr <- NULL ld <- NULL # We want to know the ordering of samples in the bed file fam.order <- NULL # preprocess the bed file (only need to do once for each data set) snp_readBed(\"EUR.QC.bed\") # now attach the genotype object obj.bigSNP <- snp_attach(\"EUR.QC.rds\") # extract the SNP information from the genotype map <- obj.bigSNP$map[-3] names(map) <- c(\"chr\", \"rsid\", \"pos\", \"a1\", \"a0\") # perform SNP matching info_snp <- snp_match(sumstats, map) # Assign the genotype to a variable for easier downstream analysis genotype <- obj.bigSNP$genotypes # Rename the data structures CHR <- map$chr POS <- map$pos # get the CM information from 1000 Genome # will download the 1000G file to the current directory (\".\") POS2 <- snp_asGeneticPos(CHR, POS, dir = \".\") # calculate LD for (chr in 1:22) { # Extract SNPs that are included in the chromosome ind.chr <- which(info_snp$chr == chr) ind.chr2 <- info_snp$`_NUM_ID_`[ind.chr] # Calculate the LD corr0 <- snp_cor( genotype, ind.col = ind.chr2, ncores = NCORES, infos.pos = POS2[ind.chr2], size = 3 / 1000 ) if (chr == 1) { ld <- Matrix::colSums(corr0^2) corr <- as_SFBM(corr0, tmp) } else { ld <- c(ld, Matrix::colSums(corr0^2)) corr$add_columns(corr0, nrow(corr)) } } # We assume the fam order is the same across different chromosomes fam.order <- as.data.table(obj.bigSNP$fam) # Rename fam order setnames(fam.order, c(\"family.ID\", \"sample.ID\"), c(\"FID\", \"IID\")) Chromosome separated bed files # Get maximum amount of cores NCORES <- nb_cores() # Open a temporary file tmp <- tempfile(tmpdir = \"tmp-data\") on.exit(file.remove(paste0(tmp, \".sbk\")), add = TRUE) # Initialize variables for storing the LD score and LD matrix corr <- NULL ld <- NULL # We want to know the ordering of samples in the bed file info_snp <- NULL fam.order <- NULL for (chr in 1:22) { # preprocess the bed file (only need to do once for each data set) # Assuming the file naming is EUR_chr#.bed snp_readBed(paste0(\"EUR_chr\",chr,\".bed\")) # now attach the genotype object obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\".rds\")) # extract the SNP information from the genotype map <- obj.bigSNP$map[-3] names(map) <- c(\"chr\", \"rsid\", \"pos\", \"a1\", \"a0\") # perform SNP matching tmp_snp <- snp_match(sumstats[sumstats$chr==chr,], map) info_snp <- rbind(info_snp, tmp_snp) # Assign the genotype to a variable for easier downstream analysis genotype <- obj.bigSNP$genotypes # Rename the data structures CHR <- map$chr POS <- map$pos # get the CM information from 1000 Genome # will download the 1000G file to the current directory (\".\") POS2 <- snp_asGeneticPos(CHR, POS, dir = \".\") # calculate LD # Extract SNPs that are included in the chromosome ind.chr <- which(tmp_snp$chr == chr) ind.chr2 <- tmp_snp$`_NUM_ID_`[ind.chr] # Calculate the LD corr0 <- snp_cor( genotype, ind.col = ind.chr2, ncores = NCORES, infos.pos = POS2[ind.chr2], size = 3 / 1000 ) if (chr == 1) { ld <- Matrix::colSums(corr0^2) corr <- as_SFBM(corr0, tmp) } else { ld <- c(ld, Matrix::colSums(corr0^2)) corr$add_columns(corr0, nrow(corr)) } # We assume the fam order is the same across different chromosomes if(is.null(fam.order)){ fam.order <- as.data.table(obj.bigSNP$fam) } } # Rename fam order setnames(fam.order, c(\"family.ID\", \"sample.ID\"), c(\"FID\", \"IID\")) 4. Perform LD score regression \u00b6 Perform LD score regression df_beta <- info_snp[,c(\"beta\", \"beta_se\", \"n_eff\", \"_NUM_ID_\")] ldsc <- snp_ldsc( ld, length(ld), chi2 = (df_beta$beta / df_beta$beta_se)^2, sample_size = df_beta$n_eff, blocks = NULL) h2_est <- ldsc[[\"h2\"]] 5. Calculate the null R2 \u00b6 Calculate the null R2 (quantitative trait) # Reformat the phenotype file such that y is of the same order as the # sample ordering in the genotype file y <- pheno[fam.order, on = c(\"FID\", \"IID\")] # Calculate the null R2 # use glm for binary trait # (will also need the fmsb package to calculate the pseudo R2) null.model <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~Sex+\", .) %>% as.formula %>% lm(., data = y) %>% summary null.r2 <- null.model$r.squared Calculate the null R2 (binary trait) library(fmsb) # Reformat the phenotype file such that y is of the same order as the # sample ordering in the genotype file y <- pheno[fam.order, on = c(\"FID\", \"IID\")] # Calculate the null R2 # use glm for binary trait # (will also need the fmsb package to calculate the pseudo R2) null.model <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~Sex+\", .) %>% as.formula %>% glm(., data = y, family=binomial) %>% summary null.r2 <- fmsb::NagelkerkeR2(null.model) Important Scripts for binary trait analysis only serve as a reference as we have not simulate any binary traits. In addition, Nagelkerke \\(R^2\\) is biased when there are ascertainment of samples. For more information, please refer to this paper infinitesimal model beta_inf <- snp_ldpred2_inf(corr, df_beta, h2 = h2_est) grid model # Prepare data for grid model p_seq <- signif(seq_log(1e-4, 1, length.out = 17), 2) h2_seq <- round(h2_est * c(0.7, 1, 1.4), 4) grid.param <- expand.grid(p = p_seq, h2 = h2_seq, sparse = c(FALSE, TRUE)) # Get adjusted beta from grid model beta_grid <- snp_ldpred2_grid(corr, df_beta, grid.param, ncores = NCORES) auto model # Get adjusted beta from the auto model multi_auto <- snp_ldpred2_auto( corr, df_beta, h2_init = h2_est, vec_p_init = seq_log(1e-4, 0.9, length.out = NCORES), ncores = NCORES ) beta_auto <- sapply(multi_auto, function(auto) auto$beta_est) 7. Obtain model PRS \u00b6 Using Genome wide bed file \u00b6 infinitesimal model if(is.null(obj.bigSNP)){ obj.bigSNP <- snp_attach(\"EUR.QC.rds\") } genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) pred_inf <- big_prodVec( genotype, beta_inf, ind.row = ind.test, ind.col = info_snp$`_NUM_ID_`) grid model if(is.null(obj.bigSNP)){ obj.bigSNP <- snp_attach(\"EUR.QC.rds\") } genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) pred_grid <- big_prodMat( genotype, beta_grid, ind.col = info_snp$`_NUM_ID_`) auto model if(is.null(obj.bigSNP)){ obj.bigSNP <- snp_attach(\"EUR.QC.rds\") } genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) pred_auto <- big_prodMat(genotype, beta_auto, ind.row = ind.test, ind.col = info_snp$`_NUM_ID_`) # scale the PRS generated from AUTO pred_scaled <- apply(pred_auto, 2, sd) final_beta_auto <- rowMeans(beta_auto[, abs(pred_scaled - median(pred_scaled)) < 3 * mad(pred_scaled)]) pred_auto <- big_prodVec(genotype, final_beta_auto, ind.row = ind.test, ind.col = info_snp$`_NUM_ID_`) Using chromosome separated bed files \u00b6 infinitesimal model pred_inf <- NULL for(chr in 1:22){ obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\".rds\")) genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) # Extract SNPs in this chromosome chr.idx <- which(info_snp$chr == chr) ind.chr <- info_snp$`_NUM_ID_`[chr.idx] tmp <- big_prodVec(genotype, beta_inf[chr.idx], ind.row = ind.test, ind.col = ind.chr) if(is.null(pred_inf)){ pred_inf <- tmp }else{ pred_inf <- pred_inf + tmp } } grid model pred_grid <- NULL for(chr in 1:22){ obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\"_.rds\")) genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) # Extract SNPs in this chromosome chr.idx <- which(info_snp$chr == chr) ind.chr <- info_snp$`_NUM_ID_`[chr.idx] tmp <- big_prodMat( genotype, beta_grid[chr.idx], ind.col = ind.chr) if(is.null(pred_grid)){ pred_grid <- tmp }else{ pred_grid <- pred_grid + tmp } } auto model pred_auto <- NULL for(chr in 1:22){ obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\"_.rds\")) genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) # Extract SNPs in this chromosome chr.idx <- which(info_snp$chr == chr) ind.chr <- info_snp$`_NUM_ID_`[chr.idx] tmp <- big_prodMat(genotype, beta_auto[chr.idx], ind.row = ind.test, ind.col = ind.chr) # scale the PRS generated from AUTO pred_scaled <- apply(tmp, 2, sd) final_beta_auto <- rowMeans(beta_auto[chr.idx, abs(pred_scaled - median(pred_scaled)) < 3 * mad(pred_scaled)]) tmp <- big_prodVec(genotype, final_beta_auto, ind.row = ind.test, ind.col = ind.chr) if(is.null(pred_auto)){ pred_auto <- tmp }else{ pred_auto <- pred_auto + tmp } } 8. Get the final performance of the LDpred models \u00b6 infinitesimal model reg.formula <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~PRS+Sex+\", .) %>% as.formula reg.dat <- y reg.dat$PRS <- pred_inf inf.model <- lm(reg.formula, dat=reg.dat) %>% summary (result <- data.table( infinitesimal = inf.model$r.squared - null.r2, null = null.r2 )) grid model reg.formula <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~PRS+Sex+\", .) %>% as.formula reg.dat <- y max.r2 <- 0 for(i in 1:ncol(pred_grid)){ reg.dat$PRS <- pred_grid[,i] grid.model <- lm(reg.formula, dat=reg.dat) %>% summary if(max.r2 < grid.model$r.squared){ max.r2 <- grid.model$r.squared } } (result <- data.table( grid = max.r2 - null.r2, null = null.r2 )) auto model reg.formula <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~PRS+Sex+\", .) %>% as.formula reg.dat <- y reg.dat$PRS <- pred_auto auto.model <- lm(reg.formula, dat=reg.dat) %>% summary (result <- data.table( auto = auto.model$r.squared - null.r2, null = null.r2 )) How much phenotypic variation does the PRS from each model explain? Infinitesimal = 0.0100 Grid Model = 0.00180 Auto Model = 0.171","title":"Introduction"},{"location":"liftover/#introduction","text":"While this tutorial is intended to be as robust as possible, there are some additional processes and considerations that may be needed for individual datasets. This section serves to provide examples of when and how to incorportant various tools and commands that were not necessary for our example HapMap dataset, but are likely to come up in other analyses.","title":"Introduction"},{"location":"liftover/#installing-ldpred-2","text":"Note The script used here is based on LDpred 2 implemented under bigsnpr version 1.4.7 Note For more details, please refer to LDpred 2's homepage You can install LDpred and its dependencies in R with the following command: install.packages(\"remotes\") library(remotes) remotes::install_github(\"https://github.com/privefl/bigsnpr.git\") Note For mac users, you might need to follow the guide here to be able to install LDpred2","title":"Installing LDpred-2"},{"location":"liftover/#required-data","text":"We assume that you have the following files (or you can download it from here ): File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Warning While we do provide a rough guide on how to perform LDpred on bed files separated into individual chromosomes, this script is untested and extra caution is required","title":"Required Data"},{"location":"liftover/#0-prepare-workspace","text":"On some server, you might need to first use the following code in order to run LDpred with multi-thread prepare workspace and load bigsnpr library(bigsnpr) options(bigstatsr.check.parallel.blas = FALSE) options(default.nproc.blas = NULL)","title":"0. Prepare workspace"},{"location":"liftover/#1-read-in-the-phenotype-and-covariate-files","text":"read in phenotype and covariates library(data.table) library(magrittr) phenotype <- fread(\"EUR.height\") covariate <- fread(\"EUR.cov\") pcs <- fread(\"EUR.eigenvec\") # rename columns colnames(pcs) <- c(\"FID\",\"IID\", paste0(\"PC\",1:6)) # generate required table pheno <- merge(phenotype, covariate) %>% merge(., pcs)","title":"1. Read in the phenotype and covariate files"},{"location":"liftover/#2-obtain-hapmap3-snps","text":"LDpred2 authors recommend restricting the analysis to only the HapMap3 SNPs load HapMap3 SNPs info <- readRDS(url(\"https://github.com/privefl/bigsnpr/raw/master/data-raw/hm3_variants.rds\"))","title":"2. Obtain HapMap3 SNPs"},{"location":"liftover/#3-load-and-transform-the-summary-statistic-file","text":"Load summary statistic file # Read in the summary statistic file sumstats <- bigreadr::fread2(\"Height.QC.gz\") # LDpred 2 require the header to follow the exact naming names(sumstats) <- c(\"chr\", \"pos\", \"rsid\", \"a1\", \"a0\", \"n_eff\", \"beta_se\", \"p\", \"OR\", \"INFO\", \"MAF\") # Transform the OR into log(OR) sumstats$beta <- log(sumstats$OR) # Filter out hapmap SNPs sumstats <- sumstats[sumstats$rsid%in% info$rsid,] Warning Here, we know the exact ordering of the summary statistics file. However, in many cases, the ordering of the summary statistics differ, thus one must rename the columns according to their actual ordering","title":"3. Load and transform the summary statistic file"},{"location":"liftover/#3-calculate-the-ld-matrix","text":"Genome Wide bed file # Get maximum amount of cores NCORES <- nb_cores() # Open a temporary file tmp <- tempfile(tmpdir = \"tmp-data\") on.exit(file.remove(paste0(tmp, \".sbk\")), add = TRUE) # Initialize variables for storing the LD score and LD matrix corr <- NULL ld <- NULL # We want to know the ordering of samples in the bed file fam.order <- NULL # preprocess the bed file (only need to do once for each data set) snp_readBed(\"EUR.QC.bed\") # now attach the genotype object obj.bigSNP <- snp_attach(\"EUR.QC.rds\") # extract the SNP information from the genotype map <- obj.bigSNP$map[-3] names(map) <- c(\"chr\", \"rsid\", \"pos\", \"a1\", \"a0\") # perform SNP matching info_snp <- snp_match(sumstats, map) # Assign the genotype to a variable for easier downstream analysis genotype <- obj.bigSNP$genotypes # Rename the data structures CHR <- map$chr POS <- map$pos # get the CM information from 1000 Genome # will download the 1000G file to the current directory (\".\") POS2 <- snp_asGeneticPos(CHR, POS, dir = \".\") # calculate LD for (chr in 1:22) { # Extract SNPs that are included in the chromosome ind.chr <- which(info_snp$chr == chr) ind.chr2 <- info_snp$`_NUM_ID_`[ind.chr] # Calculate the LD corr0 <- snp_cor( genotype, ind.col = ind.chr2, ncores = NCORES, infos.pos = POS2[ind.chr2], size = 3 / 1000 ) if (chr == 1) { ld <- Matrix::colSums(corr0^2) corr <- as_SFBM(corr0, tmp) } else { ld <- c(ld, Matrix::colSums(corr0^2)) corr$add_columns(corr0, nrow(corr)) } } # We assume the fam order is the same across different chromosomes fam.order <- as.data.table(obj.bigSNP$fam) # Rename fam order setnames(fam.order, c(\"family.ID\", \"sample.ID\"), c(\"FID\", \"IID\")) Chromosome separated bed files # Get maximum amount of cores NCORES <- nb_cores() # Open a temporary file tmp <- tempfile(tmpdir = \"tmp-data\") on.exit(file.remove(paste0(tmp, \".sbk\")), add = TRUE) # Initialize variables for storing the LD score and LD matrix corr <- NULL ld <- NULL # We want to know the ordering of samples in the bed file info_snp <- NULL fam.order <- NULL for (chr in 1:22) { # preprocess the bed file (only need to do once for each data set) # Assuming the file naming is EUR_chr#.bed snp_readBed(paste0(\"EUR_chr\",chr,\".bed\")) # now attach the genotype object obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\".rds\")) # extract the SNP information from the genotype map <- obj.bigSNP$map[-3] names(map) <- c(\"chr\", \"rsid\", \"pos\", \"a1\", \"a0\") # perform SNP matching tmp_snp <- snp_match(sumstats[sumstats$chr==chr,], map) info_snp <- rbind(info_snp, tmp_snp) # Assign the genotype to a variable for easier downstream analysis genotype <- obj.bigSNP$genotypes # Rename the data structures CHR <- map$chr POS <- map$pos # get the CM information from 1000 Genome # will download the 1000G file to the current directory (\".\") POS2 <- snp_asGeneticPos(CHR, POS, dir = \".\") # calculate LD # Extract SNPs that are included in the chromosome ind.chr <- which(tmp_snp$chr == chr) ind.chr2 <- tmp_snp$`_NUM_ID_`[ind.chr] # Calculate the LD corr0 <- snp_cor( genotype, ind.col = ind.chr2, ncores = NCORES, infos.pos = POS2[ind.chr2], size = 3 / 1000 ) if (chr == 1) { ld <- Matrix::colSums(corr0^2) corr <- as_SFBM(corr0, tmp) } else { ld <- c(ld, Matrix::colSums(corr0^2)) corr$add_columns(corr0, nrow(corr)) } # We assume the fam order is the same across different chromosomes if(is.null(fam.order)){ fam.order <- as.data.table(obj.bigSNP$fam) } } # Rename fam order setnames(fam.order, c(\"family.ID\", \"sample.ID\"), c(\"FID\", \"IID\"))","title":"3. Calculate the LD matrix"},{"location":"liftover/#4-perform-ld-score-regression","text":"Perform LD score regression df_beta <- info_snp[,c(\"beta\", \"beta_se\", \"n_eff\", \"_NUM_ID_\")] ldsc <- snp_ldsc( ld, length(ld), chi2 = (df_beta$beta / df_beta$beta_se)^2, sample_size = df_beta$n_eff, blocks = NULL) h2_est <- ldsc[[\"h2\"]]","title":"4. Perform LD score regression"},{"location":"liftover/#5-calculate-the-null-r2","text":"Calculate the null R2 (quantitative trait) # Reformat the phenotype file such that y is of the same order as the # sample ordering in the genotype file y <- pheno[fam.order, on = c(\"FID\", \"IID\")] # Calculate the null R2 # use glm for binary trait # (will also need the fmsb package to calculate the pseudo R2) null.model <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~Sex+\", .) %>% as.formula %>% lm(., data = y) %>% summary null.r2 <- null.model$r.squared Calculate the null R2 (binary trait) library(fmsb) # Reformat the phenotype file such that y is of the same order as the # sample ordering in the genotype file y <- pheno[fam.order, on = c(\"FID\", \"IID\")] # Calculate the null R2 # use glm for binary trait # (will also need the fmsb package to calculate the pseudo R2) null.model <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~Sex+\", .) %>% as.formula %>% glm(., data = y, family=binomial) %>% summary null.r2 <- fmsb::NagelkerkeR2(null.model) Important Scripts for binary trait analysis only serve as a reference as we have not simulate any binary traits. In addition, Nagelkerke \\(R^2\\) is biased when there are ascertainment of samples. For more information, please refer to this paper infinitesimal model beta_inf <- snp_ldpred2_inf(corr, df_beta, h2 = h2_est) grid model # Prepare data for grid model p_seq <- signif(seq_log(1e-4, 1, length.out = 17), 2) h2_seq <- round(h2_est * c(0.7, 1, 1.4), 4) grid.param <- expand.grid(p = p_seq, h2 = h2_seq, sparse = c(FALSE, TRUE)) # Get adjusted beta from grid model beta_grid <- snp_ldpred2_grid(corr, df_beta, grid.param, ncores = NCORES) auto model # Get adjusted beta from the auto model multi_auto <- snp_ldpred2_auto( corr, df_beta, h2_init = h2_est, vec_p_init = seq_log(1e-4, 0.9, length.out = NCORES), ncores = NCORES ) beta_auto <- sapply(multi_auto, function(auto) auto$beta_est)","title":"5. Calculate the null R2"},{"location":"liftover/#7-obtain-model-prs","text":"","title":"7. Obtain model PRS"},{"location":"liftover/#using-genome-wide-bed-file","text":"infinitesimal model if(is.null(obj.bigSNP)){ obj.bigSNP <- snp_attach(\"EUR.QC.rds\") } genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) pred_inf <- big_prodVec( genotype, beta_inf, ind.row = ind.test, ind.col = info_snp$`_NUM_ID_`) grid model if(is.null(obj.bigSNP)){ obj.bigSNP <- snp_attach(\"EUR.QC.rds\") } genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) pred_grid <- big_prodMat( genotype, beta_grid, ind.col = info_snp$`_NUM_ID_`) auto model if(is.null(obj.bigSNP)){ obj.bigSNP <- snp_attach(\"EUR.QC.rds\") } genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) pred_auto <- big_prodMat(genotype, beta_auto, ind.row = ind.test, ind.col = info_snp$`_NUM_ID_`) # scale the PRS generated from AUTO pred_scaled <- apply(pred_auto, 2, sd) final_beta_auto <- rowMeans(beta_auto[, abs(pred_scaled - median(pred_scaled)) < 3 * mad(pred_scaled)]) pred_auto <- big_prodVec(genotype, final_beta_auto, ind.row = ind.test, ind.col = info_snp$`_NUM_ID_`)","title":"Using Genome wide bed file"},{"location":"liftover/#using-chromosome-separated-bed-files","text":"infinitesimal model pred_inf <- NULL for(chr in 1:22){ obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\".rds\")) genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) # Extract SNPs in this chromosome chr.idx <- which(info_snp$chr == chr) ind.chr <- info_snp$`_NUM_ID_`[chr.idx] tmp <- big_prodVec(genotype, beta_inf[chr.idx], ind.row = ind.test, ind.col = ind.chr) if(is.null(pred_inf)){ pred_inf <- tmp }else{ pred_inf <- pred_inf + tmp } } grid model pred_grid <- NULL for(chr in 1:22){ obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\"_.rds\")) genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) # Extract SNPs in this chromosome chr.idx <- which(info_snp$chr == chr) ind.chr <- info_snp$`_NUM_ID_`[chr.idx] tmp <- big_prodMat( genotype, beta_grid[chr.idx], ind.col = ind.chr) if(is.null(pred_grid)){ pred_grid <- tmp }else{ pred_grid <- pred_grid + tmp } } auto model pred_auto <- NULL for(chr in 1:22){ obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\"_.rds\")) genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) # Extract SNPs in this chromosome chr.idx <- which(info_snp$chr == chr) ind.chr <- info_snp$`_NUM_ID_`[chr.idx] tmp <- big_prodMat(genotype, beta_auto[chr.idx], ind.row = ind.test, ind.col = ind.chr) # scale the PRS generated from AUTO pred_scaled <- apply(tmp, 2, sd) final_beta_auto <- rowMeans(beta_auto[chr.idx, abs(pred_scaled - median(pred_scaled)) < 3 * mad(pred_scaled)]) tmp <- big_prodVec(genotype, final_beta_auto, ind.row = ind.test, ind.col = ind.chr) if(is.null(pred_auto)){ pred_auto <- tmp }else{ pred_auto <- pred_auto + tmp } }","title":"Using chromosome separated bed files"},{"location":"liftover/#8-get-the-final-performance-of-the-ldpred-models","text":"infinitesimal model reg.formula <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~PRS+Sex+\", .) %>% as.formula reg.dat <- y reg.dat$PRS <- pred_inf inf.model <- lm(reg.formula, dat=reg.dat) %>% summary (result <- data.table( infinitesimal = inf.model$r.squared - null.r2, null = null.r2 )) grid model reg.formula <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~PRS+Sex+\", .) %>% as.formula reg.dat <- y max.r2 <- 0 for(i in 1:ncol(pred_grid)){ reg.dat$PRS <- pred_grid[,i] grid.model <- lm(reg.formula, dat=reg.dat) %>% summary if(max.r2 < grid.model$r.squared){ max.r2 <- grid.model$r.squared } } (result <- data.table( grid = max.r2 - null.r2, null = null.r2 )) auto model reg.formula <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~PRS+Sex+\", .) %>% as.formula reg.dat <- y reg.dat$PRS <- pred_auto auto.model <- lm(reg.formula, dat=reg.dat) %>% summary (result <- data.table( auto = auto.model$r.squared - null.r2, null = null.r2 )) How much phenotypic variation does the PRS from each model explain? Infinitesimal = 0.0100 Grid Model = 0.00180 Auto Model = 0.171","title":"8. Get the final performance of the LDpred models"},{"location":"plink/","text":"Background \u00b6 On this page, you will compute PRS using the popular genetic analyses tool plink - while plink is not a dedicated PRS software, you can perform every required steps of the C+T approach with plink . This multi-step process is a good way to learn the processes involved in computing PRS, which are typically performed automatically by PRS software. Required Data \u00b6 In the previous sections, we have generated the following files: File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples Update Effect Size \u00b6 When the effect size relates to disease risk and is thus given as an odds ratio (OR), rather than BETA (for continuous traits), then the PRS is computed as a product of ORs. To simplify this calculation, we take the natural logarithm of the OR so that the PRS can be computed using summation instead (which can be back-transformed afterwards). We can obtain the transformed summary statistics with R : Without data.table dat <- read.table(gzfile(\"Height.QC.gz\"), header=T) dat$BETA <- log(dat$OR) write.table(dat, \"Height.QC.Transformed\", quote=F, row.names=F) q() # exit R With data.table library(data.table) dat <- fread(\"Height.QC.gz\") fwrite(dat[,BETA:=log(OR)], \"Height.QC.Transformed\", sep=\"\\t\") q() # exit R Warning Due to rounding of values, using awk to log transform OR can lead to less accurate results. Therefore, we recommend performing the transformation in R or allow the PRS software to perform the transformation directly. Clumping \u00b6 Linkage disequilibrium, which corresponds to the correlation between the genotypes of genetic variants across the genome, makes identifying the contribution from causal independent genetic variants extremely challenging. One way of approximately capturing the right level of causal signal is to perform clumping, which removes SNPs in ways that only weakly correlated SNPs are retained but preferentially retaining the SNPs most associated with the phenotype under study. Clumping can be performed using the following command in plink : plink \\ --bfile EUR.QC \\ --clump-p1 1 \\ --clump-r2 0.1 \\ --clump-kb 250 \\ --clump Height.QC.Transformed \\ --clump-snp-field SNP \\ --clump-field P \\ --out EUR Each of the new parameters corresponds to the following Parameter Value Description clump-p1 1 P-value threshold for a SNP to be included as an index SNP. 1 is selected such that all SNPs are include for clumping clump-r2 0.1 SNPs having \\(r^2\\) higher than 0.1 with the index SNPs will be removed clump-kb 250 SNPs within 250k of the index SNP are considered for clumping clump Height.QC.Transformed Base data (summary statistic) file containing the P-value information clump-snp-field SNP Specifies that the column SNP contains the SNP IDs clump-field P Specifies that the column P contains the P-value information A more detailed description of the clumping process can be found here Note The \\(r^2\\) values computed by --clump are based on maximum likelihood haplotype frequency estimates This will generate EUR.clumped , containing the index SNPs after clumping is performed. We can extract the index SNP ID by performing the following command: awk 'NR!=1{print $3}' EUR.clumped > EUR.valid.snp $3 because the third column contains the SNP ID Note If your target data are small (e.g. N < 500) then you can use the 1000 Genomes Project samples for the LD calculation. Make sure to use the population that most closely reflects represents the base sample. Generate PRS \u00b6 plink provides a convenient function --score and --q-score-range for calculating polygenic scores. We will need three files: The base data file: Height.QC.Transformed A file containing SNP IDs and their corresponding P-values ( $3 because SNP ID is located in the third column; $8 because the P-value is located in the eighth column) awk '{print $3,$8}' Height.QC.Transformed > SNP.pvalue A file containing the different P-value thresholds for inclusion of SNPs in the PRS. Here calculate PRS corresponding to a few thresholds for illustration purposes: echo \"0.001 0 0.001\" > range_list echo \"0.05 0 0.05\" >> range_list echo \"0.1 0 0.1\" >> range_list echo \"0.2 0 0.2\" >> range_list echo \"0.3 0 0.3\" >> range_list echo \"0.4 0 0.4\" >> range_list echo \"0.5 0 0.5\" >> range_list The format of the range_list file should be as follows: Name of Threshold Lower bound Upper Bound Note The threshold boundaries are inclusive. For example, for the 0.05 threshold, we include all SNPs with P-value from 0 to 0.05 , including any SNPs with P-value equal to 0.05 . We can then calculate the PRS with the following plink command: plink \\ --bfile EUR.QC \\ --score Height.QC.Transformed 3 4 12 header \\ --q-score-range range_list SNP.pvalue \\ --extract EUR.valid.snp \\ --out EUR The meaning of the new parameters are as follows: Paramter Value Description score Height.QC.Transformed 3 4 12 header We read from the Height.QC.Transformed file, assuming that the 3 st column is the SNP ID; 4 th column is the effective allele information; the 12 th column is the effect size estimate; and that the file contains a header q-score-range range_list SNP.pvalue We want to calculate PRS based on the thresholds defined in range_list , where the threshold values (P-values) were stored in SNP.pvalue The above command and range_list will generate 7 files: EUR.0.5.profile EUR.0.4.profile EUR.0.3.profile EUR.0.2.profile EUR.0.1.profile EUR.0.05.profile EUR.0.001.profile Note The default formula for PRS calculation in PLINK is: \\[ PRS_j =\\frac{ \\sum_i^NS_i*G_{ij}}{P*M_j} \\] where the effect size of SNP \\(i\\) is \\(S_i\\) ; the number of effect alleles observed in sample \\(j\\) is \\(G_{ij}\\) ; the ploidy of the sample is \\(P\\) (is generally 2 for humans); the total number of SNPs included in the PRS is \\(N\\) ; and the number of non-missing SNPs observed in sample \\(j\\) is \\(M_j\\) . If the sample has a missing genotype for SNP \\(i\\) , then the population minor allele frequency multiplied by the ploidy ( \\(MAF_i*P\\) ) is used instead of \\(G_{ij}\\) . Accounting for Population Stratification \u00b6 Population structure is the principal source of confounding in GWAS and is usually accounted for by incorporating principal components (PCs) as covariates. We can incorporate PCs into our PRS analysis to account for population stratification. Again, we can calculate the PCs using plink : # First, we need to perform prunning plink \\ --bfile EUR.QC \\ --indep-pairwise 200 50 0.25 \\ --out EUR # Then we calculate the first 6 PCs plink \\ --bfile EUR.QC \\ --extract EUR.prune.in \\ --pca 6 \\ --out EUR Note One way to select the appropriate number of PCs is to perform GWAS on the phenotype under study with different numbers of PCs. LDSC analysis can then be performed on the set of GWAS summary statistics and the GWAS that used the number of PCs that gave an LDSC intercept closest to 1 should correspond to that for which population structure was most accurately controlled for. Here the PCs have been stored in the EUR.eigenvec file and can be used as covariates in the regression model to account for population stratification. Important If the base and target samples are collected from different worldwide populations then the results from the PRS analysis may be biased (see Section 3.4 of our papper). Finding the \"best-fit\" PRS \u00b6 The P-value threshold that provides the \"best-fit\" PRS under the C+T method is usually unknown. To approximate the \"best-fit\" PRS, we can perform a regression between PRS calculated at a range of P-value thresholds and then select the PRS that explains the highest phenotypic variance (please see Section 4.6 of our paper on overfitting issues). This can be achieved using R as follows: detail p.threshold <- c(0.001,0.05,0.1,0.2,0.3,0.4,0.5) # Read in the phenotype file phenotype <- read.table(\"EUR.height\", header=T) # Read in the PCs pcs <- read.table(\"EUR.eigenvec\", header=F) # The default output from plink does not include a header # To make things simple, we will add the appropriate headers # (1:6 because there are 6 PCs) colnames(pcs) <- c(\"FID\", \"IID\", paste0(\"PC\",1:6)) # Read in the covariates (here, it is sex) covariate <- read.table(\"EUR.cov\", header=T) # Now merge the files pheno <- merge(merge(phenotype, covariate, by=c(\"FID\", \"IID\")), pcs, by=c(\"FID\",\"IID\")) # We can then calculate the null model (model with PRS) using a linear regression # (as height is quantitative) null.model <- lm(Height~., data=pheno[,!colnames(pheno)%in%c(\"FID\",\"IID\")]) # And the R2 of the null model is null.r2 <- summary(null.model)$r.squared prs.result <- NULL for(i in p.threshold){ # Go through each p-value threshold prs <- read.table(paste0(\"EUR.\",i,\".profile\"), header=T) # Merge the prs with the phenotype matrix # We only want the FID, IID and PRS from the PRS file, therefore we only select the # relevant columns pheno.prs <- merge(pheno, prs[,c(\"FID\",\"IID\", \"SCORE\")], by=c(\"FID\", \"IID\")) # Now perform a linear regression on Height with PRS and the covariates # ignoring the FID and IID from our model model <- lm(Height~., data=pheno.prs[,!colnames(pheno.prs)%in%c(\"FID\",\"IID\")]) # model R2 is obtained as model.r2 <- summary(model)$r.squared # R2 of PRS is simply calculated as the model R2 minus the null R2 prs.r2 <- model.r2-null.r2 # We can also obtain the coeffcient and p-value of association of PRS as follow prs.coef <- summary(model)$coeff[\"SCORE\",] prs.beta <- as.numeric(prs.coef[1]) prs.se <- as.numeric(prs.coef[2]) prs.p <- as.numeric(prs.coef[4]) # We can then store the results prs.result <- rbind(prs.result, data.frame(Threshold=i, R2=prs.r2, P=prs.p, BETA=prs.beta,SE=prs.se)) } # Best result is: prs.result[which.max(prs.result$R2),] q() # exit R quick p.threshold <- c(0.001,0.05,0.1,0.2,0.3,0.4,0.5) phenotype <- read.table(\"EUR.height\", header=T) pcs <- read.table(\"EUR.eigenvec\", header=F) colnames(pcs) <- c(\"FID\", \"IID\", paste0(\"PC\",1:6)) covariate <- read.table(\"EUR.cov\", header=T) pheno <- merge(merge(phenotype, covariate, by=c(\"FID\", \"IID\")), pcs, by=c(\"FID\",\"IID\")) null.r2 <- summary(lm(Height~., data=pheno[,!colnames(pheno)%in%c(\"FID\",\"IID\")]))$r.squared prs.result <- NULL for(i in p.threshold){ pheno.prs <- merge(pheno, read.table(paste0(\"EUR.\",i,\".profile\"), header=T)[,c(\"FID\",\"IID\", \"SCORE\")], by=c(\"FID\", \"IID\")) model <- summary(lm(Height~., data=pheno.prs[,!colnames(pheno.prs)%in%c(\"FID\",\"IID\")])) model.r2 <- model$r.squared prs.r2 <- model.r2-null.r2 prs.coef <- model$coeff[\"SCORE\",] prs.result <- rbind(prs.result, data.frame(Threshold=i, R2=prs.r2, P=as.numeric(prs.coef[4]), BETA=as.numeric(prs.coef[1]), SE=as.numeric(prs.coef[2]))) } print(prs.result[which.max(prs.result$R2),]) q() # exit R with data.table and magrittr library(data.table) library(magrittr) p.threshold <- c(0.001,0.05,0.1,0.2,0.3,0.4,0.5) phenotype <- fread(\"EUR.height\") pcs <- fread(\"EUR.eigenvec\", header=F) %>% setnames(., colnames(.), c(\"FID\", \"IID\", paste0(\"PC\",1:6)) ) covariate <- fread(\"EUR.cov\") pheno <- merge(phenotype, covariate) %>% merge(., pcs) null.r2 <- summary(lm(Height~., data=pheno[,-c(\"FID\", \"IID\")]))$r.squared prs.result <- NULL for(i in p.threshold){ pheno.prs <- paste0(\"EUR.\", i, \".profile\") %>% fread(.) %>% .[,c(\"FID\", \"IID\", \"SCORE\")] %>% merge(., pheno, by=c(\"FID\", \"IID\")) model <- lm(Height~., data=pheno.prs[,-c(\"FID\",\"IID\")]) %>% summary model.r2 <- model$r.squared prs.r2 <- model.r2-null.r2 prs.coef <- model$coeff[\"SCORE\",] prs.result %<>% rbind(., data.frame(Threshold=i, R2=prs.r2, P=as.numeric(prs.coef[4]), BETA=as.numeric(prs.coef[1]), SE=as.numeric(prs.coef[2]))) } print(prs.result[which.max(prs.result$R2),]) q() # exit R Which P-value threshold generates the \"best-fit\" PRS? 0.3 How much phenotypic variation does the \"best-fit\" PRS explain? 0.1638468","title":"PLINK"},{"location":"plink/#background","text":"On this page, you will compute PRS using the popular genetic analyses tool plink - while plink is not a dedicated PRS software, you can perform every required steps of the C+T approach with plink . This multi-step process is a good way to learn the processes involved in computing PRS, which are typically performed automatically by PRS software.","title":"Background"},{"location":"plink/#required-data","text":"In the previous sections, we have generated the following files: File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples","title":"Required Data"},{"location":"plink/#update-effect-size","text":"When the effect size relates to disease risk and is thus given as an odds ratio (OR), rather than BETA (for continuous traits), then the PRS is computed as a product of ORs. To simplify this calculation, we take the natural logarithm of the OR so that the PRS can be computed using summation instead (which can be back-transformed afterwards). We can obtain the transformed summary statistics with R : Without data.table dat <- read.table(gzfile(\"Height.QC.gz\"), header=T) dat$BETA <- log(dat$OR) write.table(dat, \"Height.QC.Transformed\", quote=F, row.names=F) q() # exit R With data.table library(data.table) dat <- fread(\"Height.QC.gz\") fwrite(dat[,BETA:=log(OR)], \"Height.QC.Transformed\", sep=\"\\t\") q() # exit R Warning Due to rounding of values, using awk to log transform OR can lead to less accurate results. Therefore, we recommend performing the transformation in R or allow the PRS software to perform the transformation directly.","title":"Update Effect Size"},{"location":"plink/#clumping","text":"Linkage disequilibrium, which corresponds to the correlation between the genotypes of genetic variants across the genome, makes identifying the contribution from causal independent genetic variants extremely challenging. One way of approximately capturing the right level of causal signal is to perform clumping, which removes SNPs in ways that only weakly correlated SNPs are retained but preferentially retaining the SNPs most associated with the phenotype under study. Clumping can be performed using the following command in plink : plink \\ --bfile EUR.QC \\ --clump-p1 1 \\ --clump-r2 0.1 \\ --clump-kb 250 \\ --clump Height.QC.Transformed \\ --clump-snp-field SNP \\ --clump-field P \\ --out EUR Each of the new parameters corresponds to the following Parameter Value Description clump-p1 1 P-value threshold for a SNP to be included as an index SNP. 1 is selected such that all SNPs are include for clumping clump-r2 0.1 SNPs having \\(r^2\\) higher than 0.1 with the index SNPs will be removed clump-kb 250 SNPs within 250k of the index SNP are considered for clumping clump Height.QC.Transformed Base data (summary statistic) file containing the P-value information clump-snp-field SNP Specifies that the column SNP contains the SNP IDs clump-field P Specifies that the column P contains the P-value information A more detailed description of the clumping process can be found here Note The \\(r^2\\) values computed by --clump are based on maximum likelihood haplotype frequency estimates This will generate EUR.clumped , containing the index SNPs after clumping is performed. We can extract the index SNP ID by performing the following command: awk 'NR!=1{print $3}' EUR.clumped > EUR.valid.snp $3 because the third column contains the SNP ID Note If your target data are small (e.g. N < 500) then you can use the 1000 Genomes Project samples for the LD calculation. Make sure to use the population that most closely reflects represents the base sample.","title":"Clumping"},{"location":"plink/#generate-prs","text":"plink provides a convenient function --score and --q-score-range for calculating polygenic scores. We will need three files: The base data file: Height.QC.Transformed A file containing SNP IDs and their corresponding P-values ( $3 because SNP ID is located in the third column; $8 because the P-value is located in the eighth column) awk '{print $3,$8}' Height.QC.Transformed > SNP.pvalue A file containing the different P-value thresholds for inclusion of SNPs in the PRS. Here calculate PRS corresponding to a few thresholds for illustration purposes: echo \"0.001 0 0.001\" > range_list echo \"0.05 0 0.05\" >> range_list echo \"0.1 0 0.1\" >> range_list echo \"0.2 0 0.2\" >> range_list echo \"0.3 0 0.3\" >> range_list echo \"0.4 0 0.4\" >> range_list echo \"0.5 0 0.5\" >> range_list The format of the range_list file should be as follows: Name of Threshold Lower bound Upper Bound Note The threshold boundaries are inclusive. For example, for the 0.05 threshold, we include all SNPs with P-value from 0 to 0.05 , including any SNPs with P-value equal to 0.05 . We can then calculate the PRS with the following plink command: plink \\ --bfile EUR.QC \\ --score Height.QC.Transformed 3 4 12 header \\ --q-score-range range_list SNP.pvalue \\ --extract EUR.valid.snp \\ --out EUR The meaning of the new parameters are as follows: Paramter Value Description score Height.QC.Transformed 3 4 12 header We read from the Height.QC.Transformed file, assuming that the 3 st column is the SNP ID; 4 th column is the effective allele information; the 12 th column is the effect size estimate; and that the file contains a header q-score-range range_list SNP.pvalue We want to calculate PRS based on the thresholds defined in range_list , where the threshold values (P-values) were stored in SNP.pvalue The above command and range_list will generate 7 files: EUR.0.5.profile EUR.0.4.profile EUR.0.3.profile EUR.0.2.profile EUR.0.1.profile EUR.0.05.profile EUR.0.001.profile Note The default formula for PRS calculation in PLINK is: \\[ PRS_j =\\frac{ \\sum_i^NS_i*G_{ij}}{P*M_j} \\] where the effect size of SNP \\(i\\) is \\(S_i\\) ; the number of effect alleles observed in sample \\(j\\) is \\(G_{ij}\\) ; the ploidy of the sample is \\(P\\) (is generally 2 for humans); the total number of SNPs included in the PRS is \\(N\\) ; and the number of non-missing SNPs observed in sample \\(j\\) is \\(M_j\\) . If the sample has a missing genotype for SNP \\(i\\) , then the population minor allele frequency multiplied by the ploidy ( \\(MAF_i*P\\) ) is used instead of \\(G_{ij}\\) .","title":"Generate PRS"},{"location":"plink/#accounting-for-population-stratification","text":"Population structure is the principal source of confounding in GWAS and is usually accounted for by incorporating principal components (PCs) as covariates. We can incorporate PCs into our PRS analysis to account for population stratification. Again, we can calculate the PCs using plink : # First, we need to perform prunning plink \\ --bfile EUR.QC \\ --indep-pairwise 200 50 0.25 \\ --out EUR # Then we calculate the first 6 PCs plink \\ --bfile EUR.QC \\ --extract EUR.prune.in \\ --pca 6 \\ --out EUR Note One way to select the appropriate number of PCs is to perform GWAS on the phenotype under study with different numbers of PCs. LDSC analysis can then be performed on the set of GWAS summary statistics and the GWAS that used the number of PCs that gave an LDSC intercept closest to 1 should correspond to that for which population structure was most accurately controlled for. Here the PCs have been stored in the EUR.eigenvec file and can be used as covariates in the regression model to account for population stratification. Important If the base and target samples are collected from different worldwide populations then the results from the PRS analysis may be biased (see Section 3.4 of our papper).","title":"Accounting for Population Stratification"},{"location":"plink/#finding-the-best-fit-prs","text":"The P-value threshold that provides the \"best-fit\" PRS under the C+T method is usually unknown. To approximate the \"best-fit\" PRS, we can perform a regression between PRS calculated at a range of P-value thresholds and then select the PRS that explains the highest phenotypic variance (please see Section 4.6 of our paper on overfitting issues). This can be achieved using R as follows: detail p.threshold <- c(0.001,0.05,0.1,0.2,0.3,0.4,0.5) # Read in the phenotype file phenotype <- read.table(\"EUR.height\", header=T) # Read in the PCs pcs <- read.table(\"EUR.eigenvec\", header=F) # The default output from plink does not include a header # To make things simple, we will add the appropriate headers # (1:6 because there are 6 PCs) colnames(pcs) <- c(\"FID\", \"IID\", paste0(\"PC\",1:6)) # Read in the covariates (here, it is sex) covariate <- read.table(\"EUR.cov\", header=T) # Now merge the files pheno <- merge(merge(phenotype, covariate, by=c(\"FID\", \"IID\")), pcs, by=c(\"FID\",\"IID\")) # We can then calculate the null model (model with PRS) using a linear regression # (as height is quantitative) null.model <- lm(Height~., data=pheno[,!colnames(pheno)%in%c(\"FID\",\"IID\")]) # And the R2 of the null model is null.r2 <- summary(null.model)$r.squared prs.result <- NULL for(i in p.threshold){ # Go through each p-value threshold prs <- read.table(paste0(\"EUR.\",i,\".profile\"), header=T) # Merge the prs with the phenotype matrix # We only want the FID, IID and PRS from the PRS file, therefore we only select the # relevant columns pheno.prs <- merge(pheno, prs[,c(\"FID\",\"IID\", \"SCORE\")], by=c(\"FID\", \"IID\")) # Now perform a linear regression on Height with PRS and the covariates # ignoring the FID and IID from our model model <- lm(Height~., data=pheno.prs[,!colnames(pheno.prs)%in%c(\"FID\",\"IID\")]) # model R2 is obtained as model.r2 <- summary(model)$r.squared # R2 of PRS is simply calculated as the model R2 minus the null R2 prs.r2 <- model.r2-null.r2 # We can also obtain the coeffcient and p-value of association of PRS as follow prs.coef <- summary(model)$coeff[\"SCORE\",] prs.beta <- as.numeric(prs.coef[1]) prs.se <- as.numeric(prs.coef[2]) prs.p <- as.numeric(prs.coef[4]) # We can then store the results prs.result <- rbind(prs.result, data.frame(Threshold=i, R2=prs.r2, P=prs.p, BETA=prs.beta,SE=prs.se)) } # Best result is: prs.result[which.max(prs.result$R2),] q() # exit R quick p.threshold <- c(0.001,0.05,0.1,0.2,0.3,0.4,0.5) phenotype <- read.table(\"EUR.height\", header=T) pcs <- read.table(\"EUR.eigenvec\", header=F) colnames(pcs) <- c(\"FID\", \"IID\", paste0(\"PC\",1:6)) covariate <- read.table(\"EUR.cov\", header=T) pheno <- merge(merge(phenotype, covariate, by=c(\"FID\", \"IID\")), pcs, by=c(\"FID\",\"IID\")) null.r2 <- summary(lm(Height~., data=pheno[,!colnames(pheno)%in%c(\"FID\",\"IID\")]))$r.squared prs.result <- NULL for(i in p.threshold){ pheno.prs <- merge(pheno, read.table(paste0(\"EUR.\",i,\".profile\"), header=T)[,c(\"FID\",\"IID\", \"SCORE\")], by=c(\"FID\", \"IID\")) model <- summary(lm(Height~., data=pheno.prs[,!colnames(pheno.prs)%in%c(\"FID\",\"IID\")])) model.r2 <- model$r.squared prs.r2 <- model.r2-null.r2 prs.coef <- model$coeff[\"SCORE\",] prs.result <- rbind(prs.result, data.frame(Threshold=i, R2=prs.r2, P=as.numeric(prs.coef[4]), BETA=as.numeric(prs.coef[1]), SE=as.numeric(prs.coef[2]))) } print(prs.result[which.max(prs.result$R2),]) q() # exit R with data.table and magrittr library(data.table) library(magrittr) p.threshold <- c(0.001,0.05,0.1,0.2,0.3,0.4,0.5) phenotype <- fread(\"EUR.height\") pcs <- fread(\"EUR.eigenvec\", header=F) %>% setnames(., colnames(.), c(\"FID\", \"IID\", paste0(\"PC\",1:6)) ) covariate <- fread(\"EUR.cov\") pheno <- merge(phenotype, covariate) %>% merge(., pcs) null.r2 <- summary(lm(Height~., data=pheno[,-c(\"FID\", \"IID\")]))$r.squared prs.result <- NULL for(i in p.threshold){ pheno.prs <- paste0(\"EUR.\", i, \".profile\") %>% fread(.) %>% .[,c(\"FID\", \"IID\", \"SCORE\")] %>% merge(., pheno, by=c(\"FID\", \"IID\")) model <- lm(Height~., data=pheno.prs[,-c(\"FID\",\"IID\")]) %>% summary model.r2 <- model$r.squared prs.r2 <- model.r2-null.r2 prs.coef <- model$coeff[\"SCORE\",] prs.result %<>% rbind(., data.frame(Threshold=i, R2=prs.r2, P=as.numeric(prs.coef[4]), BETA=as.numeric(prs.coef[1]), SE=as.numeric(prs.coef[2]))) } print(prs.result[which.max(prs.result$R2),]) q() # exit R Which P-value threshold generates the \"best-fit\" PRS? 0.3 How much phenotypic variation does the \"best-fit\" PRS explain? 0.1638468","title":"Finding the \"best-fit\" PRS"},{"location":"plink_visual/","text":"Plotting the Results \u00b6 The PRS results corresponding to a range of P-value thresholds obtained by application of the C+T PRS method (eg. using PLINK or PRSice-2) can be visualised using R as follows: Note We will be using prs.result variable, which was generated in the previous section Without ggplot2 # We strongly recommend the use of ggplot2. Only follow this code if you # are desperate. # Specify that we want to generate plot in EUR.height.bar.png png(\"EUR.height.bar.png\", height=10, width=10, res=300, unit=\"in\") # First, obtain the colorings based on the p-value col <- suppressWarnings(colorRampPalette(c(\"dodgerblue\", \"firebrick\"))) # We want the color gradient to match the ranking of p-values prs.result <- prs.result[order(-log10(prs.result$P)),] prs.result$color <- col(nrow(prs.result)) prs.result <- prs.result[order(prs.result$Threshold),] # generate a pretty format for p-value output prs.result$print.p <- round(prs.result$P, digits = 3) prs.result$print.p[!is.na(prs.result$print.p) & prs.result$print.p == 0 ] <- format(prs.result$P[!is.na(prs.result$print.p) & prs.result$print.p == 0 ], digits = 2) prs.result$print.p <- sub(\"e\", \"*x*10^\", prs.result$print.p) # Generate the axis labels xlab <- expression(italic(P) - value ~ threshold ~ (italic(P)[T])) ylab <- expression(paste(\"PRS model fit: \", R ^ 2)) # Setup the drawing area layout(t(1:2), widths=c(8.8,1.2)) par( cex.lab=1.5, cex.axis=1.25, font.lab=2, oma=c(0,0.5,0,0), mar=c(4,6,0.5,0.5)) # Plotting the bars b<- barplot(height=prs.result$R2, col=prs.result$color, border=NA, ylim=c(0, max(prs.result$R2)*1.25), axes = F, ann=F) # Plot the axis labels and axis ticks odd <- seq(0,nrow(prs.result)+1,2) even <- seq(1,nrow(prs.result),2) axis(side=1, at=b[odd], labels=prs.result$Threshold[odd], lwd=2) axis(side=1, at=b[even], labels=prs.result$Threshold[even],lwd=2) axis(side=1, at=c(0,b[1],2*b[length(b)]-b[length(b)-1]), labels=c(\"\",\"\",\"\"), lwd=2, lwd.tick=0) # Write the p-value on top of each bar text( parse(text=paste( prs.result$print.p)), x = b+0.1, y = prs.result$R2+ (max(prs.result$R2)*1.05-max(prs.result$R2)), srt = 45) # Now plot the axis lines box(bty='L', lwd=2) axis(2,las=2, lwd=2) # Plot the axis titles title(ylab=ylab, line=4, cex.lab=1.5, font=2 ) title(xlab=xlab, line=2.5, cex.lab=1.5, font=2 ) # Generate plot area for the legend par(cex.lab=1.5, cex.axis=1.25, font.lab=2, mar=c(20,0,20,4)) prs.result <- prs.result[order(-log10(prs.result$P)),] image(1, -log10(prs.result$P), t(seq_along(-log10(prs.result$P))), col=prs.result$color, axes=F,ann=F) axis(4,las=2,xaxs='r',yaxs='r', tck=0.2, col=\"white\") # plot legend title title(bquote(atop(-log[10] ~ model, italic(P) - value), ), line=2, cex=1.5, font=2, adj=0) # write the plot to file dev.off() q() # exit R ggplot2 # ggplot2 is a handy package for plotting library(ggplot2) # generate a pretty format for p-value output prs.result$print.p <- round(prs.result$P, digits = 3) prs.result$print.p[!is.na(prs.result$print.p) & prs.result$print.p == 0] <- format(prs.result$P[!is.na(prs.result$print.p) & prs.result$print.p == 0], digits = 2) prs.result$print.p <- sub(\"e\", \"*x*10^\", prs.result$print.p) # Initialize ggplot, requiring the threshold as the x-axis (use factor so that it is uniformly distributed) ggplot(data = prs.result, aes(x = factor(Threshold), y = R2)) + # Specify that we want to print p-value on top of the bars geom_text( aes(label = paste(print.p)), vjust = -1.5, hjust = 0, angle = 45, cex = 4, parse = T ) + # Specify the range of the plot, *1.25 to provide enough space for the p-values scale_y_continuous(limits = c(0, max(prs.result$R2) * 1.25)) + # Specify the axis labels xlab(expression(italic(P) - value ~ threshold ~ (italic(P)[T]))) + ylab(expression(paste(\"PRS model fit: \", R ^ 2))) + # Draw a bar plot geom_bar(aes(fill = -log10(P)), stat = \"identity\") + # Specify the colors scale_fill_gradient2( low = \"dodgerblue\", high = \"firebrick\", mid = \"dodgerblue\", midpoint = 1e-4, name = bquote(atop(-log[10] ~ model, italic(P) - value),) ) + # Some beautification of the plot theme_classic() + theme( axis.title = element_text(face = \"bold\", size = 18), axis.text = element_text(size = 14), legend.title = element_text(face = \"bold\", size = 18), legend.text = element_text(size = 14), axis.text.x = element_text(angle = 45, hjust = 1) ) # save the plot ggsave(\"EUR.height.bar.png\", height = 7, width = 7) q() # exit R An example bar plot generated using ggplot2 In addition, we can visualise the relationship between the \"best-fit\" PRS (which may have been obtained from any of the PRS programs) and the phenotype of interest, coloured according to sex: Without ggplot2 # Read in the files prs <- read.table(\"EUR.0.3.profile\", header=T) height <- read.table(\"EUR.height\", header=T) sex <- read.table(\"EUR.cov\", header=T) # Rename the sex sex$Sex <- as.factor(sex$Sex) levels(sex$Sex) <- c(\"Male\", \"Female\") # Merge the files dat <- merge(merge(prs, height), sex) # Start plotting plot(x=dat$SCORE, y=dat$Height, col=\"white\", xlab=\"Polygenic Score\", ylab=\"Height\") with(subset(dat, Sex==\"Male\"), points(x=SCORE, y=Height, col=\"red\")) with(subset(dat, Sex==\"Female\"), points(x=SCORE, y=Height, col=\"blue\")) q() # exit R ggplot2 library(ggplot2) # Read in the files prs <- read.table(\"EUR.0.3.profile\", header=T) height <- read.table(\"EUR.height\", header=T) sex <- read.table(\"EUR.cov\", header=T) # Rename the sex sex$Sex <- as.factor(sex$Sex) levels(sex$Sex) <- c(\"Male\", \"Female\") # Merge the files dat <- merge(merge(prs, height), sex) # Start plotting ggplot(dat, aes(x=SCORE, y=Height, color=Sex))+ geom_point()+ theme_classic()+ labs(x=\"Polygenic Score\", y=\"Height\") q() # exit R An example scatter plot generated using ggplot2 Programs such as PRSice-2 and bigsnpr include numerous options for plotting PRS results.","title":"4. Visualizing association results"},{"location":"plink_visual/#plotting-the-results","text":"The PRS results corresponding to a range of P-value thresholds obtained by application of the C+T PRS method (eg. using PLINK or PRSice-2) can be visualised using R as follows: Note We will be using prs.result variable, which was generated in the previous section Without ggplot2 # We strongly recommend the use of ggplot2. Only follow this code if you # are desperate. # Specify that we want to generate plot in EUR.height.bar.png png(\"EUR.height.bar.png\", height=10, width=10, res=300, unit=\"in\") # First, obtain the colorings based on the p-value col <- suppressWarnings(colorRampPalette(c(\"dodgerblue\", \"firebrick\"))) # We want the color gradient to match the ranking of p-values prs.result <- prs.result[order(-log10(prs.result$P)),] prs.result$color <- col(nrow(prs.result)) prs.result <- prs.result[order(prs.result$Threshold),] # generate a pretty format for p-value output prs.result$print.p <- round(prs.result$P, digits = 3) prs.result$print.p[!is.na(prs.result$print.p) & prs.result$print.p == 0 ] <- format(prs.result$P[!is.na(prs.result$print.p) & prs.result$print.p == 0 ], digits = 2) prs.result$print.p <- sub(\"e\", \"*x*10^\", prs.result$print.p) # Generate the axis labels xlab <- expression(italic(P) - value ~ threshold ~ (italic(P)[T])) ylab <- expression(paste(\"PRS model fit: \", R ^ 2)) # Setup the drawing area layout(t(1:2), widths=c(8.8,1.2)) par( cex.lab=1.5, cex.axis=1.25, font.lab=2, oma=c(0,0.5,0,0), mar=c(4,6,0.5,0.5)) # Plotting the bars b<- barplot(height=prs.result$R2, col=prs.result$color, border=NA, ylim=c(0, max(prs.result$R2)*1.25), axes = F, ann=F) # Plot the axis labels and axis ticks odd <- seq(0,nrow(prs.result)+1,2) even <- seq(1,nrow(prs.result),2) axis(side=1, at=b[odd], labels=prs.result$Threshold[odd], lwd=2) axis(side=1, at=b[even], labels=prs.result$Threshold[even],lwd=2) axis(side=1, at=c(0,b[1],2*b[length(b)]-b[length(b)-1]), labels=c(\"\",\"\",\"\"), lwd=2, lwd.tick=0) # Write the p-value on top of each bar text( parse(text=paste( prs.result$print.p)), x = b+0.1, y = prs.result$R2+ (max(prs.result$R2)*1.05-max(prs.result$R2)), srt = 45) # Now plot the axis lines box(bty='L', lwd=2) axis(2,las=2, lwd=2) # Plot the axis titles title(ylab=ylab, line=4, cex.lab=1.5, font=2 ) title(xlab=xlab, line=2.5, cex.lab=1.5, font=2 ) # Generate plot area for the legend par(cex.lab=1.5, cex.axis=1.25, font.lab=2, mar=c(20,0,20,4)) prs.result <- prs.result[order(-log10(prs.result$P)),] image(1, -log10(prs.result$P), t(seq_along(-log10(prs.result$P))), col=prs.result$color, axes=F,ann=F) axis(4,las=2,xaxs='r',yaxs='r', tck=0.2, col=\"white\") # plot legend title title(bquote(atop(-log[10] ~ model, italic(P) - value), ), line=2, cex=1.5, font=2, adj=0) # write the plot to file dev.off() q() # exit R ggplot2 # ggplot2 is a handy package for plotting library(ggplot2) # generate a pretty format for p-value output prs.result$print.p <- round(prs.result$P, digits = 3) prs.result$print.p[!is.na(prs.result$print.p) & prs.result$print.p == 0] <- format(prs.result$P[!is.na(prs.result$print.p) & prs.result$print.p == 0], digits = 2) prs.result$print.p <- sub(\"e\", \"*x*10^\", prs.result$print.p) # Initialize ggplot, requiring the threshold as the x-axis (use factor so that it is uniformly distributed) ggplot(data = prs.result, aes(x = factor(Threshold), y = R2)) + # Specify that we want to print p-value on top of the bars geom_text( aes(label = paste(print.p)), vjust = -1.5, hjust = 0, angle = 45, cex = 4, parse = T ) + # Specify the range of the plot, *1.25 to provide enough space for the p-values scale_y_continuous(limits = c(0, max(prs.result$R2) * 1.25)) + # Specify the axis labels xlab(expression(italic(P) - value ~ threshold ~ (italic(P)[T]))) + ylab(expression(paste(\"PRS model fit: \", R ^ 2))) + # Draw a bar plot geom_bar(aes(fill = -log10(P)), stat = \"identity\") + # Specify the colors scale_fill_gradient2( low = \"dodgerblue\", high = \"firebrick\", mid = \"dodgerblue\", midpoint = 1e-4, name = bquote(atop(-log[10] ~ model, italic(P) - value),) ) + # Some beautification of the plot theme_classic() + theme( axis.title = element_text(face = \"bold\", size = 18), axis.text = element_text(size = 14), legend.title = element_text(face = \"bold\", size = 18), legend.text = element_text(size = 14), axis.text.x = element_text(angle = 45, hjust = 1) ) # save the plot ggsave(\"EUR.height.bar.png\", height = 7, width = 7) q() # exit R An example bar plot generated using ggplot2 In addition, we can visualise the relationship between the \"best-fit\" PRS (which may have been obtained from any of the PRS programs) and the phenotype of interest, coloured according to sex: Without ggplot2 # Read in the files prs <- read.table(\"EUR.0.3.profile\", header=T) height <- read.table(\"EUR.height\", header=T) sex <- read.table(\"EUR.cov\", header=T) # Rename the sex sex$Sex <- as.factor(sex$Sex) levels(sex$Sex) <- c(\"Male\", \"Female\") # Merge the files dat <- merge(merge(prs, height), sex) # Start plotting plot(x=dat$SCORE, y=dat$Height, col=\"white\", xlab=\"Polygenic Score\", ylab=\"Height\") with(subset(dat, Sex==\"Male\"), points(x=SCORE, y=Height, col=\"red\")) with(subset(dat, Sex==\"Female\"), points(x=SCORE, y=Height, col=\"blue\")) q() # exit R ggplot2 library(ggplot2) # Read in the files prs <- read.table(\"EUR.0.3.profile\", header=T) height <- read.table(\"EUR.height\", header=T) sex <- read.table(\"EUR.cov\", header=T) # Rename the sex sex$Sex <- as.factor(sex$Sex) levels(sex$Sex) <- c(\"Male\", \"Female\") # Merge the files dat <- merge(merge(prs, height), sex) # Start plotting ggplot(dat, aes(x=SCORE, y=Height, color=Sex))+ geom_point()+ theme_classic()+ labs(x=\"Polygenic Score\", y=\"Height\") q() # exit R An example scatter plot generated using ggplot2 Programs such as PRSice-2 and bigsnpr include numerous options for plotting PRS results.","title":"Plotting the Results"},{"location":"popstrat/","text":"Population Stratification \u00b6 In this tutorial, you will learn how to restrict the quality-controlled HapMap data to individuals that are all of the same genetic ancestry. Individuals of European ancestry will be kept. Most genetic research focuses on one ancestry group at the time, as mixing ancestry groups may lead to population stratification confounding the results reference? . Within this reference group, we will compute clusters that can be used to correct for further confounding due to population stratification. Obtaining the 1000 genome reference data set \u00b6 To construct population clusters from the HapMap data, we will need to compare genomes in this dataset to genomes from a reference panel for which ancestry is known. The reference panel we will use is the 1000 genomes project. You can download the 1000 Genomes data we use for this tutorial using the following command: wget ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20100804/ALL.2of4intersection.20100804.genotypes.vcf.gz Note that, while more recently updated data from the 1000 Genomes project is available, we decided to use this data for two reasons: The most recently updated data set is much larger and separated by chromosome, which makes it more computationally intensive and requires an adjustment in the coding to account for the seperate chromosomes. We are able to sufficiently parse out European samples and account for population stratification using this data set. For details on how to use the most updated set see the Separated by Chromosome section on the Additional Considerations page. The data comes in a zipped vcf format. Note that the download is quite sizeable (~65GB). When working with genetic data, one regularly encounters sizeable files. It is good practice to check the validity of the downloaded files, before working with them further. Sometimes, the distributor of the data will provide a hash value for the file of interest, such that you are able to check the integrity of your downloaded file. You can check the hash value for the 1000 genomes data using this bash command, after downloading. md5sum ALL.2of4intersection.20100804.genotypes.vcf.gz which should return 9de60b62a195455359390d4c951d92d4 . If the value for your download is not the same, your download might be corrupted, and we advise you to download the data again. As in tutorial 1, we use PLINK to perform analysis on this dataset. First, the data will be converted into the plink binary format ( .bim,.bed,.fam ) using plink's --make-bed flag. There is no need to unzip the vcf file, as plink can read and convert gzipped files directly. plink --vcf ALL.2of4intersection.20100804.genotypes.vcf.gz --make-bed --out 1000genomes.genotypes Inspect the resulting .fam and .bim files head 1000genomes.genotypes.fam HG00098 HG00098 0 0 0 -9 HG00100 HG00100 0 0 0 -9 HG00106 HG00106 0 0 0 -9 HG00112 HG00112 0 0 0 -9 HG00114 HG00114 0 0 0 -9 HG00116 HG00116 0 0 0 -9 HG00117 HG00117 0 0 0 -9 HG00118 HG00118 0 0 0 -9 HG00119 HG00119 0 0 0 -9 HG00120 HG00120 0 0 0 -9 head 1000genomes.genotypes.bim 1 rs112750067 0 10327 C T 1 rs117577454 0 10469 G C 1 rs55998931 0 10492 T C 1 rs58108140 0 10583 A G 1 . 0 11508 A G 1 . 0 11565 T G 1 . 0 12783 G A 1 . 0 13116 G T 1 . 0 13327 C G 1 . 0 13980 C T One thing to note from the .fam file is that sex ids are missing, plink will read these as ambiguous observations. The head of the .bim file further shows that some SNPs have no assigned rs-id. We use plink to assign unique identifiers to these SNPs, using the --set-missing-var-ids flag. You are free to construct your own ids, but ensure that they are unique. We build our id using the chromosome number (@), a colon, base-pair position (#), the build (b37), the reference allele, a comma, and the alternative allele. The 1000 genomes data uses the Genome Reference Consortium Human Build 37 (GRCh37). It is important to note that if you are using a different build you will need to adjust the [b37] accordingly. plink --bfile 1000genomes.genotypes --set-missing-var-ids @:#[b37]\\$1,\\$2 --make-bed --out 1000genomes_nomissing.genotypes Our .bim file now looks as follows: head 1000genomes_nomissing.genotypes.bim 1 rs112750067 0 10327 C T 1 rs117577454 0 10469 G C 1 rs55998931 0 10492 T C 1 rs58108140 0 10583 A G 1 1:11508[b37]A,G 0 11508 A G 1 1:11565[b37]G,T 0 11565 T G 1 1:12783[b37]A,G 0 12783 G A 1 1:13116[b37]G,T 0 13116 G T 1 1:13327[b37]C,G 0 13327 C G 1 1:13980[b37]C,T 0 13980 C T QC on 1000 Genomes data. \u00b6 Before we can use the 1000 Genomes data as our reference panel, it is important to perform quality control procedures on this dataset in a similar fashion as outlined in Tutorial 1 . We choose the same parameters as in this tutorial for our SNP missingness threshold, individual missingness threshold, and MAF threshold. FILE_1K=1000genomes_nomissing.genotypes GENO=0.02 INDV=0.02 MAF=0.05 plink --bfile $FILE_1K --geno 0.2 --make-bed --out $FILE_1K.geno.temp plink --bfile $FILE_1K.geno.temp --mind 0.2 --allow-no-sex --make-bed --out $FILE_1K.geno.mind.temp plink --bfile $FILE_1K.geno.mind.temp --geno $GENO --make-bed --out $FILE_1K.geno plink --bfile $FILE_1K.geno --mind $INDV --allow-no-sex --make-bed --out $FILE_1K.geno.mind plink --bfile $FILE_1K.geno.mind --maf $MAF --make-bed --out $FILE_1K.geno.mind.maf 5,808,310 variants and all 629 people pass filters and QC. Extract the variants present in HapMap from the 1000 genomes dataset Our reference panel and our dataset of interest (e.g. the cleaned HapMap data) must consist of the same set of SNPs. For the remainder of this tutorial, we will set the environmental variable FILE_QC to refer to the clean HapMap data (see Tutorial 1 ). The awk '{print$2}' command selects the second column of the bim file to save the rsid values in a text file. We will then use the --extract flag in plink to extract only the SNPs in the 1000 genomes data that are also present in the HapMap data, and vice versa. FILE_QC=HapMap_3_r3_1.qcout awk '{print$2}' \"$FILE_QC.bim\"> QCFILE_SNPs.txt plink --bfile $FILE_1K.geno.mind.maf --extract QCFILE_SNPs.txt --make-bed --out $FILE_1K.geno.mind.maf.extract awk '{print$2}' $FILE_1K.geno.mind.maf.extract.bim > 1kG_SNPs.txt plink --bfile $FILE_QC --extract 1kG_SNPs.txt --make-bed --out $FILE_QC.extract This results in 1,072,511 SNPs that both datasets have in common. Change 1000 Genomes data build to match build of HapMap data \u00b6 Our next goal is to merge the HapMap data with the 1000 genomes data, such that we can compare the (unknown) population clusters of the HapMap data with the (known) population clusters in the 1000 genomes data. Before merging genetic data, however, a couple of steps need to be taken to ensure that the files align. One of these steps is to ensure that both datasets are in the same build. The 1000 genomes data uses the Genome Reference Consortium Human Build 37 (GRCh37), whereas the HapMap data uses the older build 36. This means that, for the same rsids, the base pair position will slightly differ between these two datasets. We will use the --update-map flag in plink to change all base pair positions in the 1000 genomes to the positions as given by the HapMap data. Note Most current genetic datasets use build 37 or 38. If you are working with genetic data that is not into the build of your liking, you may wish to move your dataset to another build entirely at the start of your project. Look at our Liftover tutorial to see how to move data to another build. awk '{print$2,$4}' $FILE_QC.extract.bim > buildmap.txt # buildmap.txt contains one SNP-id and physical position per line. plink --bfile $FILE_1K.geno.mind.maf.extract --update-map buildmap.txt --make-bed --out $FILE_1K.geno.mind.maf.extract.build ??? note: \"The resulting output:\" ```bash PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to 1000genomes_nomissing.genotypes.geno.mind.maf.extract.build.log. Options in effect: --bfile 1000genomes_nomissing.genotypes.geno.mind.maf.extract --make-bed --out 1000genomes_nomissing.genotypes.geno.mind.maf.extract.build --update-map buildmap.txt 12574 MB RAM detected; reserving 6287 MB for main workspace. 1072511 variants loaded from .bim file. 629 people (0 males, 0 females, 629 ambiguous) loaded from .fam. Ambiguous sex IDs written to 1000genomes_nomissing.genotypes.geno.mind.maf.extract.build.nosex . --update-map: 1072511 values updated. Warning: Base-pair positions are now unsorted! Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 629 founders and 0 nonfounders present. Calculating allele frequencies... done. Total genotyping rate is 0.999412. 1072511 variants and 629 people pass filters and QC. Note: No phenotypes present. --make-bed to 1000genomes_nomissing.genotypes.geno.mind.maf.extract.build.bed + 1000genomes_nomissing.genotypes.geno.mind.maf.extract.build.bim + 1000genomes_nomissing.genotypes.geno.mind.maf.extract.build.fam ... done. ``` Merge the HapMap and 1000 Genomes data sets \u00b6 Prior to merging 1000 Genomes data with the data we want to make sure that the files are mergeable, for this we conduct 3 steps: 1) Make sure the reference allele is similar in the HapMap data and the 1000 Genomes Project datasets 2) Resolve strand issues. 3) Remove the SNPs which after the previous two steps still differ between datasets 1) set reference allele The .bim file records the reference allele and the alternative allele at each SNP. These are not necessarily the same in both datasets. In the following commands, we use the --reference-allele flag in PLINK to ensure that the reference alleles in the HapMap data are the same as the reference alleles in 1K genomes. awk '{print$2,$5}' $FILE_1K.geno.mind.maf.extract.build.bim > 1kg_ref-list.txt plink --bfile $FILE_QC.extract --reference-allele 1kg_ref-list.txt --make-bed --out Map-adj # The 1kG_MDS6 and the HapMap-adj have the same reference genome for all SNPs. ??? The resulting output: \" plink --bfile $FILE_QC. extract --reference-allele 1kg_ref-list.txt --make-bed --out Map-adj PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to Map-adj.log. Options in effect: --a1-allele 1kg_ref-list.txt --bfile HapMap_3_r3_1.qcout.extract --make-bed --out Map-adj 12574 MB RAM detected; reserving 6287 MB for main workspace. 1072511 variants loaded from .bim file. 160 people (77 males, 83 females) loaded from .fam. 108 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 108 founders and 52 nonfounders present. Calculating allele frequencies... done. Total genotyping rate is 0.99815. Warning: Impossible A1 allele assignment for variant rs11488462. Warning: Impossible A1 allele assignment for variant rs4648786. Warning: Impossible A1 allele assignment for variant rs12748433. Warning: Impossible A1 allele assignment for variant rs28635343. Warning: Impossible A1 allele assignment for variant rs28456011. Warning: Impossible A1 allele assignment for variant rs6604981. Warning: Impossible A1 allele assignment for variant rs4074196. [...] --a1-allele: 1072511 assignments made. 1072511 variants and 160 people pass filters and QC. Among remaining phenotypes, 54 are cases and 54 are controls. (52 phenotypes are missing.) --make-bed to Map-adj.bed + Map-adj.bim + Map-adj.fam ... done. ``` 2) Resolve strand issues Note that PLINK warns us that some A1 allele assignments are impossible. We can use grep on both files to see why one of these SNPs looks different in both files: grep rs11488462 $FILE_1K.geno.mind.maf.extract.build.bim 1 rs11488462 0 1343243 C T grep rs11488462 $FILE_QC.extract.bim 1 rs11488462 0 1343243 G A It is clear that in these cases, flipping the allele in the HapMap data from G to A is of no use, since the reference allele at this SNP in the 1K genomes is C. How could a given SNP have a different reference allele as well as a different alternative allele? These differences are due to strand issues. The reference allele is represented by a nucleotide, but each nucleotide has an underlying complement that is present on the other DNA strand, where C complements G and T complements A. Hence, if different strands were genotyped in different samples, the same reference allele could be labelled by a C (T) in the one sample, and by a G (A) in the other. In the example above, the issue is easily resolved by flipping the reference and alternative allele in the HapMap data to their complimentary nucleotides: G to C and A to T, thus aligning this particular SNP with the 1K data. We resolve these strand issues for all SNPs in which PLINK could not align reference alleles using the --flip flag in PLINK, which will flip a given list of rsids towards its complimentary strand. Before we can use the --flip flag in PLINK, we have to create a list of SNP rsids that consists only of the conflicting strands that need flipping. We do this in the first three lines of the following code, where we use ``sort | uniq'' to extract only those lines that are not the same in the .bim files of HapMap and 1K Genomes in terms of the combination of rsid, reference allele, and alternative alleles. In the fourth line, we keep only the rsid, and keep each rsid once to obtain the list of rsid's which strands we have to flip in the HapMap data. awk '{print$2,$5,$6}' $FILE_1K.geno.mind.maf.extract.build.bim > 1kGMDS_strand_tmp awk '{print$2,$5,$6}' Map-adj.bim > Map-adj_tmp sort 1kGMDS_strand_tmp Map-adj_tmp |uniq -u > all_differences.txt awk '{print$1}' all_differences.txt | sort -u > flip_list.txt plink --bfile Map-adj --flip flip_list.txt --reference-allele 1kg_ref-list.txt --make-bed --out corrected_map Check for SNPs which are still problematic after they have been flipped. Are both datasets fully aligned now? Unfortunately not. Let's again extract SNPs that are not the same in terms of their rsid, reference allele, and alternative allele in .bim files of both the HapMap data and the 1K genomes data. awk '{print$2,$5,$6}' corrected_map.bim > corrected_map_tmp sort 1kGMDS_strand_tmp corrected_map_tmp |uniq -u > uncorresponding_SNPs.txt wc -l uncorresponding_SNPs.txt There are still 53 SNPs that do not have the same reference allele and alternative allele in both datasets. Let's again look at one of these SNPs in both .bim files to get a sense of what is going on. head uncorresponding_SNPs.txt rs10060593 A G rs10060593 A T rs10083559 T C rs10083559 T G rs10116901 C A rs10116901 C T rs11524965 T C rs11524965 T G rs11687477 A C rs11687477 T C Clearly, flipping strand issues has not aligned the SNPs from both datasets here. Since it only concerns a small number of SNPs, it is the safest to throw them away, rather than spend more time on trying to resolve these remaining alignment issues. 3) Remove problematic SNPs from your data and from the 1000 Genomes. Here, we want to remove both SNPs that still don't match between data sets. awk '{print$1}' uncorresponding_SNPs.txt | sort -u > SNPs_for_exclusion.txt plink --bfile corrected_map --exclude SNPs_for_exclusion.txt --make-bed --out $FILE_QC.extract.rem plink --bfile $FILE_1K.geno.mind.maf.extract.build --exclude SNPs_for_exclusion.txt --make-bed --out $FILE_1K.geno.mind.maf.extract.build.rem Merge HapMap data with 1000 Genomes Data \u00b6 plink --bfile $FILE_QC.extract.rem --bmerge $FILE_1K.geno.mind.maf.extract.build.rem --allow-no-sex --make-bed --out merged After merging the data, we want to do a final removal of ambiguous (A/T and G/C) SNPs. A/T and G/C loci are unable to be strand-resolved and for this are routinely removed. We use a script from https://github.com/eatkinson/Post-QC: find_cg_at_snps.py with some minor modifications to be compatible with python3. For python2 versions change \"open(bimfile)\" to file(bimfile). wget https://github.com/eatkinson/Post-QC/raw/master/find_cg_at_snps.py sed -i -e 's/file(bimfile)/open(bimfile)/g' find_cg_at_snps.py ##for compatibility in python3 sed -i -e 's/ line\\[1\\]/(line\\[1\\])/g' find_cg_at_snps.py ##for compatibility in python3 python find_cg_at_snps.py merged.bim > ATCGsites plink --bfile merged --exclude ATCGsites --make-bed --out MDS_merge2 Perform MDS and PCA on Map-CEU data anchored by 1000 Genomes data using a set of pruned SNPs \u00b6 Our next goal is to perform a cluster analysis on the merged dataset, and determine which clusters belong to which underlying ancestry group. To this end, we need to know the population to which 1000 genomes individuals belong, which is stored in the .PANEL file that we download here: Download the file with population information of the 1000 genomes dataset. wget ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20100804/20100804.ALL.panel The file 20130502.ALL.panel contains population codes of the individuals of 1000 genomes. Convert population codes into superpopulation codes (i.e., AFR,AMR,ASN, and EUR). The .PANEL file contains information on the population that each 1K genomes individual belongs too. We extract family ID and individual ID (which are the same for 1K genomes). and population into a seperate file. For the HapMap data we create a similar file, but we substitute the population info with a flag labeled \"OWN\", as population information for HapMap individuals is unknown. We next change the detailed 1K genomes population categories to less precise superpopulation codes reference? , using sed, a bash command which let's you search and replace text. The last line in this block of code concatenates (using cat) the ``superpopulated'' 1K genomes racefile with the HapMap racefile, for which race is flagged as unknown, and adds a header using sed. awk '{print$1,$1,$2}' 20100804.ALL.panel > race_1kG.txt Create a racefile of your own data. awk '{print$1,$2,\"OWN\"}' $FILE_QC.extract.rem.fam > racefile_own.txt make txt file of concatenated racefiles sed 's/JPT/ASN/g' race_1kG.txt>race_1kG2.txt sed 's/ASW/AFR/g' race_1kG2.txt>race_1kG3.txt sed 's/CEU/EUR/g' race_1kG3.txt>race_1kG4.txt sed 's/CHB/ASN/g' race_1kG4.txt>race_1kG5.txt sed 's/CHD/ASN/g' race_1kG5.txt>race_1kG6.txt sed 's/YRI/AFR/g' race_1kG6.txt>race_1kG7.txt sed 's/LWK/AFR/g' race_1kG7.txt>race_1kG8.txt sed 's/TSI/EUR/g' race_1kG8.txt>race_1kG9.txt sed 's/MXL/AMR/g' race_1kG9.txt>race_1kG10.txt sed 's/GBR/EUR/g' race_1kG10.txt>race_1kG11.txt sed 's/FIN/EUR/g' race_1kG11.txt>race_1kG12.txt sed 's/CHS/ASN/g' race_1kG12.txt>race_1kG13.txt sed 's/PUR/AMR/g' race_1kG13.txt>race_1kG14.txt cat race_1kG14.txt racefile_own.txt | sed -e '1i\\FID IID race' > racefile.txt Perform PCA \u00b6 We next compute the first 10 principal components of the combined genetic data. These principal components correlate with genetic ancestry. Again, we first clump the data to obtain a set of SNPs that is approximately in linkage equilibrium. The second line in the code below performs the principal component analysis on the extracted set of approximately independent SNPs, calculating the first ten PCs, and adding a `header' to the resulting output file (PCA_MERGE.eigenvec) using the appropriate option. Extracting ancestry information from principal components values. \u00b6 To understand how values of the PCs map into ancestry information, we plot the first two PC values against each other, giving datapoints from different population groups a different color. Perform PCA Clustering plink --bfile MDS_merge2 --indep-pairwise 50 5 0.5 plink --bfile MDS_merge2 --extract plink.prune.in --make-bed --pca 10 'header' --out PCA_MERGE Create PCA-plot Performed in R library(dplyr) data<- read.table(file=\"PCA_MERGE.eigenvec\",header=TRUE) eigenval<-read.table(file=\"PCA_MERGE.eigenval\",header=F) race<- read.table(file=\"racefile.txt\",header=TRUE) datafile<- merge(data,race,by=c(\"IID\",\"FID\")) datafile=datafile %>% filter(datafile$race %in% c(\"EUR\",\"ASN\",\"AMR\",\"AFR\",\"OWN\")) head(datafile) pve = eigenval/sum(eigenval)*100 library(ggplot2) library(gridExtra) png(\"pcagg.png\",units=\"in\",width=13,height=7,res=300) b <- ggplot(datafile, aes(PC1, PC2, col = race,group=race)) b <- b+ geom_point(size = 2) b <-b + xlab(paste0(\"PC1 (\", signif(pve[1,1],3), \"%)\")) + ylab(paste0(\"PC2 (\", signif(pve[2,1], 3), \"%)\"))+ theme(text = element_text(size=13),legend.text=element_text(size=13),legend.title = element_blank(),legend.position='top')+guides(color = guide_legend(override.aes = list(size=8))) c <-ggplot(datafile, aes(PC1, PC2, col = race,group=race)) c <-c+ggplot2::stat_ellipse( geom = \"path\", position = \"identity\", show.legend = NA, size=2, inherit.aes = TRUE, type = \"t\", level = 0.95, segments = 51 ) c <- c + xlab(paste0(\"PC1 (\", signif(pve[1,1],3), \"%)\")) + ylab(paste0(\"PC2 (\", signif(pve[2,1], 3), \"%)\"))+ theme(text = element_text(size=13),legend.title = element_blank(),legend.position = \"none\") grid.arrange(b, c, ncol=2) dev.off() The resulting plot clearly seperates Europeans, Africans, Americans, and Asians load differently on the first two PCs. The HapMap data that we use here clearly corresponds with the European ancestry cluster, but when using your own datasets you may find that your own dataset overlaps with multiple ancestry cluster from 1K genomes, and filter your data accordingly. As a last step, we filter on the observed values in which Europeans fall to get a list of European sample ids. awk '{ if ($3 <-0.005 && $4 <-0.005) print $1,$2 }' PCA_MERGE.eigenvec > EUR_samp_PC.txt Perform MDS Perform MDS \u00b6 We perform MDS clustering to perform unique clusters of the genomic data that overlap with ancestry information. We only use the list of approximately independent SNPs that was determined using the clumping algorithm in Tutorial 1 . In the following two lines of code, the --genome option prepares the data by calculating pairwise identity by state (IBS) for each individual. The resulting file, MDS_merge2.genome , is used as input in the next command, where the identity by state is used to calculate 10 MDS clusters. Perform MDS Clustering plink --bfile MDS_merge2 --extract plink.prune.in --genome --out MDS_merge2 plink --bfile MDS_merge2 --read-genome MDS_merge2.genome --cluster --mds-plot 10 --out MDS_merge2 The resulting output, MDS_merge2.mds, gives us the value of the first 10 MDS clusters for each individual in the stacked HapMap and 1K genomes data. Extracting ancestry information from MDS cluster values. \u00b6 To understand how values of the MDS clusters map into ancestry information, we plot the first two MDS cluster values against each other, giving datapoints from different population groups a different color. Create MDS-plot Performed in R library(ggplot2) library(gridExtra) data<- read.table(file=\"MDS_merge2.mds\",header=TRUE) race<- read.table(file=\"racefile.txt\",header=TRUE) datafile<- merge(data,race,by=c(\"IID\",\"FID\")) head(datafile) png(\"MDS.png\",units=\"in\",width=13,height=7,res=300) b <- ggplot(datafile, aes(C1, C2, col = race,group=race)) b <- b + geom_point(size = 2) b <- b + xlab(\"MDS1\") + ylab(\"MDS2\")+ theme(text = element_text(size=13),legend.text=element_text(size=13),legend.title = element_blank(),legend.position='top')+guides(color = guide_legend(override.aes = list(size=8))) c <-ggplot(datafile, aes(C1, C2, col = race,group=race)) c <-c+ggplot2::stat_ellipse( geom = \"path\", position = \"identity\", show.legend = NA, size=2, inherit.aes = TRUE, type = \"t\", level = 0.95, segments = 51 ) c <- c + xlab(\"MDS1\") + ylab(\"MDS2\")+ theme(text = element_text(size=13),legend.title = element_blank(),legend.position = \"none\") grid.arrange(b, c, ncol=2) dev.off() ## Run Admixture algorithm Note You must install the ADMIXTURE software here . If you are running on a conda environment you can install the admixture software using the following command: conda install -c bioconda admixture Concatenate racefiles. awk '{print$1,$2,\"-\"}' $FILE_QC.extract.rem.fam > racefile_own.txt cat racefile_own.txt race_1kG14.txt | sed -e '1i\\FID IID race' > MDS_merge2.pop sed -i -e \"1d\" MDS_merge2.pop #removes header cut -d \" \" -f 3- MDS_merge2.pop >temp.txt make popfile for admixture script mv temp.txt MDS_merge2.pop run admixture script admixture --supervised ./MDS_merge2.bed 4 > log_merge_admixture.out Warning This step can take a very long time. Running overnight may be neccessary. After admixture is done running, you can prepare the files for plotting. You need to rename MDS_merge2.pop to ind2pop.txt and then make the pong_filemap file with the following format: k4r1 4 locationofQfile/MDS_merge2.12.Q The first column has the letter \"k\" denoting clusters and \"r\" denoting run number. This column can be any unique ID for the run as long as there is at least one letter and one number. The second column denotes the number of clusters. The third column should have the full path to the Q file output from the admixture command. Performed in R R tbl=read.table(\"MDS_merge2.12.Q\") popGroups = read.table(\"MDS_merge2.pop\") fam=read.table(\"MDS_merge2.fam\") mergedAdmWithPopGroups = cbind(tbl, popGroups) ordered = mergedAdmWithPopGroups #ceu v5 #tsi v12 #fin v2 #gbr v1 ordered$EUR=ordered$V5+ordered$V12+ordered$V2+ordered$V1 fam$race=ordered$EUR ids1=fam$V1[which(fam$race>0.8)] ids=data.frame(ids1) ids$ids2=fam$V2[which(fam$race>0.8)] ##list of europeans to extract from files write.table(ids,'europeans.txt',quote = FALSE,row.names = FALSE,col.names=FALSE) ordered$V5=c() ordered$V12=c() ordered$V2=c() ordered$V1=c() ordered=ordered[order(-ordered$EUR),] #barplot(t(as.matrix(subset(ordered, select=c(\"YRI\",\"ASW\",\"CHB\",\"CHS\",\"EUR\",\"JPT\",\"LWK\",\"MXL\",\"PUR\")))), col=rainbow(12), border=NA,names.arg=popnew, las=2,cex.names=0.1) pdf('admixture.pdf') barplot(t(as.matrix(subset(ordered, select=c(\"V3\" , \"V4\" , \"V6\" , \"V7\" ,\"V8\" ,\"V9\" ,\"V10\", \"V11\",\"EUR\")))), col=rainbow(9), border=NA, las=2,ylab=\"Percent Pop\",xaxt=\"n\",xlab='<---- Direction of more European') dev.off() An example plot generated using pong software. The code above can also make a plot but it does not directly label the groups like pong does. The output of this script prints out a text file europeans.txt with a list of all the individuals that are found to be 80% or more European. Exclude ethnic outliers. \u00b6 Select individuals in your own data below cut-off thresholds. The cut-off levels are not fixed thresholds but have to be determined based on the visualization of the first two PCS. To exclude ethnic outliers, the thresholds need to be set around the cluster of population of interest. Find min and max value cut offs for European individuals. data<- read.table(file=\"PCA_MERGE.eigenvec\",header=TRUE) eigenval<-read.table(file=\"PCA_MERGE.eigenval\",header=F) race<- read.table(file=\"racefile.txt\",header=TRUE) datafile<- merge(data,race,by=c(\"IID\",\"FID\")) datafile=datafile %>% filter(datafile$race %in% c(\"EUR\",\"ASN\",\"AMR\",\"AFR\",\"OWN\")) PC1max=max(datafile[datafile$race=='EUR',]$PC1) PC1min=min(datafile[datafile$race=='EUR',]$PC1) PC2max=max(datafile[datafile$race=='EUR',]$PC2) PC2min=min(datafile[datafile$race=='EUR',]$PC2) table <- matrix(c(PC1min,PC1max,PC2min,PC2max), ncol=1) colnames(table)=c('summary') rownames(table) <- c('PC1min', 'PC1max','PC2min', 'PC2max') table=data.frame(table) table$summary=as.numeric(table$summary) write.table(table,'eursum.txt',col.names=F,quote=F) file=eursum.txt cat \"$file\" ##to view file awk '{ if ($3 > -0.0255362 && $3 < -0.0155058 && $4 > -0.0295579 && $4 < -0.0194123) print $1,$2 }' PCA_MERGE.eigenvec > EUR_PCA_MERGE plink --bfile $FILE_QC --keep EUR_PCA_MERGE --make-bed --out $FILE_QC.euro Exclude ethnic outliers \u00b6 We use plink to only include those that were marked as Europeans, as confirmed by the PC analysis. The variable $FILE_Euro could also be used to define a European sample resulting from an admixture or MDS analysis. FILE_Euro=EUR_samp_PC.txt plink --bfile $FILE_QC --keep $FILE_Euro--make-bed --out $FILE_QC.euro Hardy Weinburg equilibrium filter on controls \u00b6 Now that we have restricted our dataset to Europeans only, we can finalize an import quality control step that was not included in tutorial 1. Namely, we filter out all SNPs that are not in Hardy-Weinberg equilibrium. SNPs that significantly deviate from HWE might be under evolutionary pressure, but also often point to genotyping errors. We suggest to exclude all SNPs that deviate from HWE with a p-value smaller than 1e-6. Note that SNPs which are under pressure of evolutionary selection may be of interest, for example when looking for markers associated with lethal disease. When investigating such binary phenotypes, a lower (i.e. less conservative) HWE threshold could be chosen in cases, but not in controls. HWE_CONTROL=1e-6 plink --bfile $FILE_QC.euro --hwe $HWE_CONTROL --make-bed --out $FILE_QC.euro.hwe_control All SNPs in the HapMap data are already in HWE. No SNPs were excluded. Filter founders from dataset \u00b6 This excludes all samples with at least one known parental ID in the current analysis plink --bfile $FILE_QC.euro.hwe_control --filter-founders --make-bed --out $FILE_QC.euro.hwe_control.found 52 individuals are removed. 108 remain. Check for cryptic relatedness with plink2 \u00b6 We use KING's method (implemented in Plink2) to filter out individuals that are more closely related than third cousins. KING's method is not available in plink, but is available in plink2 (which is still in alpha). The flag --make-king-table makes a table with kinship estimations between all individuals within the dataset. First degree relations (parent-child or siblings) have an expected kinship value of 0.25 using KING's method. Second-degree relations have a value of 0.125, third-degree relations a value of 0.0625, etc. The flag --king-table-filter can be used to only maintain relationships within the data that surpass a certain kinship threshold. We want to exclude one individual from each pair that is second-degree related or more. Therefore, we set our filter at 0.0884: The geometric mean between second and third degree relations. #http://s3.amazonaws.com/plink2-assets/alpha2/plink2_linux_x86_64.zip #unzip plink2_linux_x86_64.zip ./plink2 --bfile $FILE_QC.euro.hwe_control.found --make-king-table --king-table-filter 0.0884 sed 's/^#//' plink2.kin0 > kin.txt #Replaces the hashtag in the header of the king-table so R can read it easily. ??? note: \"The resulting output:\" cat kin.txt FID1 ID1 FID2 ID2 NSNP HETHET IBS0 KINSHIP 1454 NA12813 13291 NA07045 1176687 0.189994 0.0174787 0.236928 These two individuals are first-degree relatives, as their estimated kinship is close to 0.25, but were not identified as such in the data. We will remove one of these individuals. We could do this at random, but a better approach is to remove the individual with most missing SNPs. plink --bfile $FILE_QC.euro.hwe_control.found --missing In R, run the following script to remove the individuals with the highest amount of missingness in each related pair. === \"Performed in R\" ```{r} data<- read.table(file=\"kin.txt\",header=TRUE) data_related=data[,c(1,2,3,4)] missing <- read.table(\"plink.imiss\", header =TRUE, as.is=T) FID1=data_related[,c(1,2)] FID2=data_related[,c(3,4)] FID1$index=row.names(FID1) FID2$index=row.names(FID2) FID1$IID=FID1$ID1 FID2$IID=FID2$ID2 FID1[,1:2]=c() FID2[,1:2]=c() FID1=merge(FID1,missing,by=\"IID\") FID2=merge(FID2,missing,by=\"IID\") FID1[,4:6]=c() FID2[,4:6]=c() FID1$index=as.numeric(FID1$index) FID2$index=as.numeric(FID2$index) q=c(setdiff(FID2$index, FID1$index), setdiff(FID1$index, FID2$index)) if (length(q) != 0) { FID1=FID1[!(FID1$index==q),] FID2=FID2[!(FID2$index==q),] } FID1=FID1[order(FID1[,2]),] FID2=FID2[order(FID2[,2]),] ##this binds the FIDs,IIDs,missingness for all pairs bind=cbind(FID1,FID2) bind$index=c() bind$index=c() ##this displays just the values for missingness for each pair bindval=cbind(FID1$F_MISS,FID2$F_MISS) colnames(bindval)=c(1,2) max=as.numeric(colnames(bindval)[apply(bindval,1,which.max)]) #finds the corresponding IID and FID (bind1 and bind2) for the individual with the higher missingness so it can be removed bind1=bind[cbind(seq_along(max*3-1), max*3-1)] bind2=bind[cbind(seq_along(max*3-2), max*3-2)] final=cbind(bind1,bind2) final=unique(final) write.table(final, 'low_call_rate.txt', append = FALSE, sep = \" \", dec = \".\", row.names = FALSE, col.names = FALSE,quote=FALSE) remove individual with higher missingness plink --bfile $FILE_QC.euro.hwe_control.found --remove low_call_rate.txt --make-bed --out $FILE_QC.euro.hwe_control.found.unrelated Create covariates based on PCA. \u00b6 Perform an PCA ONLY on QC'd sample data without ethnic outliers plink --bfile $FILE_QC.euro.hwe_control.found.unrelated --extract plink.prune.in --genome --out $FILE_QC.euro.hwe_control.found.unrelated plink --bfile $FILE_QC.euro.hwe_control.found.unrelated --extract plink.prune.in --make-bed --pca 10 'header' --out $FILE_QC.euro.hwe_control.found.unrelated.PCA The values of the PCs that contribute to most of the variance in our samples are subsequently used as covariates in the association analysis in the third tutorial. To determine which PCs should be included in the model, we make a scree plot using the r code below: Note that we are using the eigenavalues from the merged dataset for visualization purposes (if we used the data from only the EUR HapMap set, there is virtually no variability and the eigenvalues are all close to 1) library(ggplot2) png('scree.png',units=\"in\",width=13,height=7,res=300) val=read.table('PCA_MERGE.eigenval') val$index=as.numeric(row.names(val)) ggplot(val, aes(x=index, y=V1)) + geom_point() + geom_line()+ ggtitle(\"Screeplot of the first 10 PCs\")+ ylab('eigenvalue')+ xlab('PCs')+ scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10))+ theme(legend.position = 'none') dev.off() An example scree plot awk '{print$1, $2,$3, $4, $5, $6, $7,$8,$9,$10,$11,$12}' $FILE_QC.euro.hwe_control.found.unrelated.PCA.eigenvec > covar_PCs.txt The values in covar_mds.txt will be used as covariates, to adjust for remaining population stratification, in the third tutorial where we will perform a genome-wide association analysis. mv $FILE_QC.euro.hwe_control.found.unrelated.bed ./popstratout.bed mv $FILE_QC.euro.hwe_control.found.unrelated.bim ./popstratout.bim mv $FILE_QC.euro.hwe_control.found.unrelated.fam ./popstratout.fam cp popstratout* ../3_Association_GWAS cp covar_PCs.txt ../3_Association_GWAS cd ../3_Association_GWAS","title":"2. Population Stratification"},{"location":"popstrat/#population-stratification","text":"In this tutorial, you will learn how to restrict the quality-controlled HapMap data to individuals that are all of the same genetic ancestry. Individuals of European ancestry will be kept. Most genetic research focuses on one ancestry group at the time, as mixing ancestry groups may lead to population stratification confounding the results reference? . Within this reference group, we will compute clusters that can be used to correct for further confounding due to population stratification.","title":"Population Stratification"},{"location":"popstrat/#obtaining-the-1000-genome-reference-data-set","text":"To construct population clusters from the HapMap data, we will need to compare genomes in this dataset to genomes from a reference panel for which ancestry is known. The reference panel we will use is the 1000 genomes project. You can download the 1000 Genomes data we use for this tutorial using the following command: wget ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20100804/ALL.2of4intersection.20100804.genotypes.vcf.gz Note that, while more recently updated data from the 1000 Genomes project is available, we decided to use this data for two reasons: The most recently updated data set is much larger and separated by chromosome, which makes it more computationally intensive and requires an adjustment in the coding to account for the seperate chromosomes. We are able to sufficiently parse out European samples and account for population stratification using this data set. For details on how to use the most updated set see the Separated by Chromosome section on the Additional Considerations page. The data comes in a zipped vcf format. Note that the download is quite sizeable (~65GB). When working with genetic data, one regularly encounters sizeable files. It is good practice to check the validity of the downloaded files, before working with them further. Sometimes, the distributor of the data will provide a hash value for the file of interest, such that you are able to check the integrity of your downloaded file. You can check the hash value for the 1000 genomes data using this bash command, after downloading. md5sum ALL.2of4intersection.20100804.genotypes.vcf.gz which should return 9de60b62a195455359390d4c951d92d4 . If the value for your download is not the same, your download might be corrupted, and we advise you to download the data again. As in tutorial 1, we use PLINK to perform analysis on this dataset. First, the data will be converted into the plink binary format ( .bim,.bed,.fam ) using plink's --make-bed flag. There is no need to unzip the vcf file, as plink can read and convert gzipped files directly. plink --vcf ALL.2of4intersection.20100804.genotypes.vcf.gz --make-bed --out 1000genomes.genotypes Inspect the resulting .fam and .bim files head 1000genomes.genotypes.fam HG00098 HG00098 0 0 0 -9 HG00100 HG00100 0 0 0 -9 HG00106 HG00106 0 0 0 -9 HG00112 HG00112 0 0 0 -9 HG00114 HG00114 0 0 0 -9 HG00116 HG00116 0 0 0 -9 HG00117 HG00117 0 0 0 -9 HG00118 HG00118 0 0 0 -9 HG00119 HG00119 0 0 0 -9 HG00120 HG00120 0 0 0 -9 head 1000genomes.genotypes.bim 1 rs112750067 0 10327 C T 1 rs117577454 0 10469 G C 1 rs55998931 0 10492 T C 1 rs58108140 0 10583 A G 1 . 0 11508 A G 1 . 0 11565 T G 1 . 0 12783 G A 1 . 0 13116 G T 1 . 0 13327 C G 1 . 0 13980 C T One thing to note from the .fam file is that sex ids are missing, plink will read these as ambiguous observations. The head of the .bim file further shows that some SNPs have no assigned rs-id. We use plink to assign unique identifiers to these SNPs, using the --set-missing-var-ids flag. You are free to construct your own ids, but ensure that they are unique. We build our id using the chromosome number (@), a colon, base-pair position (#), the build (b37), the reference allele, a comma, and the alternative allele. The 1000 genomes data uses the Genome Reference Consortium Human Build 37 (GRCh37). It is important to note that if you are using a different build you will need to adjust the [b37] accordingly. plink --bfile 1000genomes.genotypes --set-missing-var-ids @:#[b37]\\$1,\\$2 --make-bed --out 1000genomes_nomissing.genotypes Our .bim file now looks as follows: head 1000genomes_nomissing.genotypes.bim 1 rs112750067 0 10327 C T 1 rs117577454 0 10469 G C 1 rs55998931 0 10492 T C 1 rs58108140 0 10583 A G 1 1:11508[b37]A,G 0 11508 A G 1 1:11565[b37]G,T 0 11565 T G 1 1:12783[b37]A,G 0 12783 G A 1 1:13116[b37]G,T 0 13116 G T 1 1:13327[b37]C,G 0 13327 C G 1 1:13980[b37]C,T 0 13980 C T","title":"Obtaining the 1000 genome reference data set"},{"location":"popstrat/#qc-on-1000-genomes-data","text":"Before we can use the 1000 Genomes data as our reference panel, it is important to perform quality control procedures on this dataset in a similar fashion as outlined in Tutorial 1 . We choose the same parameters as in this tutorial for our SNP missingness threshold, individual missingness threshold, and MAF threshold. FILE_1K=1000genomes_nomissing.genotypes GENO=0.02 INDV=0.02 MAF=0.05 plink --bfile $FILE_1K --geno 0.2 --make-bed --out $FILE_1K.geno.temp plink --bfile $FILE_1K.geno.temp --mind 0.2 --allow-no-sex --make-bed --out $FILE_1K.geno.mind.temp plink --bfile $FILE_1K.geno.mind.temp --geno $GENO --make-bed --out $FILE_1K.geno plink --bfile $FILE_1K.geno --mind $INDV --allow-no-sex --make-bed --out $FILE_1K.geno.mind plink --bfile $FILE_1K.geno.mind --maf $MAF --make-bed --out $FILE_1K.geno.mind.maf 5,808,310 variants and all 629 people pass filters and QC. Extract the variants present in HapMap from the 1000 genomes dataset Our reference panel and our dataset of interest (e.g. the cleaned HapMap data) must consist of the same set of SNPs. For the remainder of this tutorial, we will set the environmental variable FILE_QC to refer to the clean HapMap data (see Tutorial 1 ). The awk '{print$2}' command selects the second column of the bim file to save the rsid values in a text file. We will then use the --extract flag in plink to extract only the SNPs in the 1000 genomes data that are also present in the HapMap data, and vice versa. FILE_QC=HapMap_3_r3_1.qcout awk '{print$2}' \"$FILE_QC.bim\"> QCFILE_SNPs.txt plink --bfile $FILE_1K.geno.mind.maf --extract QCFILE_SNPs.txt --make-bed --out $FILE_1K.geno.mind.maf.extract awk '{print$2}' $FILE_1K.geno.mind.maf.extract.bim > 1kG_SNPs.txt plink --bfile $FILE_QC --extract 1kG_SNPs.txt --make-bed --out $FILE_QC.extract This results in 1,072,511 SNPs that both datasets have in common.","title":"QC on 1000 Genomes data."},{"location":"popstrat/#change-1000-genomes-data-build-to-match-build-of-hapmap-data","text":"Our next goal is to merge the HapMap data with the 1000 genomes data, such that we can compare the (unknown) population clusters of the HapMap data with the (known) population clusters in the 1000 genomes data. Before merging genetic data, however, a couple of steps need to be taken to ensure that the files align. One of these steps is to ensure that both datasets are in the same build. The 1000 genomes data uses the Genome Reference Consortium Human Build 37 (GRCh37), whereas the HapMap data uses the older build 36. This means that, for the same rsids, the base pair position will slightly differ between these two datasets. We will use the --update-map flag in plink to change all base pair positions in the 1000 genomes to the positions as given by the HapMap data. Note Most current genetic datasets use build 37 or 38. If you are working with genetic data that is not into the build of your liking, you may wish to move your dataset to another build entirely at the start of your project. Look at our Liftover tutorial to see how to move data to another build. awk '{print$2,$4}' $FILE_QC.extract.bim > buildmap.txt # buildmap.txt contains one SNP-id and physical position per line. plink --bfile $FILE_1K.geno.mind.maf.extract --update-map buildmap.txt --make-bed --out $FILE_1K.geno.mind.maf.extract.build ??? note: \"The resulting output:\" ```bash PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to 1000genomes_nomissing.genotypes.geno.mind.maf.extract.build.log. Options in effect: --bfile 1000genomes_nomissing.genotypes.geno.mind.maf.extract --make-bed --out 1000genomes_nomissing.genotypes.geno.mind.maf.extract.build --update-map buildmap.txt 12574 MB RAM detected; reserving 6287 MB for main workspace. 1072511 variants loaded from .bim file. 629 people (0 males, 0 females, 629 ambiguous) loaded from .fam. Ambiguous sex IDs written to 1000genomes_nomissing.genotypes.geno.mind.maf.extract.build.nosex . --update-map: 1072511 values updated. Warning: Base-pair positions are now unsorted! Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 629 founders and 0 nonfounders present. Calculating allele frequencies... done. Total genotyping rate is 0.999412. 1072511 variants and 629 people pass filters and QC. Note: No phenotypes present. --make-bed to 1000genomes_nomissing.genotypes.geno.mind.maf.extract.build.bed + 1000genomes_nomissing.genotypes.geno.mind.maf.extract.build.bim + 1000genomes_nomissing.genotypes.geno.mind.maf.extract.build.fam ... done. ```","title":"Change 1000 Genomes data build to match build of HapMap data"},{"location":"popstrat/#merge-the-hapmap-and-1000-genomes-data-sets","text":"Prior to merging 1000 Genomes data with the data we want to make sure that the files are mergeable, for this we conduct 3 steps: 1) Make sure the reference allele is similar in the HapMap data and the 1000 Genomes Project datasets 2) Resolve strand issues. 3) Remove the SNPs which after the previous two steps still differ between datasets 1) set reference allele The .bim file records the reference allele and the alternative allele at each SNP. These are not necessarily the same in both datasets. In the following commands, we use the --reference-allele flag in PLINK to ensure that the reference alleles in the HapMap data are the same as the reference alleles in 1K genomes. awk '{print$2,$5}' $FILE_1K.geno.mind.maf.extract.build.bim > 1kg_ref-list.txt plink --bfile $FILE_QC.extract --reference-allele 1kg_ref-list.txt --make-bed --out Map-adj # The 1kG_MDS6 and the HapMap-adj have the same reference genome for all SNPs. ??? The resulting output: \" plink --bfile $FILE_QC. extract --reference-allele 1kg_ref-list.txt --make-bed --out Map-adj PLINK v1.90b6.17 64-bit (28 Apr 2020) www.cog-genomics.org/plink/1.9/ (C) 2005-2020 Shaun Purcell, Christopher Chang GNU General Public License v3 Logging to Map-adj.log. Options in effect: --a1-allele 1kg_ref-list.txt --bfile HapMap_3_r3_1.qcout.extract --make-bed --out Map-adj 12574 MB RAM detected; reserving 6287 MB for main workspace. 1072511 variants loaded from .bim file. 160 people (77 males, 83 females) loaded from .fam. 108 phenotype values loaded from .fam. Using 1 thread (no multithreaded calculations invoked). Before main variant filters, 108 founders and 52 nonfounders present. Calculating allele frequencies... done. Total genotyping rate is 0.99815. Warning: Impossible A1 allele assignment for variant rs11488462. Warning: Impossible A1 allele assignment for variant rs4648786. Warning: Impossible A1 allele assignment for variant rs12748433. Warning: Impossible A1 allele assignment for variant rs28635343. Warning: Impossible A1 allele assignment for variant rs28456011. Warning: Impossible A1 allele assignment for variant rs6604981. Warning: Impossible A1 allele assignment for variant rs4074196. [...] --a1-allele: 1072511 assignments made. 1072511 variants and 160 people pass filters and QC. Among remaining phenotypes, 54 are cases and 54 are controls. (52 phenotypes are missing.) --make-bed to Map-adj.bed + Map-adj.bim + Map-adj.fam ... done. ``` 2) Resolve strand issues Note that PLINK warns us that some A1 allele assignments are impossible. We can use grep on both files to see why one of these SNPs looks different in both files: grep rs11488462 $FILE_1K.geno.mind.maf.extract.build.bim 1 rs11488462 0 1343243 C T grep rs11488462 $FILE_QC.extract.bim 1 rs11488462 0 1343243 G A It is clear that in these cases, flipping the allele in the HapMap data from G to A is of no use, since the reference allele at this SNP in the 1K genomes is C. How could a given SNP have a different reference allele as well as a different alternative allele? These differences are due to strand issues. The reference allele is represented by a nucleotide, but each nucleotide has an underlying complement that is present on the other DNA strand, where C complements G and T complements A. Hence, if different strands were genotyped in different samples, the same reference allele could be labelled by a C (T) in the one sample, and by a G (A) in the other. In the example above, the issue is easily resolved by flipping the reference and alternative allele in the HapMap data to their complimentary nucleotides: G to C and A to T, thus aligning this particular SNP with the 1K data. We resolve these strand issues for all SNPs in which PLINK could not align reference alleles using the --flip flag in PLINK, which will flip a given list of rsids towards its complimentary strand. Before we can use the --flip flag in PLINK, we have to create a list of SNP rsids that consists only of the conflicting strands that need flipping. We do this in the first three lines of the following code, where we use ``sort | uniq'' to extract only those lines that are not the same in the .bim files of HapMap and 1K Genomes in terms of the combination of rsid, reference allele, and alternative alleles. In the fourth line, we keep only the rsid, and keep each rsid once to obtain the list of rsid's which strands we have to flip in the HapMap data. awk '{print$2,$5,$6}' $FILE_1K.geno.mind.maf.extract.build.bim > 1kGMDS_strand_tmp awk '{print$2,$5,$6}' Map-adj.bim > Map-adj_tmp sort 1kGMDS_strand_tmp Map-adj_tmp |uniq -u > all_differences.txt awk '{print$1}' all_differences.txt | sort -u > flip_list.txt plink --bfile Map-adj --flip flip_list.txt --reference-allele 1kg_ref-list.txt --make-bed --out corrected_map Check for SNPs which are still problematic after they have been flipped. Are both datasets fully aligned now? Unfortunately not. Let's again extract SNPs that are not the same in terms of their rsid, reference allele, and alternative allele in .bim files of both the HapMap data and the 1K genomes data. awk '{print$2,$5,$6}' corrected_map.bim > corrected_map_tmp sort 1kGMDS_strand_tmp corrected_map_tmp |uniq -u > uncorresponding_SNPs.txt wc -l uncorresponding_SNPs.txt There are still 53 SNPs that do not have the same reference allele and alternative allele in both datasets. Let's again look at one of these SNPs in both .bim files to get a sense of what is going on. head uncorresponding_SNPs.txt rs10060593 A G rs10060593 A T rs10083559 T C rs10083559 T G rs10116901 C A rs10116901 C T rs11524965 T C rs11524965 T G rs11687477 A C rs11687477 T C Clearly, flipping strand issues has not aligned the SNPs from both datasets here. Since it only concerns a small number of SNPs, it is the safest to throw them away, rather than spend more time on trying to resolve these remaining alignment issues. 3) Remove problematic SNPs from your data and from the 1000 Genomes. Here, we want to remove both SNPs that still don't match between data sets. awk '{print$1}' uncorresponding_SNPs.txt | sort -u > SNPs_for_exclusion.txt plink --bfile corrected_map --exclude SNPs_for_exclusion.txt --make-bed --out $FILE_QC.extract.rem plink --bfile $FILE_1K.geno.mind.maf.extract.build --exclude SNPs_for_exclusion.txt --make-bed --out $FILE_1K.geno.mind.maf.extract.build.rem","title":"Merge the HapMap and 1000 Genomes data sets"},{"location":"popstrat/#merge-hapmap-data-with-1000-genomes-data","text":"plink --bfile $FILE_QC.extract.rem --bmerge $FILE_1K.geno.mind.maf.extract.build.rem --allow-no-sex --make-bed --out merged After merging the data, we want to do a final removal of ambiguous (A/T and G/C) SNPs. A/T and G/C loci are unable to be strand-resolved and for this are routinely removed. We use a script from https://github.com/eatkinson/Post-QC: find_cg_at_snps.py with some minor modifications to be compatible with python3. For python2 versions change \"open(bimfile)\" to file(bimfile). wget https://github.com/eatkinson/Post-QC/raw/master/find_cg_at_snps.py sed -i -e 's/file(bimfile)/open(bimfile)/g' find_cg_at_snps.py ##for compatibility in python3 sed -i -e 's/ line\\[1\\]/(line\\[1\\])/g' find_cg_at_snps.py ##for compatibility in python3 python find_cg_at_snps.py merged.bim > ATCGsites plink --bfile merged --exclude ATCGsites --make-bed --out MDS_merge2","title":"Merge HapMap data with 1000 Genomes Data"},{"location":"popstrat/#perform-mds-and-pca-on-map-ceu-data-anchored-by-1000-genomes-data-using-a-set-of-pruned-snps","text":"Our next goal is to perform a cluster analysis on the merged dataset, and determine which clusters belong to which underlying ancestry group. To this end, we need to know the population to which 1000 genomes individuals belong, which is stored in the .PANEL file that we download here: Download the file with population information of the 1000 genomes dataset. wget ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20100804/20100804.ALL.panel The file 20130502.ALL.panel contains population codes of the individuals of 1000 genomes. Convert population codes into superpopulation codes (i.e., AFR,AMR,ASN, and EUR). The .PANEL file contains information on the population that each 1K genomes individual belongs too. We extract family ID and individual ID (which are the same for 1K genomes). and population into a seperate file. For the HapMap data we create a similar file, but we substitute the population info with a flag labeled \"OWN\", as population information for HapMap individuals is unknown. We next change the detailed 1K genomes population categories to less precise superpopulation codes reference? , using sed, a bash command which let's you search and replace text. The last line in this block of code concatenates (using cat) the ``superpopulated'' 1K genomes racefile with the HapMap racefile, for which race is flagged as unknown, and adds a header using sed. awk '{print$1,$1,$2}' 20100804.ALL.panel > race_1kG.txt Create a racefile of your own data. awk '{print$1,$2,\"OWN\"}' $FILE_QC.extract.rem.fam > racefile_own.txt make txt file of concatenated racefiles sed 's/JPT/ASN/g' race_1kG.txt>race_1kG2.txt sed 's/ASW/AFR/g' race_1kG2.txt>race_1kG3.txt sed 's/CEU/EUR/g' race_1kG3.txt>race_1kG4.txt sed 's/CHB/ASN/g' race_1kG4.txt>race_1kG5.txt sed 's/CHD/ASN/g' race_1kG5.txt>race_1kG6.txt sed 's/YRI/AFR/g' race_1kG6.txt>race_1kG7.txt sed 's/LWK/AFR/g' race_1kG7.txt>race_1kG8.txt sed 's/TSI/EUR/g' race_1kG8.txt>race_1kG9.txt sed 's/MXL/AMR/g' race_1kG9.txt>race_1kG10.txt sed 's/GBR/EUR/g' race_1kG10.txt>race_1kG11.txt sed 's/FIN/EUR/g' race_1kG11.txt>race_1kG12.txt sed 's/CHS/ASN/g' race_1kG12.txt>race_1kG13.txt sed 's/PUR/AMR/g' race_1kG13.txt>race_1kG14.txt cat race_1kG14.txt racefile_own.txt | sed -e '1i\\FID IID race' > racefile.txt","title":"Perform MDS and PCA on Map-CEU data anchored by 1000 Genomes data using a set of pruned SNPs"},{"location":"popstrat/#perform-pca","text":"We next compute the first 10 principal components of the combined genetic data. These principal components correlate with genetic ancestry. Again, we first clump the data to obtain a set of SNPs that is approximately in linkage equilibrium. The second line in the code below performs the principal component analysis on the extracted set of approximately independent SNPs, calculating the first ten PCs, and adding a `header' to the resulting output file (PCA_MERGE.eigenvec) using the appropriate option.","title":"Perform PCA"},{"location":"popstrat/#extracting-ancestry-information-from-principal-components-values","text":"To understand how values of the PCs map into ancestry information, we plot the first two PC values against each other, giving datapoints from different population groups a different color. Perform PCA Clustering plink --bfile MDS_merge2 --indep-pairwise 50 5 0.5 plink --bfile MDS_merge2 --extract plink.prune.in --make-bed --pca 10 'header' --out PCA_MERGE Create PCA-plot Performed in R library(dplyr) data<- read.table(file=\"PCA_MERGE.eigenvec\",header=TRUE) eigenval<-read.table(file=\"PCA_MERGE.eigenval\",header=F) race<- read.table(file=\"racefile.txt\",header=TRUE) datafile<- merge(data,race,by=c(\"IID\",\"FID\")) datafile=datafile %>% filter(datafile$race %in% c(\"EUR\",\"ASN\",\"AMR\",\"AFR\",\"OWN\")) head(datafile) pve = eigenval/sum(eigenval)*100 library(ggplot2) library(gridExtra) png(\"pcagg.png\",units=\"in\",width=13,height=7,res=300) b <- ggplot(datafile, aes(PC1, PC2, col = race,group=race)) b <- b+ geom_point(size = 2) b <-b + xlab(paste0(\"PC1 (\", signif(pve[1,1],3), \"%)\")) + ylab(paste0(\"PC2 (\", signif(pve[2,1], 3), \"%)\"))+ theme(text = element_text(size=13),legend.text=element_text(size=13),legend.title = element_blank(),legend.position='top')+guides(color = guide_legend(override.aes = list(size=8))) c <-ggplot(datafile, aes(PC1, PC2, col = race,group=race)) c <-c+ggplot2::stat_ellipse( geom = \"path\", position = \"identity\", show.legend = NA, size=2, inherit.aes = TRUE, type = \"t\", level = 0.95, segments = 51 ) c <- c + xlab(paste0(\"PC1 (\", signif(pve[1,1],3), \"%)\")) + ylab(paste0(\"PC2 (\", signif(pve[2,1], 3), \"%)\"))+ theme(text = element_text(size=13),legend.title = element_blank(),legend.position = \"none\") grid.arrange(b, c, ncol=2) dev.off() The resulting plot clearly seperates Europeans, Africans, Americans, and Asians load differently on the first two PCs. The HapMap data that we use here clearly corresponds with the European ancestry cluster, but when using your own datasets you may find that your own dataset overlaps with multiple ancestry cluster from 1K genomes, and filter your data accordingly. As a last step, we filter on the observed values in which Europeans fall to get a list of European sample ids. awk '{ if ($3 <-0.005 && $4 <-0.005) print $1,$2 }' PCA_MERGE.eigenvec > EUR_samp_PC.txt Perform MDS","title":"Extracting ancestry information from principal components values."},{"location":"popstrat/#perform-mds","text":"We perform MDS clustering to perform unique clusters of the genomic data that overlap with ancestry information. We only use the list of approximately independent SNPs that was determined using the clumping algorithm in Tutorial 1 . In the following two lines of code, the --genome option prepares the data by calculating pairwise identity by state (IBS) for each individual. The resulting file, MDS_merge2.genome , is used as input in the next command, where the identity by state is used to calculate 10 MDS clusters. Perform MDS Clustering plink --bfile MDS_merge2 --extract plink.prune.in --genome --out MDS_merge2 plink --bfile MDS_merge2 --read-genome MDS_merge2.genome --cluster --mds-plot 10 --out MDS_merge2 The resulting output, MDS_merge2.mds, gives us the value of the first 10 MDS clusters for each individual in the stacked HapMap and 1K genomes data.","title":"Perform MDS"},{"location":"popstrat/#extracting-ancestry-information-from-mds-cluster-values","text":"To understand how values of the MDS clusters map into ancestry information, we plot the first two MDS cluster values against each other, giving datapoints from different population groups a different color. Create MDS-plot Performed in R library(ggplot2) library(gridExtra) data<- read.table(file=\"MDS_merge2.mds\",header=TRUE) race<- read.table(file=\"racefile.txt\",header=TRUE) datafile<- merge(data,race,by=c(\"IID\",\"FID\")) head(datafile) png(\"MDS.png\",units=\"in\",width=13,height=7,res=300) b <- ggplot(datafile, aes(C1, C2, col = race,group=race)) b <- b + geom_point(size = 2) b <- b + xlab(\"MDS1\") + ylab(\"MDS2\")+ theme(text = element_text(size=13),legend.text=element_text(size=13),legend.title = element_blank(),legend.position='top')+guides(color = guide_legend(override.aes = list(size=8))) c <-ggplot(datafile, aes(C1, C2, col = race,group=race)) c <-c+ggplot2::stat_ellipse( geom = \"path\", position = \"identity\", show.legend = NA, size=2, inherit.aes = TRUE, type = \"t\", level = 0.95, segments = 51 ) c <- c + xlab(\"MDS1\") + ylab(\"MDS2\")+ theme(text = element_text(size=13),legend.title = element_blank(),legend.position = \"none\") grid.arrange(b, c, ncol=2) dev.off() ## Run Admixture algorithm Note You must install the ADMIXTURE software here . If you are running on a conda environment you can install the admixture software using the following command: conda install -c bioconda admixture Concatenate racefiles. awk '{print$1,$2,\"-\"}' $FILE_QC.extract.rem.fam > racefile_own.txt cat racefile_own.txt race_1kG14.txt | sed -e '1i\\FID IID race' > MDS_merge2.pop sed -i -e \"1d\" MDS_merge2.pop #removes header cut -d \" \" -f 3- MDS_merge2.pop >temp.txt make popfile for admixture script mv temp.txt MDS_merge2.pop run admixture script admixture --supervised ./MDS_merge2.bed 4 > log_merge_admixture.out Warning This step can take a very long time. Running overnight may be neccessary. After admixture is done running, you can prepare the files for plotting. You need to rename MDS_merge2.pop to ind2pop.txt and then make the pong_filemap file with the following format: k4r1 4 locationofQfile/MDS_merge2.12.Q The first column has the letter \"k\" denoting clusters and \"r\" denoting run number. This column can be any unique ID for the run as long as there is at least one letter and one number. The second column denotes the number of clusters. The third column should have the full path to the Q file output from the admixture command. Performed in R R tbl=read.table(\"MDS_merge2.12.Q\") popGroups = read.table(\"MDS_merge2.pop\") fam=read.table(\"MDS_merge2.fam\") mergedAdmWithPopGroups = cbind(tbl, popGroups) ordered = mergedAdmWithPopGroups #ceu v5 #tsi v12 #fin v2 #gbr v1 ordered$EUR=ordered$V5+ordered$V12+ordered$V2+ordered$V1 fam$race=ordered$EUR ids1=fam$V1[which(fam$race>0.8)] ids=data.frame(ids1) ids$ids2=fam$V2[which(fam$race>0.8)] ##list of europeans to extract from files write.table(ids,'europeans.txt',quote = FALSE,row.names = FALSE,col.names=FALSE) ordered$V5=c() ordered$V12=c() ordered$V2=c() ordered$V1=c() ordered=ordered[order(-ordered$EUR),] #barplot(t(as.matrix(subset(ordered, select=c(\"YRI\",\"ASW\",\"CHB\",\"CHS\",\"EUR\",\"JPT\",\"LWK\",\"MXL\",\"PUR\")))), col=rainbow(12), border=NA,names.arg=popnew, las=2,cex.names=0.1) pdf('admixture.pdf') barplot(t(as.matrix(subset(ordered, select=c(\"V3\" , \"V4\" , \"V6\" , \"V7\" ,\"V8\" ,\"V9\" ,\"V10\", \"V11\",\"EUR\")))), col=rainbow(9), border=NA, las=2,ylab=\"Percent Pop\",xaxt=\"n\",xlab='<---- Direction of more European') dev.off() An example plot generated using pong software. The code above can also make a plot but it does not directly label the groups like pong does. The output of this script prints out a text file europeans.txt with a list of all the individuals that are found to be 80% or more European.","title":"Extracting ancestry information from MDS cluster values."},{"location":"popstrat/#exclude-ethnic-outliers","text":"Select individuals in your own data below cut-off thresholds. The cut-off levels are not fixed thresholds but have to be determined based on the visualization of the first two PCS. To exclude ethnic outliers, the thresholds need to be set around the cluster of population of interest. Find min and max value cut offs for European individuals. data<- read.table(file=\"PCA_MERGE.eigenvec\",header=TRUE) eigenval<-read.table(file=\"PCA_MERGE.eigenval\",header=F) race<- read.table(file=\"racefile.txt\",header=TRUE) datafile<- merge(data,race,by=c(\"IID\",\"FID\")) datafile=datafile %>% filter(datafile$race %in% c(\"EUR\",\"ASN\",\"AMR\",\"AFR\",\"OWN\")) PC1max=max(datafile[datafile$race=='EUR',]$PC1) PC1min=min(datafile[datafile$race=='EUR',]$PC1) PC2max=max(datafile[datafile$race=='EUR',]$PC2) PC2min=min(datafile[datafile$race=='EUR',]$PC2) table <- matrix(c(PC1min,PC1max,PC2min,PC2max), ncol=1) colnames(table)=c('summary') rownames(table) <- c('PC1min', 'PC1max','PC2min', 'PC2max') table=data.frame(table) table$summary=as.numeric(table$summary) write.table(table,'eursum.txt',col.names=F,quote=F) file=eursum.txt cat \"$file\" ##to view file awk '{ if ($3 > -0.0255362 && $3 < -0.0155058 && $4 > -0.0295579 && $4 < -0.0194123) print $1,$2 }' PCA_MERGE.eigenvec > EUR_PCA_MERGE plink --bfile $FILE_QC --keep EUR_PCA_MERGE --make-bed --out $FILE_QC.euro","title":"Exclude ethnic outliers."},{"location":"popstrat/#exclude-ethnic-outliers_1","text":"We use plink to only include those that were marked as Europeans, as confirmed by the PC analysis. The variable $FILE_Euro could also be used to define a European sample resulting from an admixture or MDS analysis. FILE_Euro=EUR_samp_PC.txt plink --bfile $FILE_QC --keep $FILE_Euro--make-bed --out $FILE_QC.euro","title":"Exclude ethnic outliers"},{"location":"popstrat/#hardy-weinburg-equilibrium-filter-on-controls","text":"Now that we have restricted our dataset to Europeans only, we can finalize an import quality control step that was not included in tutorial 1. Namely, we filter out all SNPs that are not in Hardy-Weinberg equilibrium. SNPs that significantly deviate from HWE might be under evolutionary pressure, but also often point to genotyping errors. We suggest to exclude all SNPs that deviate from HWE with a p-value smaller than 1e-6. Note that SNPs which are under pressure of evolutionary selection may be of interest, for example when looking for markers associated with lethal disease. When investigating such binary phenotypes, a lower (i.e. less conservative) HWE threshold could be chosen in cases, but not in controls. HWE_CONTROL=1e-6 plink --bfile $FILE_QC.euro --hwe $HWE_CONTROL --make-bed --out $FILE_QC.euro.hwe_control All SNPs in the HapMap data are already in HWE. No SNPs were excluded.","title":"Hardy Weinburg equilibrium filter on controls"},{"location":"popstrat/#filter-founders-from-dataset","text":"This excludes all samples with at least one known parental ID in the current analysis plink --bfile $FILE_QC.euro.hwe_control --filter-founders --make-bed --out $FILE_QC.euro.hwe_control.found 52 individuals are removed. 108 remain.","title":"Filter founders from dataset"},{"location":"popstrat/#check-for-cryptic-relatedness-with-plink2","text":"We use KING's method (implemented in Plink2) to filter out individuals that are more closely related than third cousins. KING's method is not available in plink, but is available in plink2 (which is still in alpha). The flag --make-king-table makes a table with kinship estimations between all individuals within the dataset. First degree relations (parent-child or siblings) have an expected kinship value of 0.25 using KING's method. Second-degree relations have a value of 0.125, third-degree relations a value of 0.0625, etc. The flag --king-table-filter can be used to only maintain relationships within the data that surpass a certain kinship threshold. We want to exclude one individual from each pair that is second-degree related or more. Therefore, we set our filter at 0.0884: The geometric mean between second and third degree relations. #http://s3.amazonaws.com/plink2-assets/alpha2/plink2_linux_x86_64.zip #unzip plink2_linux_x86_64.zip ./plink2 --bfile $FILE_QC.euro.hwe_control.found --make-king-table --king-table-filter 0.0884 sed 's/^#//' plink2.kin0 > kin.txt #Replaces the hashtag in the header of the king-table so R can read it easily. ??? note: \"The resulting output:\" cat kin.txt FID1 ID1 FID2 ID2 NSNP HETHET IBS0 KINSHIP 1454 NA12813 13291 NA07045 1176687 0.189994 0.0174787 0.236928 These two individuals are first-degree relatives, as their estimated kinship is close to 0.25, but were not identified as such in the data. We will remove one of these individuals. We could do this at random, but a better approach is to remove the individual with most missing SNPs. plink --bfile $FILE_QC.euro.hwe_control.found --missing In R, run the following script to remove the individuals with the highest amount of missingness in each related pair. === \"Performed in R\" ```{r} data<- read.table(file=\"kin.txt\",header=TRUE) data_related=data[,c(1,2,3,4)] missing <- read.table(\"plink.imiss\", header =TRUE, as.is=T) FID1=data_related[,c(1,2)] FID2=data_related[,c(3,4)] FID1$index=row.names(FID1) FID2$index=row.names(FID2) FID1$IID=FID1$ID1 FID2$IID=FID2$ID2 FID1[,1:2]=c() FID2[,1:2]=c() FID1=merge(FID1,missing,by=\"IID\") FID2=merge(FID2,missing,by=\"IID\") FID1[,4:6]=c() FID2[,4:6]=c() FID1$index=as.numeric(FID1$index) FID2$index=as.numeric(FID2$index) q=c(setdiff(FID2$index, FID1$index), setdiff(FID1$index, FID2$index)) if (length(q) != 0) { FID1=FID1[!(FID1$index==q),] FID2=FID2[!(FID2$index==q),] } FID1=FID1[order(FID1[,2]),] FID2=FID2[order(FID2[,2]),] ##this binds the FIDs,IIDs,missingness for all pairs bind=cbind(FID1,FID2) bind$index=c() bind$index=c() ##this displays just the values for missingness for each pair bindval=cbind(FID1$F_MISS,FID2$F_MISS) colnames(bindval)=c(1,2) max=as.numeric(colnames(bindval)[apply(bindval,1,which.max)]) #finds the corresponding IID and FID (bind1 and bind2) for the individual with the higher missingness so it can be removed bind1=bind[cbind(seq_along(max*3-1), max*3-1)] bind2=bind[cbind(seq_along(max*3-2), max*3-2)] final=cbind(bind1,bind2) final=unique(final) write.table(final, 'low_call_rate.txt', append = FALSE, sep = \" \", dec = \".\", row.names = FALSE, col.names = FALSE,quote=FALSE) remove individual with higher missingness plink --bfile $FILE_QC.euro.hwe_control.found --remove low_call_rate.txt --make-bed --out $FILE_QC.euro.hwe_control.found.unrelated","title":"Check for cryptic relatedness with plink2"},{"location":"popstrat/#create-covariates-based-on-pca","text":"Perform an PCA ONLY on QC'd sample data without ethnic outliers plink --bfile $FILE_QC.euro.hwe_control.found.unrelated --extract plink.prune.in --genome --out $FILE_QC.euro.hwe_control.found.unrelated plink --bfile $FILE_QC.euro.hwe_control.found.unrelated --extract plink.prune.in --make-bed --pca 10 'header' --out $FILE_QC.euro.hwe_control.found.unrelated.PCA The values of the PCs that contribute to most of the variance in our samples are subsequently used as covariates in the association analysis in the third tutorial. To determine which PCs should be included in the model, we make a scree plot using the r code below: Note that we are using the eigenavalues from the merged dataset for visualization purposes (if we used the data from only the EUR HapMap set, there is virtually no variability and the eigenvalues are all close to 1) library(ggplot2) png('scree.png',units=\"in\",width=13,height=7,res=300) val=read.table('PCA_MERGE.eigenval') val$index=as.numeric(row.names(val)) ggplot(val, aes(x=index, y=V1)) + geom_point() + geom_line()+ ggtitle(\"Screeplot of the first 10 PCs\")+ ylab('eigenvalue')+ xlab('PCs')+ scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10))+ theme(legend.position = 'none') dev.off() An example scree plot awk '{print$1, $2,$3, $4, $5, $6, $7,$8,$9,$10,$11,$12}' $FILE_QC.euro.hwe_control.found.unrelated.PCA.eigenvec > covar_PCs.txt The values in covar_mds.txt will be used as covariates, to adjust for remaining population stratification, in the third tutorial where we will perform a genome-wide association analysis. mv $FILE_QC.euro.hwe_control.found.unrelated.bed ./popstratout.bed mv $FILE_QC.euro.hwe_control.found.unrelated.bim ./popstratout.bim mv $FILE_QC.euro.hwe_control.found.unrelated.fam ./popstratout.fam cp popstratout* ../3_Association_GWAS cp covar_PCs.txt ../3_Association_GWAS cd ../3_Association_GWAS","title":"Create covariates based on PCA."},{"location":"prs/","text":"Calculating and Analysing PRS \u00b6 Background \u00b6 In this section of the tutorial you will learn how to construct a Polygenic Risk Score (PRS) for Alzheimer's disease, using PLINK and ldpred, and use this PRS to predict the likelihood of late-onset alzheimer's disease. As part of this analysis, you will estimate the heritability of alzheimer's disease using ld score regression on the GWAS summary statistics, and using GCTA on the target dataset. Whenever evaluating the predictiveness of a PRS, it is of vital importance that the target data set was not included in the original GWAS analysis. When using GWAS results from previously published work, it is important to check the accompanying article for the data sources that the authors used and ensure that your target data was not included in the study. When this is the case, it is advised to search for GWAS results elsewhere, or to contact the original GWAS authors to see whether it is possible to acquire meta-analysed summary statistics that exclude the target population. The target data that we use in this tutorial is a dataset of 176 cases and 188 controls for late-onset alzheimer's disease https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2667989/pdf/main.pdf . We have cleaned this target data according to our quality control protocol as outlined in tutorials 1 and 2 , after which 341 individuals remain (167 cases and 174 controls). provide download links to cleaned target files The GWAS summary statistics that we will use are from the largest and most recently published GWAS for alzheimer's disease (Jansen et al., 2019). Click here to download the GWAS summary statistics estimated (AD_sumstats_Jansenetal_2019sept.txt.gz), which you will use as your weights for the polygenic score. Note that this is a GWAS, not on late-onset alzheimer's disease, but on a combination of alzheimer's disease and alzheimer's disease by proxy (i.e. through parental diagnoses). The construction of a PRS on a different, but related phenotype may reduce its overall predictiveness, but is still a useful endaveour as long as the phenotypes in the GWAS and target dataset are genetically related. Phenotypes are often coded differently in GWAS to maximize sample size and harmonize different datasets. One of the uses of a PRS is that they can be applied to study the relationships between genotype and phenotype in a smaller dataset in which phenotypes may be recorded in more detail. Links to install software here (GCTA,PLINK,LDpred) Before you start, make sure the clean target data file in plink binary format (.bed, .bim and .fam) and the GWAS summary statistics (.txt) are in your working directory. We define a TARGETSET and GWASSET variable at the start, to easily refer to our target and GWAS summary data, respectively. TARGETSET=adgwas.qcout.clean GWASSET=AD_sumstats_Jansenetal_2019sept.txt QC of GWAS summary data \u00b6 First, let's look at the first 5 lines of the GWAS summary statistics head -5 $GWASSET uniqID.a1a2 CHR BP A1 A2 SNP Z P Nsum Neff dir EAF BETA SE 1:715265_T_C 1 715265 T C rs12184267 2.12197306477 0.03384 359856 359856 ??+? 0.0408069 0.0126426452822487 0.0059579668998386 1:715367_G_A 1 715367 G A rs12184277 1.95791489337 0.05024 360230 360230 ??+? 0.0410687 0.0116235111669331 0.00593667845639937 1:717485_A_C 1 717485 A C rs12184279 1.91243829548 0.05582 360257 360257 ??+? 0.0405759 0.0114189092819899 0.0059708641627697 1:720381_T_G 1 720381 T G rs116801199 2.29540369149 0.02171 360980 360980 ??+? 0.042162 0.0134428918549354 0.00585643906767846 The ReadMe of the GWAS summary data clarifies each of these columns. In this tutorial, we will only use the following columns: - SNP : the rsID for each tested SNP - BP : the SNP's base position - A1 : the reference allele - A2 : the alternative allele - Z : The Z-score - BETA : the estimated coefficient of the SNP on the diagnosis for alzheimer's disease - P : the P-value associatied with BETA - Nsum The sample size for the particular SNP. We first define some variables such that we can easily to refer to the columns we need in our code: BPCOLNO=3 REFACOLNO=4 ALTACOLNO=5 SNPCOLNO=6 PVALCOLNO=8 BETACOLNO=13 ZCOLNO=7 NCOLNO=9 Remove duplicate SNPs from GWAS summary statistics \u00b6 Most GWAS software packages, such as PLINK, do not allow for duplicate SNPs in the GWAS summary data. We will obtain a list of rsIDs using awk, sort these SNPs, and extract the repeated lines using ``uniq -d'', saving these into a new file called duplicate.snp. We next use grep to filter out these duplicate SNPs from our GWAS summary statistics. awk -v c1=$SNPCOLNO '{print $c1}' $GWASSET |\\ sort |\\ uniq -d > duplicate.snp grep -vf duplicate.snp $GWASSET > $GWASSET.nodup wc -l $GWASSET 13367300 AD_sumstats_Jansenetal_2019sept.txt wc -l $GWASSET.nodup 13336963 AD_sumstats_Jansenetal_2019sept.txt.nodup This filtering procedure leaves us with over 13 million SNPs that are non-duplicates. Duplicate SNPs occur, for example, due to coding mistakes, or when SNPs are multiallelic. Further cleaning of GWAS summary statistics \u00b6 When constructing a PRS, it is of importance to ensure that the SNP weights reported in the GWAS summary statistics are correctly matched with the target data. Any SNPs that are incorrectly matched will be assigned a wrong weight, and thus introduce noise into the PRS. Here, we remove strand ambiguous alleles, as well as indels (here marked by \"I'/D\"). Furthermore, we only keep columns that have rsIDs. awk '!( ($4==\"A\" && $5==\"T\") || ($4==\"D\" && $5==\"I\") || \\ ($4==\"T\" && $5==\"A\") || ($4==\"I\" && $5==\"D\") || \\ ($4==\"G\" && $5==\"C\") || \\ ($4==\"C\" && $5==\"G\")) {print}' $GWASSET.nodup > $GWASSET.nodup.noambu awk 'NR==1' $GWASSET.nodup.noambu | awk -v c1=$SNPCOLNO '$c1 ~ /^rs/' $GWASSET.nodup.noambu > $GWASSET.nodup.noambu.rsid wc -l $GWASSET.nodup.noambu 10331009 AD_sumstats_Jansenetal_2019sept.txt.noambu wc -l $GWASSET.nodup.noambu.rsid 10227986 AD_sumstats_Jansenetal_2019sept.txt.nodup.noambu.rsid Notes on GWAS cleaning: irrelevant here: but don't include X, Y, or MT chromosome. If available, filter on info. Matching cleaned GWAS summary statistics with target data. \u00b6 Our next goal is to take our target data, and only keep the SNPs that are present in the GWAS summary statistics. We first count how many SNPs there are in the GWAS summary and target data, respectively. ### wc -l $GWASSET.nodup.noambu.rsid 10227986 AD_sumstats_Jansenetal_2019sept.txt.nodup.noambu.rsid wc -l $TARGETSET.bim 191069 adgwas.qcout.clean.bim Resolve strand issues, flip alleles Note that only 191069 SNPs are present in our GWAS summary data ( wc -l $TARGETSET.bim ), such that we cannot use any of the 10036917 remaining SNPs. We do not need to observe all SNPs from our GWAS summary statistics in our target data to construct a useful PRS. Even the missingness of causal SNPs for alzheimer's disease in the target data is not a big problem as long as we have still genotyped nearby SNPs that are in linkage disequilibrium with such causal SPNs. Nonetheless, the low SNP density in our target dataset will negatively effect the predictive power of our PRS. The GWAS summary results do have a wide SNP coverage, which is of more crucial importance, such that we can attach PRS weights to each SNP in our limited target data set. Next, we use awk to get a list of SNP rsIDs and base pair positions from our GWAS summary statistics. We next take our target data, only keep the SNPs that are present in the GWAS summary statistics, and update their base pair positions using the update-map flag in plink. This updates the base pair positions from the build of the target data set (hg36), to the build of the GWAS summary data (hg37) check awk -v c1=$SNPCOLNO -v c2=$BPCOLNO '{print $c1,$c2}' $GWASSET.nodup.noambu.rsid > gwassnps.txt awk -v c1=$SNPCOLNO 'FNR>1{print $c1}' $GWASSET.nodup.noambu.rsid > snplist.txt plink --bfile $TARGETSET --extract snplist.txt --update-map gwassnps.txt --make-bed --out $TARGETSET.out wc -l $TARGETSET.out.bim 155949 adgwas.qcout.clean.out.bim 155,949 SNPs present in our target data set are also present in our GWAS summary data. Finally, we use awk and grep to restrict our gwas summary statistics to the SNPs that are also present in our target data. #Use only variables in GWAS summary stats that are also present in genome data. (but keep header of the original summary stats file) awk '{print $2}' $TARGETSET.out.bim > $TARGETSET.SNPs.txt head -1 $GWASSET.nodup > GWASanalysis.txt && grep -wFf $TARGETSET.SNPs.txt $GWASSET.nodup >> GWASanalysis.txt wc -l GWASanalysis.txt Finally, we flip reference alleles and resolve any strand issues. awk -v c1=$SNPCOLNO -v c2=$REFACOLNO 'FNR>1{print $c1,$c2}' GWASanalysis.txt > gwasreflist.txt plink --bfile $TARGETSET.out --reference-allele gwasreflist.txt --make-bed --out $TARGETSET.out.ref awk -v c1=$SNPCOLNO -v c2=$REFACOLNO -v c3=$ALTACOLNO '{print$c1,$c2,$c3}' GWASanalysis.txt > gwasstrandlist.txt awk '{print$2,$5,$6}' $TARGETSET.out.ref.bim > $TARGETSET.strandlist sort gwasstrandlist.txt $TARGETSET.strandlist |uniq -u > all_differences.txt awk '{print$1}' all_differences.txt | sort -u > flip_list.txt plink --bfile $TARGETSET.out.ref --flip flip_list.txt --reference-allele gwasreflist.txt --make-bed --out $TARGETSET.out.ref.strand Investigate problematic SNPs and throw them out: awk '{print$2,$5,$6}' $TARGETSET.out.ref.strand.bim > corrected_map_tmp sort gwasstrandlist.txt corrected_map_tmp |uniq -u > uncorresponding_SNPs.txt wc -l uncorresponding_SNPs.txt awk '{print$1}' uncorresponding_SNPs.txt | sort -u > SNPs_for_exclusion.txt plink --bfile $TARGETSET.out.ref.strand --exclude SNPs_for_exclusion.txt --make-bed --out $TARGETSET.def LDSC to assess SNP-based heritability and confounding in GWAS summary statistics. \u00b6 GCTA to estimate SNP-based heritability \u00b6 Before constructing a PGS on the target data, it is useful to estimate the SNP-based heritability of the phenotype of interest using gcta. Due to measurement error in the PRS weights, the share of explained variance that the PRS can explain in a linear regression is lower than the overall SNP-based heritability. Therefore, estimation of SNP-based heritability gives us a reasonable expectation of the upper bound that we can expect from the predictiveness of the PRSs that we will construct. Genome-based restricted maximum likelihood (GREML) estiamtes the degree of variance explained in the phenotype that is explained by all SNPs in the target data. This is often referred to as SNP-based heritability. GCTA is a software package used to conduct such GREML analyses. It is similar to PLINK in the sense that it is operated through bash commands, using flags to guide the analysis of interest. Before using GCTA to estimate SNP-based heritability, it is advisable to conduct a power calculation using the associated power calculator: https://shiny.cnsgenomics.com/gctaPower/. To arrive at reasonable power (>80%), a sample size of 2000 individuals is roughly the minimum that is needed for most traits. The target data set here is notably smaller. With our 174 cases and 167 controls, and assuming a disease risk in the population of 0.1, a trait heritability of 0.5, alpha of 0.05, and the default variance of SNP-derived genetic relationships of 0.00002 gives us a dramatically low power of 0.0806. The standard error that we could expect for our analysis is 0.9760, which is incredibly large, given that a heritability estimate is bounded between 0 and 1 by definition. Hence, in our current data set, the estimation of SNP-based heritability using GREML is a useless endevaour. We nonetheless perform the GREML analysis anyway to illustrate how it is done in a dataset that is sufficiently large. In the following lines of code, we first collect the family and individual IDs, and the phenotype data (case or control) into a new file (phenotype.phen). We next invoke gcta to estimate genome-wide relatedness matrix (using the --make-grm flag), which serves as an input to the heritability estimation. The estimation of the GRM is very sensitive to the inclusion of cryptic related individuals. We use the --grm-cutoff flag to throuw out individuals with relatedness value larger than 0.025. This is a more conservative parameter setting than the one included in our QC pipeline. We next invoke the --reml flag in gcta64 to estimate the the SNP-based heritability. The necessary inputs are the GRM estimated in the line of code before, using the --grm flag, and the phenotype file using the --pheno flag. awk '{print $1,$2,$6}' $TARGETSET.def.fam > phenotype.phen ##To DO: visualize the phenotype here (write R-script) ##Estimate heritability using GCTA (GREML) (GCTA-GREML power calculator?) --> TO DO: Double-check we exclude close relatives. Check Yang et al. (2017). GRMCUT=0.025 gcta64 --bfile $TARGETSET.def --autosome --grm-cutoff $GRMCUT --make-grm --out $TARGETSET.grm gcta64 --reml --grm $TARGETSET.grm --pheno phenotype.phen --out greml The resulting output: ******************************************************************* * Genome-wide Complex Trait Analysis (GCTA) * version 1.93.2 beta Linux * (C) 2010-present, Jian Yang, The University of Queensland * Please report bugs to Jian Yang <jian.yang.qt@gmail.com> ******************************************************************* Analysis started at 15:54:59 CEST on Mon Mar 29 2021. Hostname: int1.bullx Accepted options: --reml --grm adgwas.qcout.clean.grm --pheno phenotype.phen --out greml Note: This is a multi-thread program. You could specify the number of threads by the --thread-num option to speed up the computation if there are multiple processors in your machine. Reading IDs of the GRM from [adgwas.qcout.clean.grm.grm.id]. 341 IDs read from [adgwas.qcout.clean.grm.grm.id]. Reading the GRM from [adgwas.qcout.clean.grm.grm.bin]. GRM for 341 individuals are included from [adgwas.qcout.clean.grm.grm.bin]. Reading phenotypes from [phenotype.phen]. Non-missing phenotypes of 341 individuals are included from [phenotype.phen]. Assuming a disease phenotype for a case-control study: 167 cases and 174 controls Note: you can specify the disease prevalence by the option --prevalence so that GCTA can transform the variance explained to the underlying liability scale. 341 individuals are in common in these files. Performing REML analysis ... (Note: may take hours depending on sample size). 341 observations, 1 fixed effect(s), and 2 variance component(s)(including residual variance). Calculating prior values of variance components by EM-REML ... Updated prior values: 0.125147 0.124882 logL: 62.9277 Running AI-REML algorithm ... Iter. logL V(G) V(e) 1 62.93 0.15331 0.09642 2 63.00 0.21404 0.03527 3 63.06 0.21182 0.03765 4 63.06 0.21194 0.03754 5 63.06 0.21193 0.03754 Log-likelihood ratio converged. Calculating the logLikelihood for the reduced model ... (variance component 1 is dropped from the model) Calculating prior values of variance components by EM-REML ... Updated prior values: 0.25063 logL: 62.32649 Running AI-REML algorithm ... Iter. logL V(e) 1 62.33 0.25063 Log-likelihood ratio converged. Summary result of REML analysis: Source Variance SE V(G) 0.211931 0.176050 V(e) 0.037542 0.174494 Vp 0.249473 0.019152 V(G)/Vp 0.849515 0.699855 Sampling variance/covariance of the estimates of variance components: 3.099372e-02 -3.053760e-02 -3.053760e-02 3.044826e-02 Summary result of REML analysis has been saved in the file [greml.hsq]. Analysis finished at 15:54:59 CEST on Mon Mar 29 2021 Overall computational time: 0.09 sec. The output shows four estimates of interest: V(G), the amount of variance in the phenotype that can be attributed to variance in the SNPs, V(e), the remaining variance that can be attributed to environmental factors, their sum Vp, and estimated SNP-based heritability: V(G) over Vp. The estimated SNP-based heritability is very high: 0.85. However, as expected, the standard error around this estimate (~0.7) is so large that even very low heritabilities can not be ruled out. In sum, the sample size is too small to derive any conclusions about SNP based heritability. Create a PRS using PLINK (clumpig and thresholding): \u00b6 Compare to repository: https://www.pgscatalog.org/trait/EFO_0000249/ Use PRSIce or comment on different thresholds We are now ready to estimate our PRS. For each individual, we multiply their reference allele count at each SNP with a SNP weight estimated from the GWAS summary statistics. However, the GWAS coefficients as estimated in GWAS summary data are not corrected for linkage disequilibrium. Constructing a PRS without any correction for LD essentially leads to an overweighting of SNPs that are in dense LD-regions compared to SNPs that are not, resulting in lower predictability of the PRS. One method to deal with this is clumping. Clumping is a form of informed pruning: The R-squared between SNPs that reside within a given kb-window is computed, and one of the SNPs is thrown out of the R-squared is higher than a given threshold. The algorithm differs from pruning because it sorts all SNPs within a window increasingly by p-value, to ensure that SNPs with the lowest p-value are kept. The clumping algorithm gives a researcher substantial degrees of freedom when constructing a polygenic score. How to set the optimal parameters? Especially the choice of the p-value threshold has been shown to impact the predictiveness of PRSs, with stark differences in optimal threshold between different traits (Ware et al., 2017). Many researchers optimize over a grid of all possible parameter combinations. Automated tools, such as PRSICE2, are available for this. However, fitting many PRSs and choosing the best in terms of their evaluated predictive value in the target data set comes at the risk of overfitting the data. The smaller the dataset, the larger the risk of overfitting. We strongly suggest researchers to scan the literature first to see whether some consensus on optimal parameter values is available. For alzheimer's disease, many studies suggest that highly restrictive p-value thresholds (such that only the most significant SNPs for alzheimer's disease are included) result in the most predictive polygenic scores. Here, we follow Chaudhury et al. (2019), and set a p-value threshold of 0.000107, a window size of 250 kb, and an r-squared threshold of 0.1. plink --bfile $TARGETSET.def --clump-p1 0.000107 --clump-p2 0.000107 --clump-r2 0.1 --clump-kb 250 --clump GWASanalysis.txt --clump-snp-field SNP \\ --clump-field P --out $TARGETSET.clump #Extract list of clumped SNPs (no header): awk 'NR!=1{print $3}' $TARGETSET.clump.clumped > $TARGETSET.clump.snp wc -l $TARGETSET.clump.snp 76 adgwas.qcout.clean.clump.snp Clumping our target data set using the p-values from gwas summary statistics leaves 76 SNPs in our dataset. We will extract these SNPs from our dataset, and construct a PRS using plink. The --score flag in PLINK next takes the reference allele count of each individual in our target data set, multiplies this value by the coefficient estimated in the GWAS, and sums this result over all included 76 SNPs. awk -v c1=$SNPCOLNO -v c2=$REFACOLNO -v c3=$BETACOLNO 'FNR>1{print $c1,$c2,$c3}' GWASanalysis.txt > score.txt plink --bfile $TARGETSET.def --extract $TARGETSET.clump.snp --pheno phenotype.phen --score score.txt --out plink_score The output is a .score file, which summarizes the Polygenic score for each individual in the variable ``SCORE'': Before evaluating our PRSs by checking their predictability against the phenotype of interest, we estimate the first 10 principal components for each individual in our target data. These will be used as control variables. We also extract sex information from the .fam file as additional covariates in our regressions. plink --bfile $TARGETSET.def --pca 10 header --out clusters awk '{print $1,$2,$5}' $TARGETSET.def.fam > temp.txt awk 'BEGIN{print \"FID IID SEX\"}1' temp.txt > sex.txt We are now ready to load the score file in R, merge with the principal components, and evaluating the predictiveness of the PRSs. We standardize our PRSs such that they have mean 0 and standard deviation 1, and change the coding of our phenotype such that cases are equal to 1 and controls are equal to zero. We first estimate a null model that regresses the phenotype on the first ten PCs and sex, using both a linear probability model and a logit specification. We next estimate four models: a linear probability regressing the phenotype on the PRS only, one with all controls, and two similar specifications using a logit model. Performed in R library(stargazer) data <- read.table(file=\"plink_score.profile\",header=TRUE) clusterdata <- read.table(file=\"clusters.eigenvec\",header=TRUE) sexdata <- read.table(file=\"sex.txt\",header=TRUE) mergedata <- merge(data,clusterdata) mergedata <- merge(mergedata,sexdata) #standardize data mergedata$SCORE <- (mergedata$SCORE-mean(mergedata$SCORE))/(sd(mergedata$SCORE)) mergedata$PHENO <- mergedata$PHENO - 1 nullmodel<-lm(PHENO~PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10+SEX,data=mergedata) nullr2<-summary(nullmodel)$r.squared model<-lm(PHENO~SCORE,data=mergedata) modelcontrols<-lm(PHENO~SCORE+PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10+SEX,data=mergedata) model$IncreaseR2<-round(summary(model)$r.squared,digits=3) modelcontrols$IncreaseR2<-round(summary(modelcontrols)$r.squared - nullr2,digits=3) logitmodel<-glm(PHENO~SCORE,data=mergedata,family=\"binomial\") logitmodelcontrols<-glm(PHENO~SCORE+PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10+SEX,data=mergedata,family=\"binomial\") nullmodellogit<-glm(PHENO~PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10+SEX,data=mergedata,family=\"binomial\") nullpseudor2<-round(1-(nullmodellogit$deviance/nullmodellogit$null.deviance),digits=3) logitmodel$PseudoR2<-round(1-(logitmodel$deviance/logitmodel$null.deviance),digits=3) logitmodelcontrols$PseudoR2<-round(1-(logitmodelcontrols$deviance/logitmodelcontrols$null.deviance),digits=3) logitmodel$IncreaseR2<-logitmodel$PseudoR2 logitmodelcontrols$IncreaseR2<-logitmodelcontrols$PseudoR2 - nullpseudor2 stargazer(model,modelcontrols,logitmodel,logitmodelcontrols,type=\"text\", out=\"PLINKScore.txt\",keep = c(\"SCORE\"), add.lines = list(c(\"10 PCs\", \"No\", \"Yes\", \"No\", \"Yes\"),c(\"Pseudo-R2\",\"\",\"\",logitmodel$PseudoR2,logitmodelcontrols$PseudoR2),c(\"Increase-R2\",model$IncreaseR2,modelcontrols$IncreaseR2,logitmodel$IncreaseR2,logitmodelcontrols$IncreaseR2)), star.cutoffs = c(0.05, 0.01, 0.001), float=FALSE) Create a PRS using LDPred \u00b6 python $PREDPATH coord --rs SNP --A1 A1 --A2 A2 --pos BP --eff_type LINREG --chr CHR --pval P --eff BETA --N $NGWAS --ssf GWASanalysis.txt --gf $TARGETSET.out --out pred.coord ## LDpred recommend radius to be Total number of SNPs in target / 3000 (CHECK!) GSize=\"$(wc -l <\"$TARGETSET.out.bim\")\" ldrnum=$(( GSize / 3000 )) python $PREDPATH gibbs --cf pred.coord --ldr $ldrnum --ldf pred.ld --out pred.weight --N $NGWAS echo \"python3 $PREDPATH score --gf $FILESET$out --rf test.weight --out test.score --pf phenotype.phen --pf-format LSTANDARD\" python $PREDPATH score --gf $TARGETSET.out --rf pred.weight --out test.score --pf phenotype.phen --pf-format LSTANDARD --pcs-file clusters.eigenvec ##P+T for reference: python $PREDPATH p+t --cf pred.coord --ldr $ldrnum --out PTpred python $PREDPATH score --gf $TARGETSET.out --rf PTpred --out PTscore.score --pf phenotype.phen --pf-format \\ LSTANDARD --pcs-file clusters.eigenvec Performed in R library(stargazer) data <- read.table(file=\"test.score_LDpred-inf.txt\",header=TRUE, sep = \",\") data$PRS <- (data$PRS-mean(data$PRS))/(sd(data$PRS)) data$true_phens <- data$true_phens - 1 #data$true_phens <- (data$true_phens-mean(data$true_phens))/(sd(data$true_phens)) model<-lm(true_phens~PRS,data=data) logitmodel<-glm(true_phens~PRS,data=data,family=\"binomial\") modelcontrols<-lm(true_phens~PRS+PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10,data=data) logitmodelcontrols<-glm(true_phens~PRS+PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10,data=data,family=\"binomial\") logitmodel$PseudoR2<-round(1-(logitmodel$deviance/logitmodel$null.deviance),digits=3) logitmodelcontrols$PseudoR2<-round(1-(logitmodelcontrols$deviance/logitmodelcontrols$null.deviance),digits=3) stargazer(model,modelcontrols,logitmodel,logitmodelcontrols, out=\"LDPREDstat.tex\",keep = c(\"PRS\"), add.lines = list(c(\"10 PCs\", \"No\", \"Yes\", \"No\", \"Yes\"),c(\"Pseudo-R2\",\"\",\"\",logitmodel$PseudoR2,logitmodelcontrols$PseudoR2)), star.cutoffs = c(0.05, 0.01, 0.001), float=FALSE) Notes on power: \u00b6 Contrast with some results from a larger dataset (e.g. UKB?) Notes on confouding, within-family analysis, etc. \u00b6 Notes on downward bias due to measurement error: Genetic IV? \u00b6 Further topics? Genetic correlations, Genomic SEM \u00b6 References \u00b6 Chaudhury, S., Brookes, K. J., Patel, T., Fallows, A., Guetta-Baranes, T., Turton, J. C., ... & Thomas, A. J. (2019). Alzheimer's disease polygenic risk score as a predictor of conversion from mild-cognitive impairment. Translational psychiatry, 9(1), 1-7. Ware, E. B., Schmitz, L. L., Faul, J., Gard, A., Mitchell, C., Smith, J. A., ... & Kardia, S. L. (2017). Heterogeneity in polygenic scores for common human traits. BioRxiv, 106062.","title":"7.PRS"},{"location":"prs/#calculating-and-analysing-prs","text":"","title":"Calculating and Analysing PRS"},{"location":"prs/#background","text":"In this section of the tutorial you will learn how to construct a Polygenic Risk Score (PRS) for Alzheimer's disease, using PLINK and ldpred, and use this PRS to predict the likelihood of late-onset alzheimer's disease. As part of this analysis, you will estimate the heritability of alzheimer's disease using ld score regression on the GWAS summary statistics, and using GCTA on the target dataset. Whenever evaluating the predictiveness of a PRS, it is of vital importance that the target data set was not included in the original GWAS analysis. When using GWAS results from previously published work, it is important to check the accompanying article for the data sources that the authors used and ensure that your target data was not included in the study. When this is the case, it is advised to search for GWAS results elsewhere, or to contact the original GWAS authors to see whether it is possible to acquire meta-analysed summary statistics that exclude the target population. The target data that we use in this tutorial is a dataset of 176 cases and 188 controls for late-onset alzheimer's disease https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2667989/pdf/main.pdf . We have cleaned this target data according to our quality control protocol as outlined in tutorials 1 and 2 , after which 341 individuals remain (167 cases and 174 controls). provide download links to cleaned target files The GWAS summary statistics that we will use are from the largest and most recently published GWAS for alzheimer's disease (Jansen et al., 2019). Click here to download the GWAS summary statistics estimated (AD_sumstats_Jansenetal_2019sept.txt.gz), which you will use as your weights for the polygenic score. Note that this is a GWAS, not on late-onset alzheimer's disease, but on a combination of alzheimer's disease and alzheimer's disease by proxy (i.e. through parental diagnoses). The construction of a PRS on a different, but related phenotype may reduce its overall predictiveness, but is still a useful endaveour as long as the phenotypes in the GWAS and target dataset are genetically related. Phenotypes are often coded differently in GWAS to maximize sample size and harmonize different datasets. One of the uses of a PRS is that they can be applied to study the relationships between genotype and phenotype in a smaller dataset in which phenotypes may be recorded in more detail. Links to install software here (GCTA,PLINK,LDpred) Before you start, make sure the clean target data file in plink binary format (.bed, .bim and .fam) and the GWAS summary statistics (.txt) are in your working directory. We define a TARGETSET and GWASSET variable at the start, to easily refer to our target and GWAS summary data, respectively. TARGETSET=adgwas.qcout.clean GWASSET=AD_sumstats_Jansenetal_2019sept.txt","title":"Background"},{"location":"prs/#qc-of-gwas-summary-data","text":"First, let's look at the first 5 lines of the GWAS summary statistics head -5 $GWASSET uniqID.a1a2 CHR BP A1 A2 SNP Z P Nsum Neff dir EAF BETA SE 1:715265_T_C 1 715265 T C rs12184267 2.12197306477 0.03384 359856 359856 ??+? 0.0408069 0.0126426452822487 0.0059579668998386 1:715367_G_A 1 715367 G A rs12184277 1.95791489337 0.05024 360230 360230 ??+? 0.0410687 0.0116235111669331 0.00593667845639937 1:717485_A_C 1 717485 A C rs12184279 1.91243829548 0.05582 360257 360257 ??+? 0.0405759 0.0114189092819899 0.0059708641627697 1:720381_T_G 1 720381 T G rs116801199 2.29540369149 0.02171 360980 360980 ??+? 0.042162 0.0134428918549354 0.00585643906767846 The ReadMe of the GWAS summary data clarifies each of these columns. In this tutorial, we will only use the following columns: - SNP : the rsID for each tested SNP - BP : the SNP's base position - A1 : the reference allele - A2 : the alternative allele - Z : The Z-score - BETA : the estimated coefficient of the SNP on the diagnosis for alzheimer's disease - P : the P-value associatied with BETA - Nsum The sample size for the particular SNP. We first define some variables such that we can easily to refer to the columns we need in our code: BPCOLNO=3 REFACOLNO=4 ALTACOLNO=5 SNPCOLNO=6 PVALCOLNO=8 BETACOLNO=13 ZCOLNO=7 NCOLNO=9","title":"QC of GWAS summary data"},{"location":"prs/#remove-duplicate-snps-from-gwas-summary-statistics","text":"Most GWAS software packages, such as PLINK, do not allow for duplicate SNPs in the GWAS summary data. We will obtain a list of rsIDs using awk, sort these SNPs, and extract the repeated lines using ``uniq -d'', saving these into a new file called duplicate.snp. We next use grep to filter out these duplicate SNPs from our GWAS summary statistics. awk -v c1=$SNPCOLNO '{print $c1}' $GWASSET |\\ sort |\\ uniq -d > duplicate.snp grep -vf duplicate.snp $GWASSET > $GWASSET.nodup wc -l $GWASSET 13367300 AD_sumstats_Jansenetal_2019sept.txt wc -l $GWASSET.nodup 13336963 AD_sumstats_Jansenetal_2019sept.txt.nodup This filtering procedure leaves us with over 13 million SNPs that are non-duplicates. Duplicate SNPs occur, for example, due to coding mistakes, or when SNPs are multiallelic.","title":"Remove duplicate SNPs from GWAS summary statistics"},{"location":"prs/#further-cleaning-of-gwas-summary-statistics","text":"When constructing a PRS, it is of importance to ensure that the SNP weights reported in the GWAS summary statistics are correctly matched with the target data. Any SNPs that are incorrectly matched will be assigned a wrong weight, and thus introduce noise into the PRS. Here, we remove strand ambiguous alleles, as well as indels (here marked by \"I'/D\"). Furthermore, we only keep columns that have rsIDs. awk '!( ($4==\"A\" && $5==\"T\") || ($4==\"D\" && $5==\"I\") || \\ ($4==\"T\" && $5==\"A\") || ($4==\"I\" && $5==\"D\") || \\ ($4==\"G\" && $5==\"C\") || \\ ($4==\"C\" && $5==\"G\")) {print}' $GWASSET.nodup > $GWASSET.nodup.noambu awk 'NR==1' $GWASSET.nodup.noambu | awk -v c1=$SNPCOLNO '$c1 ~ /^rs/' $GWASSET.nodup.noambu > $GWASSET.nodup.noambu.rsid wc -l $GWASSET.nodup.noambu 10331009 AD_sumstats_Jansenetal_2019sept.txt.noambu wc -l $GWASSET.nodup.noambu.rsid 10227986 AD_sumstats_Jansenetal_2019sept.txt.nodup.noambu.rsid Notes on GWAS cleaning: irrelevant here: but don't include X, Y, or MT chromosome. If available, filter on info.","title":"Further cleaning of GWAS summary statistics"},{"location":"prs/#matching-cleaned-gwas-summary-statistics-with-target-data","text":"Our next goal is to take our target data, and only keep the SNPs that are present in the GWAS summary statistics. We first count how many SNPs there are in the GWAS summary and target data, respectively. ### wc -l $GWASSET.nodup.noambu.rsid 10227986 AD_sumstats_Jansenetal_2019sept.txt.nodup.noambu.rsid wc -l $TARGETSET.bim 191069 adgwas.qcout.clean.bim Resolve strand issues, flip alleles Note that only 191069 SNPs are present in our GWAS summary data ( wc -l $TARGETSET.bim ), such that we cannot use any of the 10036917 remaining SNPs. We do not need to observe all SNPs from our GWAS summary statistics in our target data to construct a useful PRS. Even the missingness of causal SNPs for alzheimer's disease in the target data is not a big problem as long as we have still genotyped nearby SNPs that are in linkage disequilibrium with such causal SPNs. Nonetheless, the low SNP density in our target dataset will negatively effect the predictive power of our PRS. The GWAS summary results do have a wide SNP coverage, which is of more crucial importance, such that we can attach PRS weights to each SNP in our limited target data set. Next, we use awk to get a list of SNP rsIDs and base pair positions from our GWAS summary statistics. We next take our target data, only keep the SNPs that are present in the GWAS summary statistics, and update their base pair positions using the update-map flag in plink. This updates the base pair positions from the build of the target data set (hg36), to the build of the GWAS summary data (hg37) check awk -v c1=$SNPCOLNO -v c2=$BPCOLNO '{print $c1,$c2}' $GWASSET.nodup.noambu.rsid > gwassnps.txt awk -v c1=$SNPCOLNO 'FNR>1{print $c1}' $GWASSET.nodup.noambu.rsid > snplist.txt plink --bfile $TARGETSET --extract snplist.txt --update-map gwassnps.txt --make-bed --out $TARGETSET.out wc -l $TARGETSET.out.bim 155949 adgwas.qcout.clean.out.bim 155,949 SNPs present in our target data set are also present in our GWAS summary data. Finally, we use awk and grep to restrict our gwas summary statistics to the SNPs that are also present in our target data. #Use only variables in GWAS summary stats that are also present in genome data. (but keep header of the original summary stats file) awk '{print $2}' $TARGETSET.out.bim > $TARGETSET.SNPs.txt head -1 $GWASSET.nodup > GWASanalysis.txt && grep -wFf $TARGETSET.SNPs.txt $GWASSET.nodup >> GWASanalysis.txt wc -l GWASanalysis.txt Finally, we flip reference alleles and resolve any strand issues. awk -v c1=$SNPCOLNO -v c2=$REFACOLNO 'FNR>1{print $c1,$c2}' GWASanalysis.txt > gwasreflist.txt plink --bfile $TARGETSET.out --reference-allele gwasreflist.txt --make-bed --out $TARGETSET.out.ref awk -v c1=$SNPCOLNO -v c2=$REFACOLNO -v c3=$ALTACOLNO '{print$c1,$c2,$c3}' GWASanalysis.txt > gwasstrandlist.txt awk '{print$2,$5,$6}' $TARGETSET.out.ref.bim > $TARGETSET.strandlist sort gwasstrandlist.txt $TARGETSET.strandlist |uniq -u > all_differences.txt awk '{print$1}' all_differences.txt | sort -u > flip_list.txt plink --bfile $TARGETSET.out.ref --flip flip_list.txt --reference-allele gwasreflist.txt --make-bed --out $TARGETSET.out.ref.strand Investigate problematic SNPs and throw them out: awk '{print$2,$5,$6}' $TARGETSET.out.ref.strand.bim > corrected_map_tmp sort gwasstrandlist.txt corrected_map_tmp |uniq -u > uncorresponding_SNPs.txt wc -l uncorresponding_SNPs.txt awk '{print$1}' uncorresponding_SNPs.txt | sort -u > SNPs_for_exclusion.txt plink --bfile $TARGETSET.out.ref.strand --exclude SNPs_for_exclusion.txt --make-bed --out $TARGETSET.def","title":"Matching cleaned GWAS summary statistics with target data."},{"location":"prs/#ldsc-to-assess-snp-based-heritability-and-confounding-in-gwas-summary-statistics","text":"","title":"LDSC to assess SNP-based heritability and confounding in GWAS summary statistics."},{"location":"prs/#gcta-to-estimate-snp-based-heritability","text":"Before constructing a PGS on the target data, it is useful to estimate the SNP-based heritability of the phenotype of interest using gcta. Due to measurement error in the PRS weights, the share of explained variance that the PRS can explain in a linear regression is lower than the overall SNP-based heritability. Therefore, estimation of SNP-based heritability gives us a reasonable expectation of the upper bound that we can expect from the predictiveness of the PRSs that we will construct. Genome-based restricted maximum likelihood (GREML) estiamtes the degree of variance explained in the phenotype that is explained by all SNPs in the target data. This is often referred to as SNP-based heritability. GCTA is a software package used to conduct such GREML analyses. It is similar to PLINK in the sense that it is operated through bash commands, using flags to guide the analysis of interest. Before using GCTA to estimate SNP-based heritability, it is advisable to conduct a power calculation using the associated power calculator: https://shiny.cnsgenomics.com/gctaPower/. To arrive at reasonable power (>80%), a sample size of 2000 individuals is roughly the minimum that is needed for most traits. The target data set here is notably smaller. With our 174 cases and 167 controls, and assuming a disease risk in the population of 0.1, a trait heritability of 0.5, alpha of 0.05, and the default variance of SNP-derived genetic relationships of 0.00002 gives us a dramatically low power of 0.0806. The standard error that we could expect for our analysis is 0.9760, which is incredibly large, given that a heritability estimate is bounded between 0 and 1 by definition. Hence, in our current data set, the estimation of SNP-based heritability using GREML is a useless endevaour. We nonetheless perform the GREML analysis anyway to illustrate how it is done in a dataset that is sufficiently large. In the following lines of code, we first collect the family and individual IDs, and the phenotype data (case or control) into a new file (phenotype.phen). We next invoke gcta to estimate genome-wide relatedness matrix (using the --make-grm flag), which serves as an input to the heritability estimation. The estimation of the GRM is very sensitive to the inclusion of cryptic related individuals. We use the --grm-cutoff flag to throuw out individuals with relatedness value larger than 0.025. This is a more conservative parameter setting than the one included in our QC pipeline. We next invoke the --reml flag in gcta64 to estimate the the SNP-based heritability. The necessary inputs are the GRM estimated in the line of code before, using the --grm flag, and the phenotype file using the --pheno flag. awk '{print $1,$2,$6}' $TARGETSET.def.fam > phenotype.phen ##To DO: visualize the phenotype here (write R-script) ##Estimate heritability using GCTA (GREML) (GCTA-GREML power calculator?) --> TO DO: Double-check we exclude close relatives. Check Yang et al. (2017). GRMCUT=0.025 gcta64 --bfile $TARGETSET.def --autosome --grm-cutoff $GRMCUT --make-grm --out $TARGETSET.grm gcta64 --reml --grm $TARGETSET.grm --pheno phenotype.phen --out greml The resulting output: ******************************************************************* * Genome-wide Complex Trait Analysis (GCTA) * version 1.93.2 beta Linux * (C) 2010-present, Jian Yang, The University of Queensland * Please report bugs to Jian Yang <jian.yang.qt@gmail.com> ******************************************************************* Analysis started at 15:54:59 CEST on Mon Mar 29 2021. Hostname: int1.bullx Accepted options: --reml --grm adgwas.qcout.clean.grm --pheno phenotype.phen --out greml Note: This is a multi-thread program. You could specify the number of threads by the --thread-num option to speed up the computation if there are multiple processors in your machine. Reading IDs of the GRM from [adgwas.qcout.clean.grm.grm.id]. 341 IDs read from [adgwas.qcout.clean.grm.grm.id]. Reading the GRM from [adgwas.qcout.clean.grm.grm.bin]. GRM for 341 individuals are included from [adgwas.qcout.clean.grm.grm.bin]. Reading phenotypes from [phenotype.phen]. Non-missing phenotypes of 341 individuals are included from [phenotype.phen]. Assuming a disease phenotype for a case-control study: 167 cases and 174 controls Note: you can specify the disease prevalence by the option --prevalence so that GCTA can transform the variance explained to the underlying liability scale. 341 individuals are in common in these files. Performing REML analysis ... (Note: may take hours depending on sample size). 341 observations, 1 fixed effect(s), and 2 variance component(s)(including residual variance). Calculating prior values of variance components by EM-REML ... Updated prior values: 0.125147 0.124882 logL: 62.9277 Running AI-REML algorithm ... Iter. logL V(G) V(e) 1 62.93 0.15331 0.09642 2 63.00 0.21404 0.03527 3 63.06 0.21182 0.03765 4 63.06 0.21194 0.03754 5 63.06 0.21193 0.03754 Log-likelihood ratio converged. Calculating the logLikelihood for the reduced model ... (variance component 1 is dropped from the model) Calculating prior values of variance components by EM-REML ... Updated prior values: 0.25063 logL: 62.32649 Running AI-REML algorithm ... Iter. logL V(e) 1 62.33 0.25063 Log-likelihood ratio converged. Summary result of REML analysis: Source Variance SE V(G) 0.211931 0.176050 V(e) 0.037542 0.174494 Vp 0.249473 0.019152 V(G)/Vp 0.849515 0.699855 Sampling variance/covariance of the estimates of variance components: 3.099372e-02 -3.053760e-02 -3.053760e-02 3.044826e-02 Summary result of REML analysis has been saved in the file [greml.hsq]. Analysis finished at 15:54:59 CEST on Mon Mar 29 2021 Overall computational time: 0.09 sec. The output shows four estimates of interest: V(G), the amount of variance in the phenotype that can be attributed to variance in the SNPs, V(e), the remaining variance that can be attributed to environmental factors, their sum Vp, and estimated SNP-based heritability: V(G) over Vp. The estimated SNP-based heritability is very high: 0.85. However, as expected, the standard error around this estimate (~0.7) is so large that even very low heritabilities can not be ruled out. In sum, the sample size is too small to derive any conclusions about SNP based heritability.","title":"GCTA to estimate SNP-based heritability"},{"location":"prs/#create-a-prs-using-plink-clumpig-and-thresholding","text":"Compare to repository: https://www.pgscatalog.org/trait/EFO_0000249/ Use PRSIce or comment on different thresholds We are now ready to estimate our PRS. For each individual, we multiply their reference allele count at each SNP with a SNP weight estimated from the GWAS summary statistics. However, the GWAS coefficients as estimated in GWAS summary data are not corrected for linkage disequilibrium. Constructing a PRS without any correction for LD essentially leads to an overweighting of SNPs that are in dense LD-regions compared to SNPs that are not, resulting in lower predictability of the PRS. One method to deal with this is clumping. Clumping is a form of informed pruning: The R-squared between SNPs that reside within a given kb-window is computed, and one of the SNPs is thrown out of the R-squared is higher than a given threshold. The algorithm differs from pruning because it sorts all SNPs within a window increasingly by p-value, to ensure that SNPs with the lowest p-value are kept. The clumping algorithm gives a researcher substantial degrees of freedom when constructing a polygenic score. How to set the optimal parameters? Especially the choice of the p-value threshold has been shown to impact the predictiveness of PRSs, with stark differences in optimal threshold between different traits (Ware et al., 2017). Many researchers optimize over a grid of all possible parameter combinations. Automated tools, such as PRSICE2, are available for this. However, fitting many PRSs and choosing the best in terms of their evaluated predictive value in the target data set comes at the risk of overfitting the data. The smaller the dataset, the larger the risk of overfitting. We strongly suggest researchers to scan the literature first to see whether some consensus on optimal parameter values is available. For alzheimer's disease, many studies suggest that highly restrictive p-value thresholds (such that only the most significant SNPs for alzheimer's disease are included) result in the most predictive polygenic scores. Here, we follow Chaudhury et al. (2019), and set a p-value threshold of 0.000107, a window size of 250 kb, and an r-squared threshold of 0.1. plink --bfile $TARGETSET.def --clump-p1 0.000107 --clump-p2 0.000107 --clump-r2 0.1 --clump-kb 250 --clump GWASanalysis.txt --clump-snp-field SNP \\ --clump-field P --out $TARGETSET.clump #Extract list of clumped SNPs (no header): awk 'NR!=1{print $3}' $TARGETSET.clump.clumped > $TARGETSET.clump.snp wc -l $TARGETSET.clump.snp 76 adgwas.qcout.clean.clump.snp Clumping our target data set using the p-values from gwas summary statistics leaves 76 SNPs in our dataset. We will extract these SNPs from our dataset, and construct a PRS using plink. The --score flag in PLINK next takes the reference allele count of each individual in our target data set, multiplies this value by the coefficient estimated in the GWAS, and sums this result over all included 76 SNPs. awk -v c1=$SNPCOLNO -v c2=$REFACOLNO -v c3=$BETACOLNO 'FNR>1{print $c1,$c2,$c3}' GWASanalysis.txt > score.txt plink --bfile $TARGETSET.def --extract $TARGETSET.clump.snp --pheno phenotype.phen --score score.txt --out plink_score The output is a .score file, which summarizes the Polygenic score for each individual in the variable ``SCORE'': Before evaluating our PRSs by checking their predictability against the phenotype of interest, we estimate the first 10 principal components for each individual in our target data. These will be used as control variables. We also extract sex information from the .fam file as additional covariates in our regressions. plink --bfile $TARGETSET.def --pca 10 header --out clusters awk '{print $1,$2,$5}' $TARGETSET.def.fam > temp.txt awk 'BEGIN{print \"FID IID SEX\"}1' temp.txt > sex.txt We are now ready to load the score file in R, merge with the principal components, and evaluating the predictiveness of the PRSs. We standardize our PRSs such that they have mean 0 and standard deviation 1, and change the coding of our phenotype such that cases are equal to 1 and controls are equal to zero. We first estimate a null model that regresses the phenotype on the first ten PCs and sex, using both a linear probability model and a logit specification. We next estimate four models: a linear probability regressing the phenotype on the PRS only, one with all controls, and two similar specifications using a logit model. Performed in R library(stargazer) data <- read.table(file=\"plink_score.profile\",header=TRUE) clusterdata <- read.table(file=\"clusters.eigenvec\",header=TRUE) sexdata <- read.table(file=\"sex.txt\",header=TRUE) mergedata <- merge(data,clusterdata) mergedata <- merge(mergedata,sexdata) #standardize data mergedata$SCORE <- (mergedata$SCORE-mean(mergedata$SCORE))/(sd(mergedata$SCORE)) mergedata$PHENO <- mergedata$PHENO - 1 nullmodel<-lm(PHENO~PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10+SEX,data=mergedata) nullr2<-summary(nullmodel)$r.squared model<-lm(PHENO~SCORE,data=mergedata) modelcontrols<-lm(PHENO~SCORE+PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10+SEX,data=mergedata) model$IncreaseR2<-round(summary(model)$r.squared,digits=3) modelcontrols$IncreaseR2<-round(summary(modelcontrols)$r.squared - nullr2,digits=3) logitmodel<-glm(PHENO~SCORE,data=mergedata,family=\"binomial\") logitmodelcontrols<-glm(PHENO~SCORE+PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10+SEX,data=mergedata,family=\"binomial\") nullmodellogit<-glm(PHENO~PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10+SEX,data=mergedata,family=\"binomial\") nullpseudor2<-round(1-(nullmodellogit$deviance/nullmodellogit$null.deviance),digits=3) logitmodel$PseudoR2<-round(1-(logitmodel$deviance/logitmodel$null.deviance),digits=3) logitmodelcontrols$PseudoR2<-round(1-(logitmodelcontrols$deviance/logitmodelcontrols$null.deviance),digits=3) logitmodel$IncreaseR2<-logitmodel$PseudoR2 logitmodelcontrols$IncreaseR2<-logitmodelcontrols$PseudoR2 - nullpseudor2 stargazer(model,modelcontrols,logitmodel,logitmodelcontrols,type=\"text\", out=\"PLINKScore.txt\",keep = c(\"SCORE\"), add.lines = list(c(\"10 PCs\", \"No\", \"Yes\", \"No\", \"Yes\"),c(\"Pseudo-R2\",\"\",\"\",logitmodel$PseudoR2,logitmodelcontrols$PseudoR2),c(\"Increase-R2\",model$IncreaseR2,modelcontrols$IncreaseR2,logitmodel$IncreaseR2,logitmodelcontrols$IncreaseR2)), star.cutoffs = c(0.05, 0.01, 0.001), float=FALSE)","title":"Create a PRS using PLINK (clumpig and thresholding):"},{"location":"prs/#create-a-prs-using-ldpred","text":"python $PREDPATH coord --rs SNP --A1 A1 --A2 A2 --pos BP --eff_type LINREG --chr CHR --pval P --eff BETA --N $NGWAS --ssf GWASanalysis.txt --gf $TARGETSET.out --out pred.coord ## LDpred recommend radius to be Total number of SNPs in target / 3000 (CHECK!) GSize=\"$(wc -l <\"$TARGETSET.out.bim\")\" ldrnum=$(( GSize / 3000 )) python $PREDPATH gibbs --cf pred.coord --ldr $ldrnum --ldf pred.ld --out pred.weight --N $NGWAS echo \"python3 $PREDPATH score --gf $FILESET$out --rf test.weight --out test.score --pf phenotype.phen --pf-format LSTANDARD\" python $PREDPATH score --gf $TARGETSET.out --rf pred.weight --out test.score --pf phenotype.phen --pf-format LSTANDARD --pcs-file clusters.eigenvec ##P+T for reference: python $PREDPATH p+t --cf pred.coord --ldr $ldrnum --out PTpred python $PREDPATH score --gf $TARGETSET.out --rf PTpred --out PTscore.score --pf phenotype.phen --pf-format \\ LSTANDARD --pcs-file clusters.eigenvec Performed in R library(stargazer) data <- read.table(file=\"test.score_LDpred-inf.txt\",header=TRUE, sep = \",\") data$PRS <- (data$PRS-mean(data$PRS))/(sd(data$PRS)) data$true_phens <- data$true_phens - 1 #data$true_phens <- (data$true_phens-mean(data$true_phens))/(sd(data$true_phens)) model<-lm(true_phens~PRS,data=data) logitmodel<-glm(true_phens~PRS,data=data,family=\"binomial\") modelcontrols<-lm(true_phens~PRS+PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10,data=data) logitmodelcontrols<-glm(true_phens~PRS+PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10,data=data,family=\"binomial\") logitmodel$PseudoR2<-round(1-(logitmodel$deviance/logitmodel$null.deviance),digits=3) logitmodelcontrols$PseudoR2<-round(1-(logitmodelcontrols$deviance/logitmodelcontrols$null.deviance),digits=3) stargazer(model,modelcontrols,logitmodel,logitmodelcontrols, out=\"LDPREDstat.tex\",keep = c(\"PRS\"), add.lines = list(c(\"10 PCs\", \"No\", \"Yes\", \"No\", \"Yes\"),c(\"Pseudo-R2\",\"\",\"\",logitmodel$PseudoR2,logitmodelcontrols$PseudoR2)), star.cutoffs = c(0.05, 0.01, 0.001), float=FALSE)","title":"Create a PRS using LDPred"},{"location":"prs/#notes-on-power","text":"Contrast with some results from a larger dataset (e.g. UKB?)","title":"Notes on power:"},{"location":"prs/#notes-on-confouding-within-family-analysis-etc","text":"","title":"Notes on confouding, within-family analysis, etc."},{"location":"prs/#notes-on-downward-bias-due-to-measurement-error-genetic-iv","text":"","title":"Notes on downward bias due to measurement error: Genetic IV?"},{"location":"prs/#further-topics-genetic-correlations-genomic-sem","text":"","title":"Further topics? Genetic correlations, Genomic SEM"},{"location":"prs/#references","text":"Chaudhury, S., Brookes, K. J., Patel, T., Fallows, A., Guetta-Baranes, T., Turton, J. C., ... & Thomas, A. J. (2019). Alzheimer's disease polygenic risk score as a predictor of conversion from mild-cognitive impairment. Translational psychiatry, 9(1), 1-7. Ware, E. B., Schmitz, L. L., Faul, J., Gard, A., Mitchell, C., Smith, J. A., ... & Kardia, S. L. (2017). Heterogeneity in polygenic scores for common human traits. BioRxiv, 106062.","title":"References"},{"location":"prsice/","text":"Background \u00b6 PRSice-2 is one of the dedicated PRS programs which automates many of the steps from the previous page that used a sequence of PLINK functions (plus some QC steps). On this page you will run a PRS analysis using PRSice-2, which implements the standard C+T method. Obtaining PRSice-2 \u00b6 PRSice-2 can be downloaded from: Operating System Link Linux 64-bit v2.3.3 OS X 64-bit v2.3.3 and can be directly used after extracting the file. In this tutorial, you will only need PRSice.R and PRSice_XXX where XXX is the operation system Required Data \u00b6 This analysis assumes that you have the following files (or you can download it from here ): File Name Description Height.QC.gz The post QC base data file. While PRSice-2 can automatically apply most filtering on the base file, it cannot remove duplicated SNPs EUR.QC.bed This file contains the genotype data that passed the QC steps EUR.QC.bim This file contains the list of SNPs that passed the QC steps EUR.QC.fam This file contains the samples that passed the QC steps EUR.height This file contains the phenotype data of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the principal components (PCs) of the samples Running PRS analysis \u00b6 To run PRSice-2 we need a single covariate file, and therefore our covariate file and PCs file should be combined. This can be done with R as follows: without data.table covariate <- read.table(\"EUR.cov\", header=T) pcs <- read.table(\"EUR.eigenvec\", header=F) colnames(pcs) <- c(\"FID\",\"IID\", paste0(\"PC\",1:6)) cov <- merge(covariate, pcs, by=c(\"FID\", \"IID\")) write.table(cov,\"EUR.covariate\", quote=F, row.names=F) q() with data.table library(data.table) covariate <- fread(\"EUR.cov\") pcs <- fread(\"EUR.eigenvec\", header=F) colnames(pcs) <- c(\"FID\",\"IID\", paste0(\"PC\",1:6)) cov <- merge(covariate, pcs) fwrite(cov,\"EUR.covariate\", sep=\"\\t\") q() which generates EUR.cov . PRSice-2 can then be run to obtain the PRS results as follows: Linux Rscript PRSice.R \\ --prsice PRSice_linux \\ --base Height.QC.gz \\ --target EUR.QC \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.covariate \\ --base-maf MAF:0.01 \\ --base-info INFO:0.8 \\ --stat OR \\ --or \\ --out EUR OS X Rscript PRSice.R \\ --prsice PRSice_mac \\ --base Height.QC.gz \\ --target EUR.QC \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.covariate \\ --base-maf MAF:0.01 \\ --base-info INFO:0.8 \\ --stat OR \\ --or \\ --out EUR Windows Rscript PRSice.R ^ --prsice PRSice_win64.exe ^ --base Height.QC.gz ^ --target EUR.QC ^ --binary-target F ^ --pheno EUR.height ^ --cov EUR.covariate ^ --base-maf MAF:0.05 ^ --base-info INFO:0.8 ^ --stat OR ^ --or ^ --out EUR The meaning of the parameters are as follow: Paramter Value Description prsice PRSice_xxx Informs PRSice.R that the location of the PRSice binary base Height.QC.gz Informs PRSice that the name of the GWAS summary statistic target EUR.QC Informs PRSice that the input genotype files should have a prefix of EUR.QC binary-target F Indicate if the phenotype of interest is a binary trait. F for no pheno EUR.height Provide PRSice with the phenotype file cov EUR.covariate Provide PRSice with the covariate file base-maf MAF:0.05 Filter out SNPs with MAF < 0.05 in the GWAS summary statistics, using information in the MAF column base-info INFO:0.8 Filter out SNPs with INFO < 0.8 in the GWAS summary statistics, using information in the INFO column stat OR Column name of the column containing the effect size or - Inform PRSice that the effect size is an Odd Ratio out EUR Informs PRSice that all output should have a prefix of EUR This will automatically perform \"high-resolution scoring\" and generate the \"best-fit\" PRS (in EUR.best ), with associated plots of the results. Users should read Section 4.6 of our paper to learn more about issues relating to overfitting in PRS analyses. Which P-value threshold generates the \"best-fit\" PRS? 0.13995 How much phenotypic variation does the \"best-fit\" PRS explain? 0.166117 lassosum is one of the dedicated PRS programs which is an R package that uses penalised regression (LASSO) in its approach to PRS calculation. Installing lassosum \u00b6 Note The script used here is based on lassosum version 0.4.4 Note For more details, please refer to lassosum's homepage You can install lassosum and its dependencies in R with the following command: install.packages(c(\"devtools\",\"RcppArmadillo\", \"data.table\", \"Matrix\"), dependencies=TRUE) library(devtools) install_github(\"tshmak/lassosum\") Required Data \u00b6 Again, we assume that we have the following files (or you can download it from here ): File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Running PRS analysis \u00b6 We can run lassosum as follows: library(lassosum) # Prefer to work with data.table as it speeds up file reading library(data.table) library(methods) library(magrittr) # For multi-threading, you can use the parallel package and # invoke cl which is then passed to lassosum.pipeline library(parallel) # This will invoke 2 threads. cl <- makeCluster(2) sum.stat <- \"Height.QC.gz\" bfile <- \"EUR.QC\" # Read in and process the covariates covariate <- fread(\"EUR.cov\") pcs <- fread(\"EUR.eigenvec\") %>% setnames(., colnames(.), c(\"FID\",\"IID\", paste0(\"PC\",1:6))) # Need as.data.frame here as lassosum doesn't handle data.table # covariates very well cov <- merge(covariate, pcs) # We will need the EUR.hg19 file provided by lassosum # which are LD regions defined in Berisa and Pickrell (2015) for the European population and the hg19 genome. ld.file <- \"EUR.hg19\" # output prefix prefix <- \"EUR\" # Read in the target phenotype file target.pheno <- fread(\"EUR.height\")[,c(\"FID\", \"IID\", \"Height\")] # Read in the summary statistics ss <- fread(sum.stat) # Remove P-value = 0, which causes problem in the transformation ss <- ss[!P == 0] # Transform the P-values into correlation cor <- p2cor(p = ss$P, n = ss$N, sign = log(ss$OR) ) fam <- fread(paste0(bfile, \".fam\")) fam[,ID:=do.call(paste, c(.SD, sep=\":\")),.SDcols=c(1:2)] # Run the lassosum pipeline # The cluster parameter is used for multi-threading # You can ignore that if you do not wish to perform multi-threaded processing out <- lassosum.pipeline( cor = cor, chr = ss$CHR, pos = ss$BP, A1 = ss$A1, A2 = ss$A2, ref.bfile = bfile, test.bfile = bfile, LDblocks = ld.file, cluster=cl ) # Store the R2 results target.res <- validate(out, pheno = target.pheno, covar=cov) # Get the maximum R2 r2 <- max(target.res$validation.table$value)^2 How much phenotypic variation does the \"best-fit\" PRS explain? 0.2395471 # Sample size \u00b6 We recommend that users only perform PRS analyses on target data of at least 100 individuals. The sample size of our target data here is 503 individuals. # File transfer \u00b6 Usually we do not need to download and transfer the target data file because it is typically generated locally. However, the file should contain an md5sum code in case we send the data file to collaborators who may want to confirm that the file has not changed during the transfer. What is the md5sum code for each of the target files? File md5sum EUR.bed 98bcef133f683b1272d3ea5f97742e0e EUR.bim 6b286002904880055a9c94e01522f059 EUR.cov 85ed18288c708e095385418517e9c3bd EUR.fam e7b856f0c7bcaffc8405926d08386e97 EUR.height dd445ce969a81cded20da5c88b82d4df # Genome build \u00b6 As stated in the base data section, the genome build for our base and target data is the same, as it should be. # Standard GWAS QC \u00b6 The target data must be quality controlled to at least the standards implemented in GWAS studies, e.g. removing SNPs with low genotyping rate, low minor allele frequency, out of Hardy-Weinberg Equilibrium, removing individuals with low genotyping rate (see Marees et al ). The following plink command applies some of these QC metrics to the target data: plink \\ --bfile EUR \\ --maf 0.01 \\ --hwe 1e-6 \\ --geno 0.01 \\ --mind 0.01 \\ --write-snplist \\ --make-just-fam \\ --out EUR.QC Each of the parameters corresponds to the following Paramter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR maf 0.01 Removes all SNPs with minor allele frequency less than 0.05. Genotyping errors typically have a larger influence on SNPs with low MAF. Studies with large sample sizes could apply a lower MAF threshold hwe 1e-6 Removes SNPs with low P-value from the Hardy-Weinberg Equilibrium Fisher's exact or chi-squared test. SNPs with significant P-values from the HWE test are more likely affected by genotyping error or the effects of natural selection. Filtering should be performed on the control samples to avoid filtering SNPs that are causal (under selection in cases). When phenotype information is included, plink will automatically perform the filtering in the controls. geno 0.01 Excludes SNPs that are missing in a high fraction of subjects. A two-stage filtering process is usually performed (see Marees et al ). mind 0.01 Excludes individuals who have a high rate of genotype missingness, since this may indicate problems in the DNA sample or processing. (see Marees et al for more details). make-just-fam - Informs plink to only generate the QC'ed sample name to avoid generating the .bed file. write-snplist - Informs plink to only generate the QC'ed SNP list to avoid generating the .bed file. out EUR.QC Informs plink that all output should have a prefix of EUR.QC How many SNPs and samples were filtered? 14 samples were removed due to a high rate of genotype missingness 5,353 SNP were removed due to missing genotype data 944 SNPs were removed due to being out of Hardy-Weinberg Equilibrium 5,061 SNPs were removed due to low minor allele frequency Note Normally, we can generate a new genotype file using the new sample list. However, this will use up a lot of storage space. Using plink 's --extract , --exclude , --keep , --remove , --make-just-fam and --write-snplist functions, we can work solely on the list of samples and SNPs without duplicating the genotype file, reducing the storage space usage. Very high or low heterozygosity rates in individuals could be due to DNA contamination or to high levels of inbreeding. Therefore, samples with extreme heterozygosity are typically removed prior to downstream analyses. First, we perform prunning to remove highly correlated SNPs: plink \\ --bfile EUR \\ --keep EUR.QC.fam \\ --extract EUR.QC.snplist \\ --indep-pairwise 200 50 0.25 \\ --out EUR.QC Each of the parameters corresponds to the following Paramter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR keep EUR.QC.fam Informs plink that we only want to use samples in EUR.QC.fam in the analysis extract EUR.QC.snplist Informs plink that we only want to use SNPs in EUR.QC.snplist in the analysis indep-pairwise 200 50 0.25 Informs plink that we wish to perform prunning with a window size of 200 variants, sliding across the genome with step size of 50 variants at a time, and filter out any SNPs with LD \\(r^2\\) higher than 0.25 out EUR.QC Informs plink that all output should have a prefix of EUR.QC This will generate two files 1) EUR.QC.prune.in and 2) EUR.QC.prune.out . All SNPs within EUR.QC.prune.in have a pairwise \\(r^2 < 0.25\\) . Heterozygosity rates can then be computed using plink : plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.fam \\ --het \\ --out EUR.QC This will generate the EUR.QC.het file, which contains F coefficient estimates for assessing heterozygosity. We will remove individuals with F coefficients that are more than 3 standard deviation (SD) units from the mean, which can be performed using the following R command (assuming that you have R downloaded, then you can open an R session by typing R in your terminal): Without library dat <- read.table(\"EUR.QC.het\", header=T) # Read in the EUR.het file, specify it has header m <- mean(dat$F) # Calculate the mean s <- sd(dat$F) # Calculate the SD valid <- subset(dat, F <= m+3*s & F >= m-3*s) # Get any samples with F coefficient within 3 SD of the population mean write.table(valid[,c(1,2)], \"EUR.valid.sample\", quote=F, row.names=F) # print FID and IID for valid samples q() # exit R With data.table library(data.table) # Read in file dat <- fread(\"EUR.QC.het\") # Get samples with F coefficient within 3 SD of the population mean valid <- dat[F<=mean(F)+3*sd(F) & F>=mean(F)-3*sd(F)] # print FID and IID for valid samples fwrite(valid[,c(\"FID\",\"IID\")], \"EUR.valid.sample\", sep=\"\\t\") q() # exit R How many samples were excluded due to high heterozygosity rate? 2 samples were excluded # Ambiguous SNPs \u00b6 These were removed during the base data QC. # Mismatching SNPs \u00b6 SNPs that have mismatching alleles reported in the base and target data may be resolvable by strand-flipping the alleles to their complementary alleles in e.g. the target data, such as for a SNP with A/C in the base data and G/T in the target. This can be achieved with the following steps: 1. Load the bim file, the summary statistic and the QC SNP list into R Without data.table # Read in bim file bim <- read.table(\"EUR.bim\") colnames(bim) <- c(\"CHR\", \"SNP\", \"CM\", \"BP\", \"B.A1\", \"B.A2\") # Read in QCed SNPs qc <- read.table(\"EUR.QC.snplist\", header = F, stringsAsFactors = F) # Read in the GWAS data height <- read.table(gzfile(\"Height.QC.gz\"), header = T, stringsAsFactors = F, sep=\"\\t\") # Change all alleles to upper case for easy comparison height$A1 <- toupper(height$A1) height$A2 <- toupper(height$A2) bim$B.A1 <- toupper(bim$B.A1) bim$B.A2 <- toupper(bim$B.A2) With data.table and magrittr # magrittr allow us to do piping, which help to reduce the # amount of intermediate data types library(data.table) library(magrittr) # Read in bim file bim <- fread(\"EUR.bim\") %>% # Note: . represents the output from previous step # The syntax here means, setnames of the data read from # the bim file, and replace the original column names by # the new names setnames(., colnames(.), c(\"CHR\", \"SNP\", \"CM\", \"BP\", \"B.A1\", \"B.A2\")) %>% # And immediately change the alleles to upper cases .[,c(\"B.A1\",\"B.A2\"):=list(toupper(B.A1), toupper(B.A2))] # Read in summary statistic data (require data.table v1.12.0+) height <- fread(\"Height.QC.gz\") %>% # And immediately change the alleles to upper cases .[,c(\"A1\",\"A2\"):=list(toupper(A1), toupper(A2))] # Read in QCed SNPs qc <- fread(\"EUR.QC.snplist\", header=F) 2. Identify SNPs that require strand flipping Without data.table # Merge summary statistic with target info <- merge(bim, height, by = c(\"SNP\", \"CHR\", \"BP\")) # Filter QCed SNPs info <- info[info$SNP %in% qc$V1,] # Function for finding the complementary allele complement <- function(x) { switch ( x, \"A\" = \"T\", \"C\" = \"G\", \"T\" = \"A\", \"G\" = \"C\", return(NA) ) } # Get SNPs that have the same alleles across base and target info.match <- subset(info, A1 == B.A1 & A2 == B.A2) # Identify SNPs that are complementary between base and target info$C.A1 <- sapply(info$B.A1, complement) info$C.A2 <- sapply(info$B.A2, complement) info.complement <- subset(info, A1 == C.A1 & A2 == C.A2) # Update the complementary alleles in the bim file # This allow us to match the allele in subsequent analysis complement.snps <- bim$SNP %in% info.complement$SNP bim[complement.snps,]$B.A1 <- sapply(bim[complement.snps,]$B.A1, complement) bim[complement.snps,]$B.A2 <- sapply(bim[complement.snps,]$B.A2, complement) With data.table and magrittr # Merge summary statistic with target info <- merge(bim, height, by=c(\"SNP\", \"CHR\", \"BP\")) %>% # And filter out QCed SNPs .[SNP %in% qc[,V1]] # Function for calculating the complementary allele complement <- function(x){ switch (x, \"A\" = \"T\", \"C\" = \"G\", \"T\" = \"A\", \"G\" = \"C\", return(NA) ) } # Get SNPs that have the same alleles across base and target info.match <- info[A1 == B.A1 & A2 == B.A2, SNP] # Identify SNPs that are complementary between base and target com.snps <- info[sapply(B.A1, complement) == A1 & sapply(B.A2, complement) == A2, SNP] # Now update the bim file bim[SNP %in% com.snps, c(\"B.A1\", \"B.A2\") := list(sapply(B.A1, complement), sapply(B.A2, complement))] 3. Identify SNPs that require recoding in the target (to ensure the coding allele in the target data is the effective allele in the base summary statistic) We assume that you have the following files (or you can download it from here ): File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Warning While we do provide a rough guide on how to perform LDpred on bed files separated into individual chromosomes, this script is untested and extra caution is required 0. Prepare workspace \u00b6 On some server, you might need to first use the following code in order to run LDpred with multi-thread prepare workspace and load bigsnpr library(bigsnpr) options(bigstatsr.check.parallel.blas = FALSE) options(default.nproc.blas = NULL) 1. Read in the phenotype and covariate files \u00b6 read in phenotype and covariates library(data.table) library(magrittr) phenotype <- fread(\"EUR.height\") covariate <- fread(\"EUR.cov\") pcs <- fread(\"EUR.eigenvec\") # rename columns colnames(pcs) <- c(\"FID\",\"IID\", paste0(\"PC\",1:6)) # generate required table pheno <- merge(phenotype, covariate) %>% merge(., pcs) 2. Obtain HapMap3 SNPs \u00b6 LDpred2 authors recommend restricting the analysis to only the HapMap3 SNPs load HapMap3 SNPs info <- readRDS(url(\"https://github.com/privefl/bigsnpr/raw/master/data-raw/hm3_variants.rds\")) 3. Load and transform the summary statistic file \u00b6 Load summary statistic file # Read in the summary statistic file sumstats <- bigreadr::fread2(\"Height.QC.gz\") # LDpred 2 require the header to follow the exact naming names(sumstats) <- c(\"chr\", \"pos\", \"rsid\", \"a1\", \"a0\", \"n_eff\", \"beta_se\", \"p\", \"OR\", \"INFO\", \"MAF\") # Transform the OR into log(OR) sumstats$beta <- log(sumstats$OR) # Filter out hapmap SNPs sumstats <- sumstats[sumstats$rsid%in% info$rsid,] Warning Here, we know the exact ordering of the summary statistics file. However, in many cases, the ordering of the summary statistics differ, thus one must rename the columns according to their actual ordering 3. Calculate the LD matrix \u00b6 Genome Wide bed file # Get maximum amount of cores NCORES <- nb_cores() # Open a temporary file tmp <- tempfile(tmpdir = \"tmp-data\") on.exit(file.remove(paste0(tmp, \".sbk\")), add = TRUE) # Initialize variables for storing the LD score and LD matrix corr <- NULL ld <- NULL # We want to know the ordering of samples in the bed file fam.order <- NULL # preprocess the bed file (only need to do once for each data set) snp_readBed(\"EUR.QC.bed\") # now attach the genotype object obj.bigSNP <- snp_attach(\"EUR.QC.rds\") # extract the SNP information from the genotype map <- obj.bigSNP$map[-3] names(map) <- c(\"chr\", \"rsid\", \"pos\", \"a1\", \"a0\") # perform SNP matching info_snp <- snp_match(sumstats, map) # Assign the genotype to a variable for easier downstream analysis genotype <- obj.bigSNP$genotypes # Rename the data structures CHR <- map$chr POS <- map$pos # get the CM information from 1000 Genome # will download the 1000G file to the current directory (\".\") POS2 <- snp_asGeneticPos(CHR, POS, dir = \".\") # calculate LD for (chr in 1:22) { # Extract SNPs that are included in the chromosome ind.chr <- which(info_snp$chr == chr) ind.chr2 <- info_snp$`_NUM_ID_`[ind.chr] # Calculate the LD corr0 <- snp_cor( genotype, ind.col = ind.chr2, ncores = NCORES, infos.pos = POS2[ind.chr2], size = 3 / 1000 ) if (chr == 1) { ld <- Matrix::colSums(corr0^2) corr <- as_SFBM(corr0, tmp) } else { ld <- c(ld, Matrix::colSums(corr0^2)) corr$add_columns(corr0, nrow(corr)) } } # We assume the fam order is the same across different chromosomes fam.order <- as.data.table(obj.bigSNP$fam) # Rename fam order setnames(fam.order, c(\"family.ID\", \"sample.ID\"), c(\"FID\", \"IID\")) Chromosome separated bed files # Get maximum amount of cores NCORES <- nb_cores() # Open a temporary file tmp <- tempfile(tmpdir = \"tmp-data\") on.exit(file.remove(paste0(tmp, \".sbk\")), add = TRUE) # Initialize variables for storing the LD score and LD matrix corr <- NULL ld <- NULL # We want to know the ordering of samples in the bed file info_snp <- NULL fam.order <- NULL for (chr in 1:22) { # preprocess the bed file (only need to do once for each data set) # Assuming the file naming is EUR_chr#.bed snp_readBed(paste0(\"EUR_chr\",chr,\".bed\")) # now attach the genotype object obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\".rds\")) # extract the SNP information from the genotype map <- obj.bigSNP$map[-3] names(map) <- c(\"chr\", \"rsid\", \"pos\", \"a1\", \"a0\") # perform SNP matching tmp_snp <- snp_match(sumstats[sumstats$chr==chr,], map) info_snp <- rbind(info_snp, tmp_snp) # Assign the genotype to a variable for easier downstream analysis genotype <- obj.bigSNP$genotypes # Rename the data structures CHR <- map$chr POS <- map$pos # get the CM information from 1000 Genome # will download the 1000G file to the current directory (\".\") POS2 <- snp_asGeneticPos(CHR, POS, dir = \".\") # calculate LD # Extract SNPs that are included in the chromosome ind.chr <- which(tmp_snp$chr == chr) ind.chr2 <- tmp_snp$`_NUM_ID_`[ind.chr] # Calculate the LD corr0 <- snp_cor( genotype, ind.col = ind.chr2, ncores = NCORES, infos.pos = POS2[ind.chr2], size = 3 / 1000 ) if (chr == 1) { ld <- Matrix::colSums(corr0^2) corr <- as_SFBM(corr0, tmp) } else { ld <- c(ld, Matrix::colSums(corr0^2)) corr$add_columns(corr0, nrow(corr)) } # We assume the fam order is the same across different chromosomes if(is.null(fam.order)){ fam.order <- as.data.table(obj.bigSNP$fam) } } # Rename fam order setnames(fam.order, c(\"family.ID\", \"sample.ID\"), c(\"FID\", \"IID\")) 4. Perform LD score regression \u00b6 Perform LD score regression df_beta <- info_snp[,c(\"beta\", \"beta_se\", \"n_eff\", \"_NUM_ID_\")] ldsc <- snp_ldsc( ld, length(ld), chi2 = (df_beta$beta / df_beta$beta_se)^2, sample_size = df_beta$n_eff, blocks = NULL) h2_est <- ldsc[[\"h2\"]] 5. Calculate the null R2 \u00b6 Calculate the null R2 (quantitative trait) # Reformat the phenotype file such that y is of the same order as the # sample ordering in the genotype file y <- pheno[fam.order, on = c(\"FID\", \"IID\")] # Calculate the null R2 # use glm for binary trait # (will also need the fmsb package to calculate the pseudo R2) null.model <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~Sex+\", .) %>% as.formula %>% lm(., data = y) %>% summary null.r2 <- null.model$r.squared Calculate the null R2 (binary trait) library(fmsb) # Reformat the phenotype file such that y is of the same order as the # sample ordering in the genotype file y <- pheno[fam.order, on = c(\"FID\", \"IID\")] # Calculate the null R2 # use glm for binary trait # (will also need the fmsb package to calculate the pseudo R2) null.model <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~Sex+\", .) %>% as.formula %>% glm(., data = y, family=binomial) %>% summary null.r2 <- fmsb::NagelkerkeR2(null.model) Important Scripts for binary trait analysis only serve as a reference as we have not simulate any binary traits. In addition, Nagelkerke \\(R^2\\) is biased when there are ascertainment of samples. For more information, please refer to this paper infinitesimal model beta_inf <- snp_ldpred2_inf(corr, df_beta, h2 = h2_est) grid model # Prepare data for grid model p_seq <- signif(seq_log(1e-4, 1, length.out = 17), 2) h2_seq <- round(h2_est * c(0.7, 1, 1.4), 4) grid.param <- expand.grid(p = p_seq, h2 = h2_seq, sparse = c(FALSE, TRUE)) # Get adjusted beta from grid model beta_grid <- snp_ldpred2_grid(corr, df_beta, grid.param, ncores = NCORES) auto model # Get adjusted beta from the auto model multi_auto <- snp_ldpred2_auto( corr, df_beta, h2_init = h2_est, vec_p_init = seq_log(1e-4, 0.9, length.out = NCORES), ncores = NCORES ) beta_auto <- sapply(multi_auto, function(auto) auto$beta_est) 7. Obtain model PRS \u00b6 Using Genome wide bed file \u00b6 infinitesimal model if(is.null(obj.bigSNP)){ obj.bigSNP <- snp_attach(\"EUR.QC.rds\") } genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) pred_inf <- big_prodVec( genotype, beta_inf, ind.row = ind.test, ind.col = info_snp$`_NUM_ID_`) grid model if(is.null(obj.bigSNP)){ obj.bigSNP <- snp_attach(\"EUR.QC.rds\") } genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) pred_grid <- big_prodMat( genotype, beta_grid, ind.col = info_snp$`_NUM_ID_`) auto model if(is.null(obj.bigSNP)){ obj.bigSNP <- snp_attach(\"EUR.QC.rds\") } genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) pred_auto <- big_prodMat(genotype, beta_auto, ind.row = ind.test, ind.col = info_snp$`_NUM_ID_`) # scale the PRS generated from AUTO pred_scaled <- apply(pred_auto, 2, sd) final_beta_auto <- rowMeans(beta_auto[, abs(pred_scaled - median(pred_scaled)) < 3 * mad(pred_scaled)]) pred_auto <- big_prodVec(genotype, final_beta_auto, ind.row = ind.test, ind.col = info_snp$`_NUM_ID_`) Using chromosome separated bed files \u00b6 infinitesimal model pred_inf <- NULL for(chr in 1:22){ obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\".rds\")) genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) # Extract SNPs in this chromosome chr.idx <- which(info_snp$chr == chr) ind.chr <- info_snp$`_NUM_ID_`[chr.idx] tmp <- big_prodVec(genotype, beta_inf[chr.idx], ind.row = ind.test, ind.col = ind.chr) if(is.null(pred_inf)){ pred_inf <- tmp }else{ pred_inf <- pred_inf + tmp } } grid model pred_grid <- NULL for(chr in 1:22){ obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\"_.rds\")) genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) # Extract SNPs in this chromosome chr.idx <- which(info_snp$chr == chr) ind.chr <- info_snp$`_NUM_ID_`[chr.idx] tmp <- big_prodMat( genotype, beta_grid[chr.idx], ind.col = ind.chr) if(is.null(pred_grid)){ pred_grid <- tmp }else{ pred_grid <- pred_grid + tmp } } auto model pred_auto <- NULL for(chr in 1:22){ obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\"_.rds\")) genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) # Extract SNPs in this chromosome chr.idx <- which(info_snp$chr == chr) ind.chr <- info_snp$`_NUM_ID_`[chr.idx] tmp <- big_prodMat(genotype, beta_auto[chr.idx], ind.row = ind.test, ind.col = ind.chr) # scale the PRS generated from AUTO pred_scaled <- apply(tmp, 2, sd) final_beta_auto <- rowMeans(beta_auto[chr.idx, abs(pred_scaled - median(pred_scaled)) < 3 * mad(pred_scaled)]) tmp <- big_prodVec(genotype, final_beta_auto, ind.row = ind.test, ind.col = ind.chr) if(is.null(pred_auto)){ pred_auto <- tmp }else{ pred_auto <- pred_auto + tmp } } 8. Get the final performance of the LDpred models \u00b6 infinitesimal model reg.formula <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~PRS+Sex+\", .) %>% as.formula reg.dat <- y reg.dat$PRS <- pred_inf inf.model <- lm(reg.formula, dat=reg.dat) %>% summary (result <- data.table( infinitesimal = inf.model$r.squared - null.r2, null = null.r2 )) grid model reg.formula <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~PRS+Sex+\", .) %>% as.formula reg.dat <- y max.r2 <- 0 for(i in 1:ncol(pred_grid)){ reg.dat$PRS <- pred_grid[,i] grid.model <- lm(reg.formula, dat=reg.dat) %>% summary if(max.r2 < grid.model$r.squared){ max.r2 <- grid.model$r.squared } } (result <- data.table( grid = max.r2 - null.r2, null = null.r2 )) auto model reg.formula <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~PRS+Sex+\", .) %>% as.formula reg.dat <- y reg.dat$PRS <- pred_auto auto.model <- lm(reg.formula, dat=reg.dat) %>% summary (result <- data.table( auto = auto.model$r.squared - null.r2, null = null.r2 )) How much phenotypic variation does the PRS from each model explain? Infinitesimal = 0.0100 Grid Model = 0.00180 Auto Model = 0.171","title":"Background"},{"location":"prsice/#background","text":"PRSice-2 is one of the dedicated PRS programs which automates many of the steps from the previous page that used a sequence of PLINK functions (plus some QC steps). On this page you will run a PRS analysis using PRSice-2, which implements the standard C+T method.","title":"Background"},{"location":"prsice/#obtaining-prsice-2","text":"PRSice-2 can be downloaded from: Operating System Link Linux 64-bit v2.3.3 OS X 64-bit v2.3.3 and can be directly used after extracting the file. In this tutorial, you will only need PRSice.R and PRSice_XXX where XXX is the operation system","title":"Obtaining PRSice-2"},{"location":"prsice/#required-data","text":"This analysis assumes that you have the following files (or you can download it from here ): File Name Description Height.QC.gz The post QC base data file. While PRSice-2 can automatically apply most filtering on the base file, it cannot remove duplicated SNPs EUR.QC.bed This file contains the genotype data that passed the QC steps EUR.QC.bim This file contains the list of SNPs that passed the QC steps EUR.QC.fam This file contains the samples that passed the QC steps EUR.height This file contains the phenotype data of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the principal components (PCs) of the samples","title":"Required Data"},{"location":"prsice/#running-prs-analysis","text":"To run PRSice-2 we need a single covariate file, and therefore our covariate file and PCs file should be combined. This can be done with R as follows: without data.table covariate <- read.table(\"EUR.cov\", header=T) pcs <- read.table(\"EUR.eigenvec\", header=F) colnames(pcs) <- c(\"FID\",\"IID\", paste0(\"PC\",1:6)) cov <- merge(covariate, pcs, by=c(\"FID\", \"IID\")) write.table(cov,\"EUR.covariate\", quote=F, row.names=F) q() with data.table library(data.table) covariate <- fread(\"EUR.cov\") pcs <- fread(\"EUR.eigenvec\", header=F) colnames(pcs) <- c(\"FID\",\"IID\", paste0(\"PC\",1:6)) cov <- merge(covariate, pcs) fwrite(cov,\"EUR.covariate\", sep=\"\\t\") q() which generates EUR.cov . PRSice-2 can then be run to obtain the PRS results as follows: Linux Rscript PRSice.R \\ --prsice PRSice_linux \\ --base Height.QC.gz \\ --target EUR.QC \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.covariate \\ --base-maf MAF:0.01 \\ --base-info INFO:0.8 \\ --stat OR \\ --or \\ --out EUR OS X Rscript PRSice.R \\ --prsice PRSice_mac \\ --base Height.QC.gz \\ --target EUR.QC \\ --binary-target F \\ --pheno EUR.height \\ --cov EUR.covariate \\ --base-maf MAF:0.01 \\ --base-info INFO:0.8 \\ --stat OR \\ --or \\ --out EUR Windows Rscript PRSice.R ^ --prsice PRSice_win64.exe ^ --base Height.QC.gz ^ --target EUR.QC ^ --binary-target F ^ --pheno EUR.height ^ --cov EUR.covariate ^ --base-maf MAF:0.05 ^ --base-info INFO:0.8 ^ --stat OR ^ --or ^ --out EUR The meaning of the parameters are as follow: Paramter Value Description prsice PRSice_xxx Informs PRSice.R that the location of the PRSice binary base Height.QC.gz Informs PRSice that the name of the GWAS summary statistic target EUR.QC Informs PRSice that the input genotype files should have a prefix of EUR.QC binary-target F Indicate if the phenotype of interest is a binary trait. F for no pheno EUR.height Provide PRSice with the phenotype file cov EUR.covariate Provide PRSice with the covariate file base-maf MAF:0.05 Filter out SNPs with MAF < 0.05 in the GWAS summary statistics, using information in the MAF column base-info INFO:0.8 Filter out SNPs with INFO < 0.8 in the GWAS summary statistics, using information in the INFO column stat OR Column name of the column containing the effect size or - Inform PRSice that the effect size is an Odd Ratio out EUR Informs PRSice that all output should have a prefix of EUR This will automatically perform \"high-resolution scoring\" and generate the \"best-fit\" PRS (in EUR.best ), with associated plots of the results. Users should read Section 4.6 of our paper to learn more about issues relating to overfitting in PRS analyses. Which P-value threshold generates the \"best-fit\" PRS? 0.13995 How much phenotypic variation does the \"best-fit\" PRS explain? 0.166117 lassosum is one of the dedicated PRS programs which is an R package that uses penalised regression (LASSO) in its approach to PRS calculation.","title":"Running PRS analysis"},{"location":"prsice/#installing-lassosum","text":"Note The script used here is based on lassosum version 0.4.4 Note For more details, please refer to lassosum's homepage You can install lassosum and its dependencies in R with the following command: install.packages(c(\"devtools\",\"RcppArmadillo\", \"data.table\", \"Matrix\"), dependencies=TRUE) library(devtools) install_github(\"tshmak/lassosum\")","title":"Installing lassosum"},{"location":"prsice/#required-data_1","text":"Again, we assume that we have the following files (or you can download it from here ): File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples","title":"Required Data"},{"location":"prsice/#running-prs-analysis_1","text":"We can run lassosum as follows: library(lassosum) # Prefer to work with data.table as it speeds up file reading library(data.table) library(methods) library(magrittr) # For multi-threading, you can use the parallel package and # invoke cl which is then passed to lassosum.pipeline library(parallel) # This will invoke 2 threads. cl <- makeCluster(2) sum.stat <- \"Height.QC.gz\" bfile <- \"EUR.QC\" # Read in and process the covariates covariate <- fread(\"EUR.cov\") pcs <- fread(\"EUR.eigenvec\") %>% setnames(., colnames(.), c(\"FID\",\"IID\", paste0(\"PC\",1:6))) # Need as.data.frame here as lassosum doesn't handle data.table # covariates very well cov <- merge(covariate, pcs) # We will need the EUR.hg19 file provided by lassosum # which are LD regions defined in Berisa and Pickrell (2015) for the European population and the hg19 genome. ld.file <- \"EUR.hg19\" # output prefix prefix <- \"EUR\" # Read in the target phenotype file target.pheno <- fread(\"EUR.height\")[,c(\"FID\", \"IID\", \"Height\")] # Read in the summary statistics ss <- fread(sum.stat) # Remove P-value = 0, which causes problem in the transformation ss <- ss[!P == 0] # Transform the P-values into correlation cor <- p2cor(p = ss$P, n = ss$N, sign = log(ss$OR) ) fam <- fread(paste0(bfile, \".fam\")) fam[,ID:=do.call(paste, c(.SD, sep=\":\")),.SDcols=c(1:2)] # Run the lassosum pipeline # The cluster parameter is used for multi-threading # You can ignore that if you do not wish to perform multi-threaded processing out <- lassosum.pipeline( cor = cor, chr = ss$CHR, pos = ss$BP, A1 = ss$A1, A2 = ss$A2, ref.bfile = bfile, test.bfile = bfile, LDblocks = ld.file, cluster=cl ) # Store the R2 results target.res <- validate(out, pheno = target.pheno, covar=cov) # Get the maximum R2 r2 <- max(target.res$validation.table$value)^2 How much phenotypic variation does the \"best-fit\" PRS explain? 0.2395471","title":"Running PRS analysis"},{"location":"prsice/#sample-size","text":"We recommend that users only perform PRS analyses on target data of at least 100 individuals. The sample size of our target data here is 503 individuals.","title":"# Sample size"},{"location":"prsice/#file-transfer","text":"Usually we do not need to download and transfer the target data file because it is typically generated locally. However, the file should contain an md5sum code in case we send the data file to collaborators who may want to confirm that the file has not changed during the transfer. What is the md5sum code for each of the target files? File md5sum EUR.bed 98bcef133f683b1272d3ea5f97742e0e EUR.bim 6b286002904880055a9c94e01522f059 EUR.cov 85ed18288c708e095385418517e9c3bd EUR.fam e7b856f0c7bcaffc8405926d08386e97 EUR.height dd445ce969a81cded20da5c88b82d4df","title":"# File transfer"},{"location":"prsice/#genome-build","text":"As stated in the base data section, the genome build for our base and target data is the same, as it should be.","title":"# Genome build"},{"location":"prsice/#standard-gwas-qc","text":"The target data must be quality controlled to at least the standards implemented in GWAS studies, e.g. removing SNPs with low genotyping rate, low minor allele frequency, out of Hardy-Weinberg Equilibrium, removing individuals with low genotyping rate (see Marees et al ). The following plink command applies some of these QC metrics to the target data: plink \\ --bfile EUR \\ --maf 0.01 \\ --hwe 1e-6 \\ --geno 0.01 \\ --mind 0.01 \\ --write-snplist \\ --make-just-fam \\ --out EUR.QC Each of the parameters corresponds to the following Paramter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR maf 0.01 Removes all SNPs with minor allele frequency less than 0.05. Genotyping errors typically have a larger influence on SNPs with low MAF. Studies with large sample sizes could apply a lower MAF threshold hwe 1e-6 Removes SNPs with low P-value from the Hardy-Weinberg Equilibrium Fisher's exact or chi-squared test. SNPs with significant P-values from the HWE test are more likely affected by genotyping error or the effects of natural selection. Filtering should be performed on the control samples to avoid filtering SNPs that are causal (under selection in cases). When phenotype information is included, plink will automatically perform the filtering in the controls. geno 0.01 Excludes SNPs that are missing in a high fraction of subjects. A two-stage filtering process is usually performed (see Marees et al ). mind 0.01 Excludes individuals who have a high rate of genotype missingness, since this may indicate problems in the DNA sample or processing. (see Marees et al for more details). make-just-fam - Informs plink to only generate the QC'ed sample name to avoid generating the .bed file. write-snplist - Informs plink to only generate the QC'ed SNP list to avoid generating the .bed file. out EUR.QC Informs plink that all output should have a prefix of EUR.QC How many SNPs and samples were filtered? 14 samples were removed due to a high rate of genotype missingness 5,353 SNP were removed due to missing genotype data 944 SNPs were removed due to being out of Hardy-Weinberg Equilibrium 5,061 SNPs were removed due to low minor allele frequency Note Normally, we can generate a new genotype file using the new sample list. However, this will use up a lot of storage space. Using plink 's --extract , --exclude , --keep , --remove , --make-just-fam and --write-snplist functions, we can work solely on the list of samples and SNPs without duplicating the genotype file, reducing the storage space usage. Very high or low heterozygosity rates in individuals could be due to DNA contamination or to high levels of inbreeding. Therefore, samples with extreme heterozygosity are typically removed prior to downstream analyses. First, we perform prunning to remove highly correlated SNPs: plink \\ --bfile EUR \\ --keep EUR.QC.fam \\ --extract EUR.QC.snplist \\ --indep-pairwise 200 50 0.25 \\ --out EUR.QC Each of the parameters corresponds to the following Paramter Value Description bfile EUR Informs plink that the input genotype files should have a prefix of EUR keep EUR.QC.fam Informs plink that we only want to use samples in EUR.QC.fam in the analysis extract EUR.QC.snplist Informs plink that we only want to use SNPs in EUR.QC.snplist in the analysis indep-pairwise 200 50 0.25 Informs plink that we wish to perform prunning with a window size of 200 variants, sliding across the genome with step size of 50 variants at a time, and filter out any SNPs with LD \\(r^2\\) higher than 0.25 out EUR.QC Informs plink that all output should have a prefix of EUR.QC This will generate two files 1) EUR.QC.prune.in and 2) EUR.QC.prune.out . All SNPs within EUR.QC.prune.in have a pairwise \\(r^2 < 0.25\\) . Heterozygosity rates can then be computed using plink : plink \\ --bfile EUR \\ --extract EUR.QC.prune.in \\ --keep EUR.QC.fam \\ --het \\ --out EUR.QC This will generate the EUR.QC.het file, which contains F coefficient estimates for assessing heterozygosity. We will remove individuals with F coefficients that are more than 3 standard deviation (SD) units from the mean, which can be performed using the following R command (assuming that you have R downloaded, then you can open an R session by typing R in your terminal): Without library dat <- read.table(\"EUR.QC.het\", header=T) # Read in the EUR.het file, specify it has header m <- mean(dat$F) # Calculate the mean s <- sd(dat$F) # Calculate the SD valid <- subset(dat, F <= m+3*s & F >= m-3*s) # Get any samples with F coefficient within 3 SD of the population mean write.table(valid[,c(1,2)], \"EUR.valid.sample\", quote=F, row.names=F) # print FID and IID for valid samples q() # exit R With data.table library(data.table) # Read in file dat <- fread(\"EUR.QC.het\") # Get samples with F coefficient within 3 SD of the population mean valid <- dat[F<=mean(F)+3*sd(F) & F>=mean(F)-3*sd(F)] # print FID and IID for valid samples fwrite(valid[,c(\"FID\",\"IID\")], \"EUR.valid.sample\", sep=\"\\t\") q() # exit R How many samples were excluded due to high heterozygosity rate? 2 samples were excluded","title":"# Standard GWAS QC"},{"location":"prsice/#ambiguous-snps","text":"These were removed during the base data QC.","title":"# Ambiguous SNPs"},{"location":"prsice/#mismatching-snps","text":"SNPs that have mismatching alleles reported in the base and target data may be resolvable by strand-flipping the alleles to their complementary alleles in e.g. the target data, such as for a SNP with A/C in the base data and G/T in the target. This can be achieved with the following steps: 1. Load the bim file, the summary statistic and the QC SNP list into R Without data.table # Read in bim file bim <- read.table(\"EUR.bim\") colnames(bim) <- c(\"CHR\", \"SNP\", \"CM\", \"BP\", \"B.A1\", \"B.A2\") # Read in QCed SNPs qc <- read.table(\"EUR.QC.snplist\", header = F, stringsAsFactors = F) # Read in the GWAS data height <- read.table(gzfile(\"Height.QC.gz\"), header = T, stringsAsFactors = F, sep=\"\\t\") # Change all alleles to upper case for easy comparison height$A1 <- toupper(height$A1) height$A2 <- toupper(height$A2) bim$B.A1 <- toupper(bim$B.A1) bim$B.A2 <- toupper(bim$B.A2) With data.table and magrittr # magrittr allow us to do piping, which help to reduce the # amount of intermediate data types library(data.table) library(magrittr) # Read in bim file bim <- fread(\"EUR.bim\") %>% # Note: . represents the output from previous step # The syntax here means, setnames of the data read from # the bim file, and replace the original column names by # the new names setnames(., colnames(.), c(\"CHR\", \"SNP\", \"CM\", \"BP\", \"B.A1\", \"B.A2\")) %>% # And immediately change the alleles to upper cases .[,c(\"B.A1\",\"B.A2\"):=list(toupper(B.A1), toupper(B.A2))] # Read in summary statistic data (require data.table v1.12.0+) height <- fread(\"Height.QC.gz\") %>% # And immediately change the alleles to upper cases .[,c(\"A1\",\"A2\"):=list(toupper(A1), toupper(A2))] # Read in QCed SNPs qc <- fread(\"EUR.QC.snplist\", header=F) 2. Identify SNPs that require strand flipping Without data.table # Merge summary statistic with target info <- merge(bim, height, by = c(\"SNP\", \"CHR\", \"BP\")) # Filter QCed SNPs info <- info[info$SNP %in% qc$V1,] # Function for finding the complementary allele complement <- function(x) { switch ( x, \"A\" = \"T\", \"C\" = \"G\", \"T\" = \"A\", \"G\" = \"C\", return(NA) ) } # Get SNPs that have the same alleles across base and target info.match <- subset(info, A1 == B.A1 & A2 == B.A2) # Identify SNPs that are complementary between base and target info$C.A1 <- sapply(info$B.A1, complement) info$C.A2 <- sapply(info$B.A2, complement) info.complement <- subset(info, A1 == C.A1 & A2 == C.A2) # Update the complementary alleles in the bim file # This allow us to match the allele in subsequent analysis complement.snps <- bim$SNP %in% info.complement$SNP bim[complement.snps,]$B.A1 <- sapply(bim[complement.snps,]$B.A1, complement) bim[complement.snps,]$B.A2 <- sapply(bim[complement.snps,]$B.A2, complement) With data.table and magrittr # Merge summary statistic with target info <- merge(bim, height, by=c(\"SNP\", \"CHR\", \"BP\")) %>% # And filter out QCed SNPs .[SNP %in% qc[,V1]] # Function for calculating the complementary allele complement <- function(x){ switch (x, \"A\" = \"T\", \"C\" = \"G\", \"T\" = \"A\", \"G\" = \"C\", return(NA) ) } # Get SNPs that have the same alleles across base and target info.match <- info[A1 == B.A1 & A2 == B.A2, SNP] # Identify SNPs that are complementary between base and target com.snps <- info[sapply(B.A1, complement) == A1 & sapply(B.A2, complement) == A2, SNP] # Now update the bim file bim[SNP %in% com.snps, c(\"B.A1\", \"B.A2\") := list(sapply(B.A1, complement), sapply(B.A2, complement))] 3. Identify SNPs that require recoding in the target (to ensure the coding allele in the target data is the effective allele in the base summary statistic) We assume that you have the following files (or you can download it from here ): File Name Description Height.QC.gz The post-QCed summary statistic EUR.QC.bed The genotype file after performing some basic filtering EUR.QC.bim This file contains the SNPs that passed the basic filtering EUR.QC.fam This file contains the samples that passed the basic filtering EUR.height This file contains the phenotype of the samples EUR.cov This file contains the covariates of the samples EUR.eigenvec This file contains the PCs of the samples Warning While we do provide a rough guide on how to perform LDpred on bed files separated into individual chromosomes, this script is untested and extra caution is required","title":"# Mismatching SNPs"},{"location":"prsice/#0-prepare-workspace","text":"On some server, you might need to first use the following code in order to run LDpred with multi-thread prepare workspace and load bigsnpr library(bigsnpr) options(bigstatsr.check.parallel.blas = FALSE) options(default.nproc.blas = NULL)","title":"0. Prepare workspace"},{"location":"prsice/#1-read-in-the-phenotype-and-covariate-files","text":"read in phenotype and covariates library(data.table) library(magrittr) phenotype <- fread(\"EUR.height\") covariate <- fread(\"EUR.cov\") pcs <- fread(\"EUR.eigenvec\") # rename columns colnames(pcs) <- c(\"FID\",\"IID\", paste0(\"PC\",1:6)) # generate required table pheno <- merge(phenotype, covariate) %>% merge(., pcs)","title":"1. Read in the phenotype and covariate files"},{"location":"prsice/#2-obtain-hapmap3-snps","text":"LDpred2 authors recommend restricting the analysis to only the HapMap3 SNPs load HapMap3 SNPs info <- readRDS(url(\"https://github.com/privefl/bigsnpr/raw/master/data-raw/hm3_variants.rds\"))","title":"2. Obtain HapMap3 SNPs"},{"location":"prsice/#3-load-and-transform-the-summary-statistic-file","text":"Load summary statistic file # Read in the summary statistic file sumstats <- bigreadr::fread2(\"Height.QC.gz\") # LDpred 2 require the header to follow the exact naming names(sumstats) <- c(\"chr\", \"pos\", \"rsid\", \"a1\", \"a0\", \"n_eff\", \"beta_se\", \"p\", \"OR\", \"INFO\", \"MAF\") # Transform the OR into log(OR) sumstats$beta <- log(sumstats$OR) # Filter out hapmap SNPs sumstats <- sumstats[sumstats$rsid%in% info$rsid,] Warning Here, we know the exact ordering of the summary statistics file. However, in many cases, the ordering of the summary statistics differ, thus one must rename the columns according to their actual ordering","title":"3. Load and transform the summary statistic file"},{"location":"prsice/#3-calculate-the-ld-matrix","text":"Genome Wide bed file # Get maximum amount of cores NCORES <- nb_cores() # Open a temporary file tmp <- tempfile(tmpdir = \"tmp-data\") on.exit(file.remove(paste0(tmp, \".sbk\")), add = TRUE) # Initialize variables for storing the LD score and LD matrix corr <- NULL ld <- NULL # We want to know the ordering of samples in the bed file fam.order <- NULL # preprocess the bed file (only need to do once for each data set) snp_readBed(\"EUR.QC.bed\") # now attach the genotype object obj.bigSNP <- snp_attach(\"EUR.QC.rds\") # extract the SNP information from the genotype map <- obj.bigSNP$map[-3] names(map) <- c(\"chr\", \"rsid\", \"pos\", \"a1\", \"a0\") # perform SNP matching info_snp <- snp_match(sumstats, map) # Assign the genotype to a variable for easier downstream analysis genotype <- obj.bigSNP$genotypes # Rename the data structures CHR <- map$chr POS <- map$pos # get the CM information from 1000 Genome # will download the 1000G file to the current directory (\".\") POS2 <- snp_asGeneticPos(CHR, POS, dir = \".\") # calculate LD for (chr in 1:22) { # Extract SNPs that are included in the chromosome ind.chr <- which(info_snp$chr == chr) ind.chr2 <- info_snp$`_NUM_ID_`[ind.chr] # Calculate the LD corr0 <- snp_cor( genotype, ind.col = ind.chr2, ncores = NCORES, infos.pos = POS2[ind.chr2], size = 3 / 1000 ) if (chr == 1) { ld <- Matrix::colSums(corr0^2) corr <- as_SFBM(corr0, tmp) } else { ld <- c(ld, Matrix::colSums(corr0^2)) corr$add_columns(corr0, nrow(corr)) } } # We assume the fam order is the same across different chromosomes fam.order <- as.data.table(obj.bigSNP$fam) # Rename fam order setnames(fam.order, c(\"family.ID\", \"sample.ID\"), c(\"FID\", \"IID\")) Chromosome separated bed files # Get maximum amount of cores NCORES <- nb_cores() # Open a temporary file tmp <- tempfile(tmpdir = \"tmp-data\") on.exit(file.remove(paste0(tmp, \".sbk\")), add = TRUE) # Initialize variables for storing the LD score and LD matrix corr <- NULL ld <- NULL # We want to know the ordering of samples in the bed file info_snp <- NULL fam.order <- NULL for (chr in 1:22) { # preprocess the bed file (only need to do once for each data set) # Assuming the file naming is EUR_chr#.bed snp_readBed(paste0(\"EUR_chr\",chr,\".bed\")) # now attach the genotype object obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\".rds\")) # extract the SNP information from the genotype map <- obj.bigSNP$map[-3] names(map) <- c(\"chr\", \"rsid\", \"pos\", \"a1\", \"a0\") # perform SNP matching tmp_snp <- snp_match(sumstats[sumstats$chr==chr,], map) info_snp <- rbind(info_snp, tmp_snp) # Assign the genotype to a variable for easier downstream analysis genotype <- obj.bigSNP$genotypes # Rename the data structures CHR <- map$chr POS <- map$pos # get the CM information from 1000 Genome # will download the 1000G file to the current directory (\".\") POS2 <- snp_asGeneticPos(CHR, POS, dir = \".\") # calculate LD # Extract SNPs that are included in the chromosome ind.chr <- which(tmp_snp$chr == chr) ind.chr2 <- tmp_snp$`_NUM_ID_`[ind.chr] # Calculate the LD corr0 <- snp_cor( genotype, ind.col = ind.chr2, ncores = NCORES, infos.pos = POS2[ind.chr2], size = 3 / 1000 ) if (chr == 1) { ld <- Matrix::colSums(corr0^2) corr <- as_SFBM(corr0, tmp) } else { ld <- c(ld, Matrix::colSums(corr0^2)) corr$add_columns(corr0, nrow(corr)) } # We assume the fam order is the same across different chromosomes if(is.null(fam.order)){ fam.order <- as.data.table(obj.bigSNP$fam) } } # Rename fam order setnames(fam.order, c(\"family.ID\", \"sample.ID\"), c(\"FID\", \"IID\"))","title":"3. Calculate the LD matrix"},{"location":"prsice/#4-perform-ld-score-regression","text":"Perform LD score regression df_beta <- info_snp[,c(\"beta\", \"beta_se\", \"n_eff\", \"_NUM_ID_\")] ldsc <- snp_ldsc( ld, length(ld), chi2 = (df_beta$beta / df_beta$beta_se)^2, sample_size = df_beta$n_eff, blocks = NULL) h2_est <- ldsc[[\"h2\"]]","title":"4. Perform LD score regression"},{"location":"prsice/#5-calculate-the-null-r2","text":"Calculate the null R2 (quantitative trait) # Reformat the phenotype file such that y is of the same order as the # sample ordering in the genotype file y <- pheno[fam.order, on = c(\"FID\", \"IID\")] # Calculate the null R2 # use glm for binary trait # (will also need the fmsb package to calculate the pseudo R2) null.model <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~Sex+\", .) %>% as.formula %>% lm(., data = y) %>% summary null.r2 <- null.model$r.squared Calculate the null R2 (binary trait) library(fmsb) # Reformat the phenotype file such that y is of the same order as the # sample ordering in the genotype file y <- pheno[fam.order, on = c(\"FID\", \"IID\")] # Calculate the null R2 # use glm for binary trait # (will also need the fmsb package to calculate the pseudo R2) null.model <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~Sex+\", .) %>% as.formula %>% glm(., data = y, family=binomial) %>% summary null.r2 <- fmsb::NagelkerkeR2(null.model) Important Scripts for binary trait analysis only serve as a reference as we have not simulate any binary traits. In addition, Nagelkerke \\(R^2\\) is biased when there are ascertainment of samples. For more information, please refer to this paper infinitesimal model beta_inf <- snp_ldpred2_inf(corr, df_beta, h2 = h2_est) grid model # Prepare data for grid model p_seq <- signif(seq_log(1e-4, 1, length.out = 17), 2) h2_seq <- round(h2_est * c(0.7, 1, 1.4), 4) grid.param <- expand.grid(p = p_seq, h2 = h2_seq, sparse = c(FALSE, TRUE)) # Get adjusted beta from grid model beta_grid <- snp_ldpred2_grid(corr, df_beta, grid.param, ncores = NCORES) auto model # Get adjusted beta from the auto model multi_auto <- snp_ldpred2_auto( corr, df_beta, h2_init = h2_est, vec_p_init = seq_log(1e-4, 0.9, length.out = NCORES), ncores = NCORES ) beta_auto <- sapply(multi_auto, function(auto) auto$beta_est)","title":"5. Calculate the null R2"},{"location":"prsice/#7-obtain-model-prs","text":"","title":"7. Obtain model PRS"},{"location":"prsice/#using-genome-wide-bed-file","text":"infinitesimal model if(is.null(obj.bigSNP)){ obj.bigSNP <- snp_attach(\"EUR.QC.rds\") } genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) pred_inf <- big_prodVec( genotype, beta_inf, ind.row = ind.test, ind.col = info_snp$`_NUM_ID_`) grid model if(is.null(obj.bigSNP)){ obj.bigSNP <- snp_attach(\"EUR.QC.rds\") } genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) pred_grid <- big_prodMat( genotype, beta_grid, ind.col = info_snp$`_NUM_ID_`) auto model if(is.null(obj.bigSNP)){ obj.bigSNP <- snp_attach(\"EUR.QC.rds\") } genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) pred_auto <- big_prodMat(genotype, beta_auto, ind.row = ind.test, ind.col = info_snp$`_NUM_ID_`) # scale the PRS generated from AUTO pred_scaled <- apply(pred_auto, 2, sd) final_beta_auto <- rowMeans(beta_auto[, abs(pred_scaled - median(pred_scaled)) < 3 * mad(pred_scaled)]) pred_auto <- big_prodVec(genotype, final_beta_auto, ind.row = ind.test, ind.col = info_snp$`_NUM_ID_`)","title":"Using Genome wide bed file"},{"location":"prsice/#using-chromosome-separated-bed-files","text":"infinitesimal model pred_inf <- NULL for(chr in 1:22){ obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\".rds\")) genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) # Extract SNPs in this chromosome chr.idx <- which(info_snp$chr == chr) ind.chr <- info_snp$`_NUM_ID_`[chr.idx] tmp <- big_prodVec(genotype, beta_inf[chr.idx], ind.row = ind.test, ind.col = ind.chr) if(is.null(pred_inf)){ pred_inf <- tmp }else{ pred_inf <- pred_inf + tmp } } grid model pred_grid <- NULL for(chr in 1:22){ obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\"_.rds\")) genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) # Extract SNPs in this chromosome chr.idx <- which(info_snp$chr == chr) ind.chr <- info_snp$`_NUM_ID_`[chr.idx] tmp <- big_prodMat( genotype, beta_grid[chr.idx], ind.col = ind.chr) if(is.null(pred_grid)){ pred_grid <- tmp }else{ pred_grid <- pred_grid + tmp } } auto model pred_auto <- NULL for(chr in 1:22){ obj.bigSNP <- snp_attach(paste0(\"EUR_chr\",chr,\"_.rds\")) genotype <- obj.bigSNP$genotypes # calculate PRS for all samples ind.test <- 1:nrow(genotype) # Extract SNPs in this chromosome chr.idx <- which(info_snp$chr == chr) ind.chr <- info_snp$`_NUM_ID_`[chr.idx] tmp <- big_prodMat(genotype, beta_auto[chr.idx], ind.row = ind.test, ind.col = ind.chr) # scale the PRS generated from AUTO pred_scaled <- apply(tmp, 2, sd) final_beta_auto <- rowMeans(beta_auto[chr.idx, abs(pred_scaled - median(pred_scaled)) < 3 * mad(pred_scaled)]) tmp <- big_prodVec(genotype, final_beta_auto, ind.row = ind.test, ind.col = ind.chr) if(is.null(pred_auto)){ pred_auto <- tmp }else{ pred_auto <- pred_auto + tmp } }","title":"Using chromosome separated bed files"},{"location":"prsice/#8-get-the-final-performance-of-the-ldpred-models","text":"infinitesimal model reg.formula <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~PRS+Sex+\", .) %>% as.formula reg.dat <- y reg.dat$PRS <- pred_inf inf.model <- lm(reg.formula, dat=reg.dat) %>% summary (result <- data.table( infinitesimal = inf.model$r.squared - null.r2, null = null.r2 )) grid model reg.formula <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~PRS+Sex+\", .) %>% as.formula reg.dat <- y max.r2 <- 0 for(i in 1:ncol(pred_grid)){ reg.dat$PRS <- pred_grid[,i] grid.model <- lm(reg.formula, dat=reg.dat) %>% summary if(max.r2 < grid.model$r.squared){ max.r2 <- grid.model$r.squared } } (result <- data.table( grid = max.r2 - null.r2, null = null.r2 )) auto model reg.formula <- paste(\"PC\", 1:6, sep = \"\", collapse = \"+\") %>% paste0(\"Height~PRS+Sex+\", .) %>% as.formula reg.dat <- y reg.dat$PRS <- pred_auto auto.model <- lm(reg.formula, dat=reg.dat) %>% summary (result <- data.table( auto = auto.model$r.squared - null.r2, null = null.r2 )) How much phenotypic variation does the PRS from each model explain? Infinitesimal = 0.0100 Grid Model = 0.00180 Auto Model = 0.171","title":"8. Get the final performance of the LDpred models"}]}